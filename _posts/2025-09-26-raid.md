---
title: Linux software RAID notes
category: tech
tags: [linux kernel, raid, IO]
---

* TOC
{:toc}

Linux software RAID(redundant array of independent disks) notes

# RAID1

## requirement

- reliability
    
    - deal with bad blocks

    - deal with lost power

- data integrity

## Linux kernel raid1 driver

### RAID1 write

#### raid1_write_request

##### Function calling

![raid1_write_request function calling graph](/assets/images/raid1_write_request_callgraph.jpeg)

```
========================================================================
RAID1 WRITE REQUEST FUNCTION CALL GRAPH - DETAILED NOTES
========================================================================

FILE: raid1_write_request_callgraph.jpeg
Generated: 2025-10-02
Function: raid1_write_request (drivers/md/raid1.c:1475)
Depth: 6+ levels with complete call chains

========================================================================
KEY HIGHLIGHTS
========================================================================

1. CRITICAL PATH (Red edges):
   raid1_write_request 
   -> wait_barrier (sync with resync)
   -> alloc_r1bio (allocate descriptor)
   -> md_account_bio (bitmap tracking)
      -> md_clone_bio
         -> md_bitmap_start
            -> bitmap_ops->start_write
               -> bitmap_start_write (BITMAP CORE)
                  -> md_bitmap_get_counter (get/alloc counter)
                  -> md_bitmap_file_set_bit (set on-disk bit)
   -> r1_bio_write_done (finalize)

2. COLOR CODING:
   - Yellow (#FFD700):     Main entry point (raid1_write_request)
   - Orange (#FFA500):     Key functions at each level
   - Pink (#FF69B4):       Bitmap core functions
   - Light Pink (#DB7093): Bitmap helpers
   - Green (#90EE90):      Callers (submit_bio path)
   - Light Blue (#E6F3FF): Caller cluster background
   
3. EDGE TYPES:
   - Red thick:     Critical path
   - Blue thick:    Completion/return path
   - Purple dashed: Callback connections (bi_end_io)
   - Green dashed:  Async operations (flush)
   - Black normal:  Regular function calls

========================================================================
FUNCTION CALL CHAINS (DEPTH 6)
========================================================================

CHAIN 1: Barrier Synchronization
----------------------------------
Level 0: raid1_write_request
Level 1: wait_barrier (raid1.c:1143)
Level 2: -> sector_to_idx (hash to 1024 buckets)
         -> _wait_barrier (raid1.c:1031)
Level 3:    -> atomic_inc (nr_pending[idx]++)
            -> smp_mb__after_atomic (CRITICAL memory barrier)
            -> wait_event_lock_irq (sleep if barrier set)
            -> atomic_dec (on error)

CHAIN 2: Bitmap Tracking (MAIN FOCUS)
--------------------------------------
Level 0: raid1_write_request
Level 1: md_account_bio (md.c:9123)
Level 2: -> percpu_ref_get (active_io++)
         -> md_clone_bio (md.c:9088)
Level 3:    -> bio_alloc_clone
            -> md_bitmap_enabled
            -> md_bitmap_start (md.c:9055)
Level 4:       -> bitmap_sector (adjust offset if defined)
               -> bitmap_ops->start_write (function pointer)
Level 5:          -> bitmap_start_write (md-bitmap.c:1678) **CORE**
Level 6:             -> md_bitmap_get_counter (md-bitmap.c:1628)
                     -> md_bitmap_file_set_bit (md-bitmap.c:1149)
                     -> md_bitmap_count_page
                     -> (*bmc)++ (counter increment)

CHAIN 3: Bitmap Counter Allocation
-----------------------------------
Level 5: bitmap_start_write
Level 6: md_bitmap_get_counter
Level 7: -> calc page index (offset >> (chunkshift + 11))
         -> md_bitmap_checkpage (md-bitmap.c:253)
         -> calc counter offset (pageoff & 0x7FF)
Level 8:    -> kzalloc(PAGE_SIZE) - allocate 4KB page
            -> hijack pointer (fallback if OOM: 2 counters vs 2048)

CHAIN 4: Bitmap Disk Bit Setting
---------------------------------
Level 5: bitmap_start_write
Level 6: md_bitmap_file_set_bit
Level 7: -> filemap_get_page (get bitmap storage page)
         -> file_page_offset (calculate bit offset)
         -> kmap_local_page
         -> set_bit / set_bit_le (1 BIT on disk vs 16 bits in memory)
         -> kunmap_local_page
         -> set_page_attr (mark DIRTY, triggers flush)
Level 8:    -> bitmap_daemon_work (async flush every 5s)
            -> bitmap_unplug (immediate flush on I/O done)
            -> filemap_write_page (write dirty page to disk)

CHAIN 5: Per-Mirror Bio Cloning
--------------------------------
Level 0: raid1_write_request
Level 1: bio_alloc_clone (per mirror, in loop)
         -> Creates mbio for each mirror device
         -> Sets mbio->bi_end_io = raid1_end_write_request
         -> Sets mbio->bi_private = r1_bio

CHAIN 6: Completion Path
-------------------------
Level 1: r1_bio_write_done (raid1.c:520)
Level 2: -> raid1_end_write_request (raid1.c:459) [recursive call]
Level 3:    -> set_bit (error flags if failure)
            -> rdev_dec_pending (release device reference)
            -> r1_bio_write_done (recursive for remaining)
            -> bio_endio (complete original bio if last)
            -> md_error (handle device failure)

CHAIN 7: Write-Behind Feature
------------------------------
Level 0: raid1_write_request
Level 1: raid1_start_write_behind (raid1.c:1382)
Level 2: -> alloc_behind_master_bio
         -> bitmap_ops->start_behind_write

CHAIN 8: Serialization
----------------------
Level 0: raid1_write_request
Level 1: wait_for_serialization (raid1.c:646)
Level 2: -> wait_event (wait on rdev->serial)

========================================================================
IMPORTANT NOTES
========================================================================

1. MEMORY BARRIERS:
   - smp_mb__after_atomic() in _wait_barrier is CRITICAL
   - Prevents race between normal I/O and raise_barrier()
   - See raid1.c:1031 for detailed explanation

2. BITMAP TWO-LAYER ARCHITECTURE:
   - In-memory: 16-bit counters (tracks # of outstanding writes)
   - On-disk: 1-bit per chunk (dirty/clean flag)
   - Conversion happens in md_bitmap_file_set_bit()

3. COUNTER OVERFLOW HANDLING:
   - Max counter value: COUNTER_MAX = 16383
   - If exceeded: prepare_to_wait() -> schedule() -> sleep
   - Woken by bitmap_end_write() when counter decrements

4. PAGE HIJACKING:
   - Normal: 2048 counters per 4KB page
   - Hijacked: 2 counters stored in pointer value
   - Fallback for memory pressure (OOM)

5. ASYNC FLUSH:
   - Bitmap pages marked DIRTY in memory
   - Flushed periodically (default 5s) by bitmap_daemon_work
   - Or immediately on unplug by bitmap_unplug

6. COMPLETION CALLBACK:
   - Each cloned bio (mbio) has bi_end_io = raid1_end_write_request
   - When device I/O completes, callback invoked
   - Decrements remaining counter
   - When all mirrors done, completes original bio

========================================================================
CALLER HIERARCHY
========================================================================

submit_bio (block layer)
 |
 +-> raid1_make_request (raid1.c:1688)
      |
      +-> raid1_write_request (raid1.c:1475) **THIS FUNCTION**

========================================================================
FUNCTION STATISTICS
========================================================================

Total functions shown: 60+
Maximum depth: 8 levels
Critical path functions: 12
Bitmap-related functions: 15
Synchronization functions: 8
Completion functions: 6

Key bottlenecks:
- wait_barrier: Synchronizes with resync thread
- md_bitmap_checkpage: May allocate memory (slow)
- wait_for_serialization: Serializes writes per device

Performance-critical:
- _wait_barrier: Lock-free fast path (~50 cycles)
- bitmap_start_write: Spinlock + counter increment (~50 cycles)
- md_bitmap_get_counter: Hash lookup (~20 cycles)

========================================================================
CROSS-REFERENCES
========================================================================

Related functions not shown in detail:
- raid1_read_request: Complementary read path
- md_do_sync: Resync thread that interacts with barriers
- raise_barrier: Sets barrier for resync
- lower_barrier: Clears barrier after resync
- bitmap_end_write: Decrements counters (counterpart)

Related files:
- drivers/md/raid1.c: Main RAID1 implementation
- drivers/md/md.c: MD core layer
- drivers/md/md-bitmap.c: Bitmap implementation
- drivers/md/raid1-10.c: Shared RAID1/10 code

========================================================================
END OF NOTES
========================================================================
```


##### Knowledge Graph

```
  Knowledge Graph: raid1_write_request Function

  Level 0: Core Function

  raid1_write_request(mddev, bio, max_write_sectors)
  ├── Purpose: Handle WRITE requests for RAID1 arrays
  ├── Location: drivers/md/raid1.c:1475
  └── Return: void (async completion via bio callbacks)

  Level 1: Major Components & Data Flow

  raid1_write_request
  │
  ├── [1] INPUT VALIDATION & SYNCHRONIZATION
  │   ├── Cluster resync check
  │   ├── wait_barrier() - Barrier synchronization
  │   └── wait_blocked_rdev() - Device blocking check
  │
  ├── [2] R1BIO ALLOCATION & SETUP
  │   ├── alloc_r1bio() - Allocate request descriptor
  │   └── Initialize r1_bio->sectors
  │
  ├── [3] DEVICE SELECTION & BAD BLOCK HANDLING
  │   ├── Iterate all mirrors (disks * 2)
  │   ├── Check device status (Faulty, WriteMostly)
  │   ├── Bad block detection & avoidance
  │   └── Build target device list
  │
  ├── [4] BIO SPLITTING & BEHIND-WRITE SETUP
  │   ├── bio_submit_split_bioset() - Split if needed
  │   ├── raid1_start_write_behind() - Setup write-behind
  │   └── md_account_bio() - Accounting
  │
  ├── [5] BIO CLONING & DISPATCH
  │   ├── Clone bio for each mirror
  │   ├── Set completion callback (raid1_end_write_request)
  │   ├── Add to pending_bio_list or plug
  │   └── Wake raid1d thread
  │
  └── [6] COMPLETION TRACKING
      ├── r1_bio_write_done() - Decrements remaining count
      └── wake_up_barrier() - Unblock waiters

  Level 2: Key Data Structures

  Primary Structures

  struct r1bio (raid1.h:131)
  ├── atomic_t remaining              # Write completion counter
  ├── atomic_t behind_remaining       # Write-behind counter
  ├── sector_t sector                 # Starting sector
  ├── int sectors                     # Number of sectors
  ├── unsigned long state             # State flags (R1BIO_*)
  ├── struct bio *master_bio          # Original bio from upper layer
  ├── struct bio *behind_master_bio   # Write-behind data copy
  └── struct bio *bios[]              # Per-mirror bio array

  struct r1conf (raid1.h:52)
  ├── struct mddev *mddev             # MD device pointer
  ├── struct raid1_info *mirrors      # Mirror device array (2x raid_disks)
  ├── int raid_disks                  # Number of active disks
  ├── spinlock_t device_lock          # Device list lock
  ├── struct bio_list pending_bio_list # Pending writes queue
  ├── wait_queue_head_t wait_barrier  # Barrier wait queue
  ├── atomic_t *nr_pending            # Per-bucket pending count
  ├── atomic_t *nr_waiting            # Per-bucket waiting count
  ├── atomic_t *barrier               # Per-bucket barrier state
  └── mempool_t *r1bio_pool           # r1bio memory pool

  struct raid1_info (raid1.h:41)
  ├── struct md_rdev *rdev            # Device descriptor
  └── sector_t head_position          # Current head position

  State Flags (r1bio->state)

  R1BIO_Uptodate      # I/O completed successfully
  R1BIO_IsSync        # Synchronous I/O
  R1BIO_BehindIO      # Write-behind I/O active
  R1BIO_ReadError     # Read error occurred
  R1BIO_Returned      # Completion callback called
  R1BIO_MadeGood      # Bad block cleared
  R1BIO_WriteError    # Write error occurred
  R1BIO_FailFast      # Fast-fail enabled

  Level 3: Critical Function Dependencies

  Barrier Synchronization System

  wait_barrier(conf, sector_nr, nowait) [raid1.c:1143]
  ├── Purpose: Wait for resync/reshape to clear
  ├── Calls: _wait_barrier(conf, idx, nowait) [raid1.c:1031]
  ├── Uses: sector_to_idx(sector) - Hash sector to bucket
  ├── Blocks on: conf->wait_barrier wait queue
  └── Checks: conf->barrier[idx], conf->nr_pending[idx]

  wake_up_barrier(conf) [raid1.c:1095]
  ├── Purpose: Wake threads waiting on barrier
  └── Wakes: conf->wait_barrier wait queue

  Memory Allocation

  alloc_r1bio(mddev, bio) [raid1.c:118]
  ├── Allocates: sizeof(r1bio) + nr_mirrors * sizeof(struct bio*)
  ├── From: conf->r1bio_pool mempool
  ├── Initializes: master_bio, mddev, sector, state
  └── Returns: struct r1bio*

  bio_alloc_clone(bdev, bio, flags, bioset) [block layer]
  ├── Purpose: Clone bio for each mirror
  ├── Shares: bio_vec data (no data copy)
  └── Independent: bi_iter, bi_end_io, bi_private

  Bad Block Management

  is_badblock(rdev, sector, max_sectors, &first_bad, &bad_sectors)
  ├── Purpose: Check for known bad blocks
  ├── Returns: 0=good, 1=bad block found, -1=error
  ├── Updates: first_bad (first bad sector), bad_sectors (count)
  └── Logic: Adjust max_sectors to avoid bad blocks

  Write-Behind Support

  raid1_start_write_behind(mddev, r1_bio, bio) [raid1.c:1454]
  ├── Checks: md_bitmap_enabled(), behind_writes limit
  ├── Calls: alloc_behind_master_bio() - Copy data to pages
  ├── Sets: R1BIO_BehindIO flag
  └── Calls: bitmap_ops->start_behind_write()

  alloc_behind_master_bio(r1_bio, bio)
  ├── Allocates: Pages for data copy
  ├── Copies: bio data to allocated pages
  └── Stores: In r1_bio->behind_master_bio

  Completion Path

  raid1_end_write_request(bio) [raid1.c:447]
  ├── Called: By block layer on bio completion
  ├── Checks: Write errors, bad block updates
  ├── Updates: rdev statistics
  ├── Calls: r1_bio_write_done(r1_bio)
  └── Handles: Behind writes, error reporting

  r1_bio_write_done(r1_bio) [raid1.c:431]
  ├── Decrements: atomic_dec_and_test(&r1_bio->remaining)
  ├── On zero:
  │   ├── close_write(r1_bio) - Cleanup bitmap tracking
  │   ├── reschedule_retry() - If errors occurred
  │   └── raid_end_bio_io() - Complete original bio
  └── Purpose: Track all mirror write completions

  close_write(r1_bio) [raid1.c:415]
  ├── Calls: bitmap_ops->end_behind_write()
  ├── Calls: md_write_end(mddev)
  └── Purpose: End bitmap/metadata tracking

  Level 4: Deep Dependencies & External Interactions

  Bitmap Operations

  md_write_start(mddev, bio) [md.c:8779]
  ├── Called: Implicitly before write
  ├── Sets: mddev->in_sync = 0
  ├── Updates: writes_pending counter
  └── Waits: For superblock updates if needed

  bitmap_ops->start_write(mddev, offset, sectors) [md-bitmap.c:1685]
  ├── Gets: bitmap counter for chunk
  ├── Increments: Reference count
  ├── Calls: md_bitmap_file_set_bit() - Set disk bit
  └── Sets: BITMAP_PAGE_DIRTY flag

  bitmap_ops->end_write(mddev, offset, sectors) [md-bitmap.c:1739]
  ├── Decrements: Bitmap counter
  ├── If counter ≤ 2: Mark for cleanup
  └── Updates: events_cleared if array clean

  Bio Submission Paths

  raid1_add_bio_to_plug(mddev, mbio, raid1_unplug, disks)
  ├── Purpose: Try to add bio to current task's plug
  ├── Returns: true if plugged, false if needs immediate submit
  └── Avoids: Lock contention when plugging available

  Alternative: bio_list_add(&conf->pending_bio_list, mbio)
  ├── Protected by: conf->device_lock
  ├── Wakes: mddev->thread (raid1d)
  └── Processed by: raid1d daemon thread

  raid1d thread processing:
  └── flush_pending_writes(conf) [raid1.c:915]
      └── flush_bio_list(conf, bio) [raid1.c:900]
          ├── raid1_prepare_flush_writes() - Flush bitmap
          └── raid1_submit_write(bio) - Submit to block layer

  Cluster Integration

  mddev->cluster_ops->area_resyncing(mddev, WRITE, sector, end)
  ├── Purpose: Check if cluster node is resyncing this area
  ├── Returns: true if resync active
  └── Action: Wait on conf->wait_barrier if active

  Error Handling & Retry

  reschedule_retry(r1_bio) [raid1.c:1990]
  ├── Adds: r1_bio to conf->retry_list
  ├── Wakes: raid1d thread
  └── Purpose: Retry failed writes or update bad blocks

  raid_end_bio_io(r1_bio) [raid1-10.c:139]
  ├── Calls: bio_endio(master_bio)
  ├── Frees: r1_bio back to pool
  └── Purpose: Complete original bio to upper layer

  Device State Management

  Device Flags (in rdev->flags):
  ├── Faulty        # Device failed, skip writes
  ├── WriteMostly   # Prefer other devices for reads
  ├── WriteErrorSeen # Known write errors, check bad blocks
  ├── FailFast      # Don't retry, fail quickly
  └── CollisionCheck # Serialize writes (for write-behind)

  atomic_inc(&rdev->nr_pending)
  ├── Purpose: Prevent device removal during I/O
  └── Paired with: rdev_dec_pending() on completion

  Barrier Bucket System

  Barrier Buckets (64MB units):
  ├── Purpose: Fine-grained resync/write synchronization
  ├── Hash: sector_to_idx(sector >> BARRIER_UNIT_SECTOR_BITS)
  ├── Size: PAGE_SIZE / sizeof(atomic_t) buckets
  └── Tracks: nr_pending, nr_waiting, nr_queued, barrier per bucket

  Barrier Logic:
  ├── conf->barrier[idx] > 0    # Resync in progress, block writes
  ├── conf->nr_pending[idx]     # Active writes in this bucket
  └── conf->nr_waiting[idx]     # Threads waiting for bucket

  Write-Behind Mechanics

  Write-Behind Flow:
  1. Check: WriteMostly flag on device
  2. Allocate: behind_master_bio (data copy)
  3. Submit: Regular mirrors immediately
  4. Submit: WriteMostly mirrors with behind_master_bio
  5. Complete: When regular mirrors finish (fast path)
  6. Track: behind_remaining counter for WriteMostly completion
  7. Cleanup: Free behind_master_bio when all complete

  Benefits:
  ├── Low-latency completion for fast mirrors
  ├── Slow mirrors don't block application
  └── Bitmap tracking ensures data safety

  Data Flow Diagram

  Application Bio
        ↓
  raid1_make_request
        ↓
  raid1_write_request ──────────────────────────────┐
        ↓                                           ↓
  [Cluster Check] → wait if resyncing        md_write_start
        ↓                                           ↓
  [wait_barrier] → block if resync       bitmap_start_write
        ↓                                           ↓
  [alloc_r1bio]                          Set bitmap bits
        ↓
  [Select Mirrors] → Check Faulty/WriteMostly/BadBlocks
        ↓
  [Clone Bios] → bio_alloc_clone for each mirror
        ↓
  [Setup Callbacks] → raid1_end_write_request
        ↓
  [Dispatch] ───┬──→ raid1_add_bio_to_plug → Immediate submit
                └──→ pending_bio_list → raid1d → Submit later

        ↓ (async)
  [Block Layer] → Physical I/O to each mirror
        ↓
  raid1_end_write_request (per mirror completion)
        ↓
  r1_bio_write_done (atomic_dec remaining)
        ↓
  [All mirrors done?] ──No──→ return
        ↓ Yes
  close_write
        ↓
  bitmap_end_write
        ↓
  raid_end_bio_io → Complete original bio
        ↓
  Application notified

  Performance-Critical Paths

  1. Fast Path (no contention):
    - wait_barrier() → immediate pass
    - bio_alloc_clone() → from mempool
    - raid1_add_bio_to_plug() → plug successful
    - Parallel writes to all mirrors
  2. Slow Path (contention):
    - wait_barrier() → sleep on wait queue
    - Bad block detection → sector adjustment
    - Bio splitting → recursive submission
    - pending_bio_list → serialized through raid1d
  3. Bitmap Impact:
    - bitmap_start_write() → per-chunk bit set
    - Multiple random writes → multiple bitmap pages dirty
    - This is where your journal idea provides huge wins!

```

#### raid1_unplug

##### function call

![raid1_unplug() function call](/assets/images/raid1_unplug_callgraph.png)


#### raid1_end_write_request

##### function call

![raid1_end_write_callgraph function call](/assets/images/raid1_end_write_callgraph.png)


#### wait_barrier

```
  Knowledge Graph: wait_barrier() Function

  Level 0: Core Function

  wait_barrier(conf, sector_nr, nowait)
  ├── Purpose: Synchronize write I/O with resync/recovery operations
  ├── Location: drivers/md/raid1.c:1143
  ├── Returns: bool (true=proceed, false=would block with nowait)
  └── Role: Gate-keeper for write I/O during resync/management operations

  Level 1: Function Call Chain & Architecture

  wait_barrier(conf, sector_nr, nowait)
  ├── [1] sector_to_idx(sector_nr) → Convert sector to bucket index
  │   └── Returns: idx (0 to BARRIER_BUCKETS_NR-1)
  │
  └── [2] _wait_barrier(conf, idx, nowait) → Core barrier logic
      ├── Fast path: No barrier, return immediately
      ├── Slow path: Wait for barrier to drop
      └── Returns: bool (success/failure)

  Level 2: Core Implementation - _wait_barrier()

  _wait_barrier(conf, idx, nowait) [raid1.c:1031]
  │
  ├── PHASE 1: OPTIMISTIC INCREMENT
  │   ├── atomic_inc(&conf->nr_pending[idx])
  │   ├── smp_mb__after_atomic() - Memory barrier
  │   └── Purpose: Signal "I'm here" before checking barrier
  │
  ├── PHASE 2: FAST PATH CHECK
  │   ├── Check: !conf->array_frozen
  │   ├── Check: !conf->barrier[idx]
  │   └── If both true → return immediately (fast path)
  │
  ├── PHASE 3: SLOW PATH (barrier active or array frozen)
  │   ├── spin_lock_irq(&conf->resync_lock)
  │   ├── atomic_inc(&conf->nr_waiting[idx])
  │   ├── atomic_dec(&conf->nr_pending[idx])
  │   ├── wake_up_barrier(conf) - Wake freeze_array() if waiting
  │   │
  │   ├── [DECISION POINT]
  │   │   ├── If nowait=true → ret=false, skip wait
  │   │   └── If nowait=false → wait_event_lock_irq(...)
  │   │
  │   ├── Wait on: conf->wait_barrier
  │   ├── Condition: !conf->array_frozen && !conf->barrier[idx]
  │   ├── Hold lock: conf->resync_lock (released during sleep)
  │   │
  │   ├── On wakeup: atomic_inc(&conf->nr_pending[idx])
  │   ├── atomic_dec(&conf->nr_waiting[idx])
  │   └── spin_unlock_irq(&conf->resync_lock)
  │
  └── PHASE 4: RETURN
      └── Return bool (true=proceed, false=blocked with nowait)

  Level 3: The Barrier Bucket System

  Bucket Architecture

  BARRIER SYSTEM (64MB units per bucket)
  ├── BARRIER_UNIT_SECTOR_BITS = 17
  ├── BARRIER_UNIT_SECTOR_SIZE = 128K sectors = 64MB
  ├── BARRIER_BUCKETS_NR_BITS = PAGE_SHIFT - 2 (typically 10)
  ├── BARRIER_BUCKETS_NR = 1024 (on 4KB page systems)
  └── Total coverage: 1024 buckets × 64MB = 64GB range

  sector_to_idx(sector) [raid1.h:194]
  └── hash_long(sector >> 17, 10) → bucket index 0-1023

  Per-Bucket State Tracking

  Each bucket (idx) tracks:

  conf->nr_pending[idx]     # Active I/Os in this bucket
  ├── Incremented: When I/O enters (wait_barrier)
  ├── Decremented: When I/O completes (allow_barrier)
  └── Purpose: Track active normal I/O count

  conf->nr_waiting[idx]     # Threads waiting on barrier
  ├── Incremented: When thread sleeps in _wait_barrier()
  ├── Decremented: When thread wakes up
  └── Purpose: Track waiting I/O count

  conf->nr_queued[idx]      # Queued I/Os (during freeze)
  ├── Incremented: When I/O queued to pending_bio_list
  ├── Decremented: When I/O submitted from queue
  └── Purpose: Track queued I/O count

  conf->barrier[idx]        # Barrier level (resync active)
  ├── Incremented: In raise_barrier() (resync/recovery)
  ├── Decremented: In lower_barrier() (resync done)
  ├── Range: 0 (no barrier) to RESYNC_DEPTH (max concurrent)
  └── Purpose: Block normal I/O when resync active

  Global State

  conf->array_frozen        # Array management freeze
  ├── Set: freeze_array() - Management operations
  ├── Clear: unfreeze_array() - Resume normal operations
  └── Blocks: ALL I/O (read and write)

  conf->nr_sync_pending     # Total sync I/Os active
  ├── Incremented: raise_barrier()
  ├── Decremented: lower_barrier()
  └── Purpose: Track resync/recovery I/O count

  Level 4: Deep Dependencies & Synchronization Protocols

  Memory Ordering Protocol

  CRITICAL RACE PREVENTION (between _wait_barrier and raise_barrier)

  Thread A (_wait_barrier):              Thread B (raise_barrier):
  1. atomic_inc(nr_pending[idx])         1. atomic_inc(barrier[idx])
  2. smp_mb__after_atomic() ────────────→2. smp_mb__after_atomic()
  3. if (barrier[idx] == 0)              3. if (nr_pending[idx] == 0)
       return (fast path)                     continue (can raise)

  Without memory barriers:
  - Thread A might read old barrier[idx]=0 (before B's increment)
  - Thread B might read old nr_pending[idx]=0 (before A's increment)
  - BOTH proceed → RACE! (I/O happens during resync)

  With memory barriers:
  - At least ONE thread sees the other's update
  - Either A waits OR B waits → NO RACE

  Lock-Free Fast Path

  Optimization: Avoid locks when no barrier

  _wait_barrier() fast path:
  ├── atomic_inc(&nr_pending[idx])        # Lock-free
  ├── smp_mb__after_atomic()              # Memory fence
  ├── Check: !array_frozen && !barrier[idx]
  └── If true: Return without taking lock  # FAST!

  Statistics:
  - Normal operation (no resync): 99.9% fast path
  - During resync: Only affected buckets take slow path
  - Performance: Lock-free increment + read + branch

  Wait Queue Integration

  conf->wait_barrier (wait_queue_head_t)
  ├── Purpose: Sleep/wake synchronization point
  │
  ├── Waiters:
  │   ├── _wait_barrier() - Normal I/O waiting for barrier drop
  │   ├── wait_read_barrier() - Read I/O waiting for unfreeze
  │   ├── raise_barrier() - Resync waiting for I/O drain
  │   └── freeze_array() - Management waiting for I/O drain
  │
  ├── Wakers:
  │   ├── wake_up_barrier() - Check and wake if sleepers
  │   ├── allow_barrier() - I/O completed
  │   ├── lower_barrier() - Resync completed
  │   ├── unfreeze_array() - Unfreeze completed
  │   └── flush_bio_list() - Queued I/O processing
  │
  └── Lock held during wait: conf->resync_lock

  Interaction with raise_barrier()

  raise_barrier(conf, sector_nr) [raid1.c:970]
  ├── Purpose: Block normal I/O to allow resync
  │
  ├── PHASE 1: Wait for no waiting threads
  │   └── wait_event_lock_irq(!nr_waiting[idx])
  │
  ├── PHASE 2: Set barrier
  │   ├── atomic_inc(&barrier[idx])
  │   └── smp_mb__after_atomic()
  │
  ├── PHASE 3: Wait for I/O drain
  │   └── wait_event_lock_irq(
  │           !array_frozen &&
  │           !nr_pending[idx] &&
  │           barrier[idx] < RESYNC_DEPTH)
  │
  └── PHASE 4: Success
      └── atomic_inc(&nr_sync_pending)

  Coordination with _wait_barrier():
  - raise_barrier sets barrier[idx]=1
  - _wait_barrier sees barrier[idx]≠0
  - _wait_barrier waits until barrier[idx]=0
  - lower_barrier() decrements barrier[idx]
  - _wait_barrier() wakes and proceeds

  Interaction with freeze_array()

  freeze_array(conf, extra) [raid1.c:1176]
  ├── Purpose: Quiesce ALL I/O for management operations
  │
  ├── PHASE 1: Set freeze flag
  │   └── conf->array_frozen = 1
  │
  ├── PHASE 2: Wait for I/O drain
  │   └── wait_event_lock_irq_cmd(
  │           get_unqueued_pending() == extra,
  │           flush_pending_writes())
  │
  └── Purpose: Drain all flying I/O

  get_unqueued_pending(conf) [raid1.c:1164]
  ├── ret = nr_sync_pending
  ├── for each bucket:
  │   └── ret += (nr_pending[idx] - nr_queued[idx])
  └── Returns: Count of I/O not yet queued

  _wait_barrier() cooperation:
  - Sees array_frozen=1
  - Moves to slow path
  - Decrements nr_pending (helps drain)
  - Increments nr_waiting
  - Wakes freeze_array() (via wake_up_barrier)
  - Waits for unfreeze

  Interaction with allow_barrier()

  allow_barrier(conf, sector_nr) [raid1.c:1156]
  ├── Calls: _allow_barrier(conf, idx)
  └── Purpose: Signal I/O completion

  _allow_barrier(conf, idx) [raid1.c:1150]
  ├── atomic_dec(&nr_pending[idx])
  ├── wake_up_barrier(conf)
  └── Purpose: Release barrier slot

  Typical I/O lifecycle:
  1. wait_barrier() → inc nr_pending[idx]
  2. Submit I/O to devices
  3. I/O completes
  4. allow_barrier() → dec nr_pending[idx]
  5. If resync waiting: wake_up allows it to proceed

  NOWAIT Flag Handling

  NOWAIT Support (for REQ_NOWAIT bios):

  Traditional blocking I/O:
  ├── wait_barrier(conf, sector, false)
  ├── If barrier active: Sleep until clear
  └── Always returns true

  Non-blocking I/O (NOWAIT):
  ├── wait_barrier(conf, sector, true)
  ├── If barrier active: Return false immediately
  └── Caller must handle failure (return -EAGAIN to user)

  Code flow:
  if (!wait_barrier(conf, sector, bio->bi_opf & REQ_NOWAIT)) {
      bio_wouldblock_error(bio);  # Return -EAGAIN
      return;
  }

  Purpose: Support async I/O (io_uring, AIO)

  State Transition Diagrams

  Normal I/O Flow

  START
    ↓
  wait_barrier(conf, sector, nowait)
    ↓
  sector_to_idx(sector) → idx
    ↓
  _wait_barrier(conf, idx, nowait)
    ↓
  atomic_inc(nr_pending[idx])
    ↓
  smp_mb__after_atomic()
    ↓
  [CHECK: array_frozen || barrier[idx]]
    ↓                           ↓
    NO (Fast Path)             YES (Slow Path)
    ↓                           ↓
    Return true                 spin_lock_irq(resync_lock)
    ↓                           ↓
  [Proceed with I/O]           atomic_inc(nr_waiting[idx])
    ↓                           atomic_dec(nr_pending[idx])
  [I/O Completes]               wake_up_barrier(conf)
    ↓                           ↓
  allow_barrier(conf, sector)  [NOWAIT?]
    ↓                           ↓              ↓
  atomic_dec(nr_pending[idx])  NO             YES
  wake_up_barrier(conf)         ↓              ↓
    ↓                          wait_event...   Return false
  DONE                          ↓              ↓
                           [Wait for clear]   [Caller handles]
                                ↓
                           atomic_inc(nr_pending[idx])
                           atomic_dec(nr_waiting[idx])
                           spin_unlock_irq(resync_lock)
                                ↓
                           Return true
                                ↓
                           [Proceed with I/O]

  Resync Coordination

  NORMAL I/O THREAD              RESYNC THREAD
        ↓                              ↓
  wait_barrier(sector)          raise_barrier(sector)
        ↓                              ↓
  atomic_inc(nr_pending[idx])   wait(!nr_waiting[idx])
        ↓                              ↓
  smp_mb() ←───────────────────→ atomic_inc(barrier[idx])
        ↓                              ↓
  Check barrier[idx]             smp_mb()
        ↓                              ↓
  If != 0:                       wait(!nr_pending[idx])
    ↓                                  ↓
    Wait on barrier[idx]=0       [When nr_pending=0]
    ↓                                  ↓
  [Barrier drops]               Do resync I/O
    ↓                                  ↓
  Proceed with I/O              lower_barrier()
    ↓                                  ↓
  allow_barrier()               atomic_dec(barrier[idx])
                                       ↓
                                wake_up_barrier()
                                       ↓
                                [Normal I/O unblocked]

  Performance Characteristics

  Fast Path (No Contention)

  Cost: ~10-20 CPU cycles
  ├── 1 atomic increment (nr_pending)
  ├── 1 memory barrier (smp_mb)
  ├── 2 atomic reads (array_frozen, barrier)
  └── 1 conditional branch

  Slow Path (Barrier Active)

  Cost: Context switch + wait time
  ├── 1 spinlock acquire/release
  ├── 2 atomic operations (nr_waiting, nr_pending)
  ├── 1 wait queue sleep/wake
  └── Wait duration: Until resync clears bucket

  Bucket Hash Benefits

  Parallelism: 1024 independent buckets
  ├── Resync in bucket 0: Only bucket 0 I/O blocks
  ├── I/O to other buckets: Fast path continues
  └── Effective concurrency: ~1024-way parallel

  Error Scenarios

  NOWAIT Failure

  Trigger: REQ_NOWAIT bio hits active barrier
  Flow:
  ├── wait_barrier(conf, sector, true)
  ├── _wait_barrier() returns false
  ├── bio_wouldblock_error(bio)
  └── Return -EAGAIN to application

  Recovery Interrupt

  Trigger: MD_RECOVERY_INTR set during resync
  Flow (in raise_barrier):
  ├── detect MD_RECOVERY_INTR
  ├── atomic_dec(&barrier[idx])
  ├── wake_up(&wait_barrier)
  └── Return -EINTR

```


#### raid1d thread

#### function call




#### resync thread

```
  RAID1 Resync Thread: When, How, and Why

  What is the Resync Thread?

  The resync thread is a kernel thread that synchronizes data between RAID1 mirrors. It runs in the background to ensure all mirror
  devices contain identical data.

  When is Resync Triggered?

  1. Device Failures & Errors

  // Write error during normal I/O (raid1.c:2091-2094)
  set_bit(WriteErrorSeen, &rdev->flags);
  if (!test_and_set_bit(WantReplacement, &rdev->flags))
      set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);

  // Unrecoverable read error (raid1.c:2352)
  set_bit(MD_RECOVERY_INTR, &mddev->recovery);

  Triggers:
  - Write error on a mirror device
  - Device marked for replacement
  - Persistent read errors requiring rebuild

  2. Adding New Devices

  // Adding a new disk to array (raid1.c:3440-3442)
  set_bit(MD_RECOVERY_RECOVER, &mddev->recovery);
  set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);
  md_wakeup_thread(mddev->thread);

  Triggers:
  - Hot-add of new disk to RAID1
  - Replacing failed disk
  - Adding spare disk that needs sync

  3. Array Resize/Reshape

  // Resize operation (raid1.c:3345)
  if (mddev->resync_offset > mddev->dev_sectors) {
      mddev->resync_offset = mddev->dev_sectors;
      set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);
  }

  Triggers:
  - Expanding array size
  - Growing device capacity

  4. Unclean Shutdown Recovery

  // Bitmap-driven recovery after crash
  if (mddev->bitmap && dirty_bits_found) {
      set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);
  }

  Triggers:
  - System crash with dirty bitmap
  - Unclean array shutdown
  - Power failure recovery

  5. Manual Triggers

  # Administrator-initiated operations
  echo check > /sys/block/md0/md/sync_action    # Data verification
  echo repair > /sys/block/md0/md/sync_action   # Fix inconsistencies
  echo resync > /sys/block/md0/md/sync_action   # Force full resync

  How is Resync Thread Started?

  Complete Call Chain:

  [EVENT: Write error, device add, etc.]
           ↓
  set_bit(MD_RECOVERY_NEEDED, &mddev->recovery)
           ↓
  md_wakeup_thread(mddev->thread)   # Wake raid1d daemon
           ↓
  raid1d() → md_check_recovery(mddev)  [md.c:10003]
           ↓
  [Check MD_RECOVERY_NEEDED flag]  [md.c:10122]
           ↓
  queue_work(md_misc_wq, &mddev->sync_work)  [md.c:10124]
           ↓
  [Work queue executes]
           ↓
  md_start_sync(work_struct *ws)  [md.c:9877]
           ↓
  md_register_thread(md_do_sync, mddev, "resync")  [md.c:9922]
           ↓
  mddev->sync_thread created
           ↓
  md_wakeup_thread(mddev->sync_thread)  [md.c:9940]
           ↓
  [Kernel thread starts]
           ↓
  md_do_sync(thread)  [md.c:9305]
           ↓
  [Loop: Call personality-specific sync_request]
           ↓
  raid1_sync_request(mddev, sector_nr, max_sector, skipped)  [raid1.c:2777]
           ↓
  [Read from good disk, write to out-of-sync disk]
           ↓
  [Repeat until all sectors synchronized]
           ↓
  set_bit(MD_RECOVERY_DONE, &mddev->recovery)
           ↓
  md_check_recovery() → md_reap_sync_thread()
           ↓
  [Thread cleanup, activate spares]

  Why Does Resync Thread Exist? (Motivations)

  1. Data Consistency After Failures

  Problem: Write to mirror A succeeds, write to mirror B fails
  Mirror A: [Old Data] → [New Data] ✓
  Mirror B: [Old Data] → [Old Data] ✗ (write failed)

  Result: Mirrors are inconsistent!

  Solution: Resync thread copies correct data from A to B
  Resync: Read Mirror A → Write to Mirror B
  Result: Both mirrors now have [New Data]

  2. Crash Recovery with Bitmap

  Problem: System crashes during writes
  Bitmap shows dirty chunks: 100, 105, 200, 300
  (These chunks may be inconsistent between mirrors)

  Solution: Resync only dirty bitmap chunks
  For each dirty chunk:
      Choose authoritative source
      Copy to all other mirrors
      Clear bitmap bit

  Much faster than full array scan!

  3. New Device Integration

  Problem: Adding empty disk to active array
  Mirror A: [All user data]
  Mirror B: [Empty / old data]

  Can't serve reads from Mirror B yet!

  Solution: Resync thread copies all data
  raid1_sync_request() copies:
      Sector 0-128KB from A to B
      Sector 128KB-256KB from A to B
      ...
      Until entire disk synchronized

  Then Mark Mirror B as "In Sync"

  4. Prevent Read Errors

  Problem: Silent data corruption or bit rot
  Mirror A: [Sector 1000] = 0xDEADBEEF (correct)
  Mirror B: [Sector 1000] = 0xDEADBAD1 (corrupted)

  Normal reads might return bad data!

  Solution: Check/Repair operations via resync
  echo check > /sys/block/md0/md/sync_action

  Resync compares all mirrors:
      If mismatch found → Report error
      If repair mode → Fix using majority vote

  5. Background Operation = No Downtime

  Key Benefit: Array remains online during resync!

  User I/O Thread:                Resync Thread:
      ↓                                ↓
  wait_barrier(sector 5000)       raise_barrier(sector 1000)
      ↓                                ↓
  [sector 5000 != 1000]          [Blocks only sector 1000]
      ↓                                ↓
  Proceed immediately!            Read sector 1000 from good disk
      ↓                                ↓
  Write to all mirrors            Write to out-of-sync disk
      ↓                                ↓
  Complete                        lower_barrier(sector 1000)
                                       ↓
                                  Move to next chunk

  Result:
  - User I/O continues uninterrupted
  - Resync progresses in background
  - Only 64MB buckets are briefly locked

  Resync Thread Operation

  Main Loop (md_do_sync)

  md_do_sync(thread) {
      while (sector < max_sector) {
          // Call RAID1-specific sync function
          sectors = raid1_sync_request(mddev, sector, max_sector, &skipped);

          if (sectors == 0)
              break;  // Done or error

          sector += sectors;

          // Throttle to avoid overwhelming system
          if (speed_limit && too_fast)
              schedule_timeout();

          // Check for interruption
          if (MD_RECOVERY_INTR)
              break;
      }

      set_bit(MD_RECOVERY_DONE, &mddev->recovery);
  }

  RAID1-Specific Sync (raid1_sync_request)

  raid1_sync_request(mddev, sector_nr, max_sector, skipped) {
      // 1. Determine which disk has good data
      for_each_mirror {
          if (disk_is_in_sync && no_bad_blocks)
              source_disk = this_disk;
      }

      // 2. Raise barrier for this sector range
      raise_barrier(conf, sector_nr);

      // 3. Read from good disk
      bio = bio_alloc_for_read(source_disk, sector_nr, RESYNC_SECTORS);
      submit_bio_wait(bio);

      // 4. Write to out-of-sync disks
      for_each_mirror {
          if (!disk_is_in_sync || has_bad_blocks) {
              clone_bio = bio_clone(bio);
              clone_bio->bi_end_io = resync_end_io;
              submit_bio(clone_bio);
          }
      }

      // 5. Wait for all writes to complete
      wait_for_completion(&resync_io_done);

      // 6. Lower barrier
      lower_barrier(conf, sector_nr);

      // 7. Update bitmap (clear dirty bits)
      md_bitmap_end_sync(mddev, sector_nr, &sync_blocks);

      return RESYNC_SECTORS;  // Processed chunk size
  }

  Performance Tuning

  Speed Limits

  # Limit resync speed to 50 MB/s (avoid overwhelming system)
  echo 50000 > /sys/block/md0/md/sync_speed_min
  echo 100000 > /sys/block/md0/md/sync_speed_max

  # Current speed
  cat /proc/mdstat
  # md0 : active raid1 sda1[0] sdb1[1]
  #       resync = 45.2% (200GB/440GB) finish=3.5min speed=85000K/sec

  Suspend/Resume

  # Pause resync (for heavy workload period)
  echo frozen > /sys/block/md0/md/sync_action

  # Resume
  echo idle > /sys/block/md0/md/sync_action

  Summary

  The resync thread exists to:
  1. Maintain data consistency across all mirrors
  2. Recover from failures without manual intervention
  3. Integrate new devices into active arrays
  4. Verify data integrity periodically
  5. Operate in background without service interruption

  It's triggered by:
  - Device failures and errors
  - Adding/replacing disks
  - Unclean shutdowns (bitmap recovery)
  - Manual administrator commands
  - Array reshape operations

  It works by:
  - Reading from known-good mirror
  - Writing to out-of-sync mirrors
  - Using barrier system to coordinate with normal I/O
  - Processing 64MB chunks at a time
  - Updating bitmap to track progress

  This design allows RAID1 to self-heal automatically while remaining fully operational during recovery!
```

#### bitmap_start_write

##### KNOWLEDGE GRAPH
```
  KNOWLEDGE GRAPH: bitmap_start_write() - DEPTH 4 ANALYSIS

  LEVEL 0: Core Function

  bitmap_start_write(mddev, offset, sectors)
  ├─ Location: drivers/md/md-bitmap.c:1678
  ├─ Purpose: Mark bitmap chunks as dirty before write I/O
  ├─ Scope: Static (internal to md-bitmap.c)
  └─ Invocation: Via bitmap_ops->start_write() function pointer

  Function Signature:
  static void bitmap_start_write(struct mddev *mddev, 
                                 sector_t offset,
                                 unsigned long sectors)

  Core Algorithm:
  1. Loop through sector range - Process chunks covering the write region
  2. Get bitmap counter - md_bitmap_get_counter() with create=1
  3. Handle counter overflow - Wait if counter == COUNTER_MAX (16383)
  4. Update counter state:
    - If 0 → Set disk bit, count page, transition to 2
    - If 1 → Transition to 2
    - Always increment counter
  5. Advance - Move to next chunk

  ---
  LEVEL 1: Direct Dependencies

  1.1 Data Structures

  struct bitmap (md-bitmap.c:152)
  ├─ counts: struct bitmap_counts
  │  ├─ lock: spinlock_t                    [Protects all counter operations]
  │  ├─ bp: struct bitmap_page*             [Array of page metadata]
  │  ├─ pages: unsigned long                [Total bitmap pages]
  │  ├─ chunkshift: unsigned long           [log2(chunk_size_bytes)]
  │  └─ chunks: unsigned long               [Total data chunks]
  ├─ overflow_wait: wait_queue_head_t       [Sleep queue for counter overflow]
  ├─ storage: struct bitmap_storage         [On-disk bitmap pages]
  ├─ mddev: struct mddev*                   [Parent MD device]
  └─ flags: unsigned long                   [BITMAP_HOSTENDIAN, etc.]

  bitmap_counter_t (typedef __u16)
  ├─ Bit 15: NEEDED_MASK (0x8000)          [Chunk needs resync]
  ├─ Bit 14: RESYNC_MASK (0x4000)          [Resync in progress]
  └─ Bits 0-13: COUNTER (0-16383)          [# of outstanding writes]

  1.2 Core Functions Called (Depth 1)

  A. md_bitmap_get_counter() - md-bitmap.c:1628

  Purpose: Get/allocate counter for sector offset
  Returns: bitmap_counter_t* (16-bit counter pointer)

  Algorithm:
  ├─ Calculate page index: offset >> (chunkshift + PAGE_COUNTER_SHIFT)
  ├─ md_bitmap_checkpage(page, create=1) → Allocate page if needed
  ├─ Handle hijacked pointer (if memory allocation failed)
  └─ Return pointer to counter in bp[page].map

  Key Feature: Two-tier addressing
  ├─ Page level: 2048 counters per page (PAGE_COUNTER_SHIFT = 11)
  └─ Counter level: 16-bit value per chunk

  B. md_bitmap_file_set_bit() - md-bitmap.c:1149

  Purpose: Set bit in on-disk bitmap (called when counter 0→2)

  Algorithm:
  ├─ Calculate chunk = block >> chunkshift
  ├─ Get physical page: filemap_get_page(chunk)
  ├─ Calculate bit offset: file_page_offset(chunk)
  ├─ kmap_local_page() to access page
  ├─ set_bit() or set_bit_le() based on BITMAP_HOSTENDIAN
  ├─ kunmap_local_page()
  └─ set_page_attr(BITMAP_PAGE_DIRTY) → Mark for flush

  Critical: ON-DISK uses 1 BIT per chunk, in-memory uses 16 BITS

  C. md_bitmap_count_page() - md-bitmap.c:1462

  Purpose: Track # of dirty chunks per page

  Algorithm:
  ├─ Calculate page = chunk >> PAGE_COUNTER_SHIFT
  ├─ bp[page].count += inc (usually +1)
  └─ md_bitmap_checkfree(page) → Free if count==0

  Why: Enables efficient page flushing - only flush pages with count > 0

  1.3 Control Flow States

  Counter State Machine:
  0 (Clean)
    ↓ [First write starts]
    ├─ md_bitmap_file_set_bit() → Set on-disk bit
    ├─ md_bitmap_count_page(+1) → Increment page dirty count
    └─ Transition to 2
  2 (Dirty, 1 write)
    ↓ [Additional writes]
    └─ Increment to 3, 4, 5... up to COUNTER_MAX (16383)
  N (Dirty, N-1 writes outstanding)
    ↓ [Writes complete via bitmap_end_write()]
    └─ Decrement back toward 0

  Special: Counter overflow
  ├─ If counter == 16383: prepare_to_wait() → schedule() → retry
  └─ Woken by bitmap_end_write() via wake_up(&overflow_wait)

  ---
  LEVEL 2: Secondary Dependencies

  2.1 Page Management Layer

  md_bitmap_checkpage() - md-bitmap.c:253

  Purpose: Ensure bitmap page is allocated for counter operations

  States:
  ├─ Already allocated: bp[page].map != NULL → Return 0
  ├─ Hijacked: bp[page].hijacked == 1 → Use pointer as counter (2 max)
  └─ Need allocation: kzalloc(PAGE_SIZE, GFP_NOIO)

  Hijack Mechanism (memory pressure fallback):
  ├─ If kzalloc() fails, set bp[page].hijacked = 1
  ├─ Use bp[page].map pointer AS TWO 16-bit counters
  └─ Limits: Only 2 counters per hijacked page vs 2048 for allocated

  Lock Semantics:
  ├─ __releases(bitmap->counts.lock) - Drops lock during allocation
  └─ __acquires(bitmap->counts.lock) - Reacquires after kzalloc()

  Why: Prevent deadlock during memory allocation under I/O pressure

  filemap_get_page() - md-bitmap.c:~400

  Purpose: Get on-disk bitmap page for bit operations

  Flow:
  ├─ Calculate index: file_page_index(chunk)
  ├─ Add sb_index (skip superblock pages)
  ├─ Add cluster_slot offset (for clustered MD)
  └─ Return storage.filemap[index]

  Storage Layout:
  ├─ storage.file: struct file* (if file-based bitmap)
  ├─ storage.sb_page: Bitmap superblock page
  └─ storage.filemap[]: Array of struct page* for bitmap data

  2.2 Wait Queue Management

  Overflow Handling (md-bitmap.c:1697-1708):

  Thread blocks when counter == COUNTER_MAX:
  ├─ DEFINE_WAIT(__wait)
  ├─ prepare_to_wait(&bitmap->overflow_wait, TASK_UNINTERRUPTIBLE)
  ├─ spin_unlock_irq() → Release lock before sleep
  ├─ schedule() → Sleep until woken
  ├─ finish_wait()
  └─ continue → Retry loop

  Wakeup Source: bitmap_end_write() (md-bitmap.c:1732):
  ├─ Decrements counter
  ├─ if (bitmap->overflow_wait.head)
  └─   wake_up(&bitmap->overflow_wait)

  Max Outstanding I/O per Chunk: 16383 writes

  2.3 Bitmap Storage I/O

  set_page_attr() - md-bitmap.c:~1180

  Purpose: Mark bitmap page for later flush

  Attributes:
  ├─ BITMAP_PAGE_DIRTY → Needs write to disk
  ├─ BITMAP_PAGE_NEEDWRITE → Immediate write needed
  └─ BITMAP_PAGE_PENDING → Write in progress

  Flush Triggers:
  ├─ Periodic: bitmap_daemon_work() every daemon_sleep seconds
  └─ Immediate: __bitmap_unplug() when I/O completes

  ---
  LEVEL 3: Tertiary Dependencies

  3.1 Caller Chain (How bitmap_start_write is invoked)

  RAID Write I/O Path:
  ├─ md_account_bio() [md.c:9123]
  │  └─ md_clone_bio() [md.c:9088]
  │     └─ md_bitmap_start() [md.c:9055]
  │        └─ mddev->bitmap_ops->start_write(offset, sectors)
  │           └─ bitmap_start_write() [md-bitmap.c:1678]
  │
  └─ Used by: RAID1, RAID10, RAID5 write paths

  Alternative Implementation:
  └─ llbitmap_start_write() [md-llbitmap.c:1061]
     └─ Low-latency bitmap (different internal structure)

  md_bitmap_start() - md.c:9055

  static void md_bitmap_start(struct mddev *mddev, 
                              struct md_io_clone *md_io_clone)
  {
      md_bitmap_fn *fn = unlikely(md_io_clone->rw == STAT_DISCARD) ?
                         mddev->bitmap_ops->start_discard :
                         mddev->bitmap_ops->start_write;

      // Allow personality to adjust sector range
      if (mddev->pers->bitmap_sector)
          mddev->pers->bitmap_sector(mddev, &md_io_clone->offset,
                                     &md_io_clone->sectors);

      fn(mddev, md_io_clone->offset, md_io_clone->sectors);
  }

  Key Insight: Discard operations also use bitmap tracking via start_discard pointer (same function)

  3.2 Bitmap Operations Table

  struct bitmap_operations bitmap_ops [md-bitmap.c:2975]:
  ├─ .start_write = bitmap_start_write
  ├─ .end_write = bitmap_end_write
  ├─ .start_discard = bitmap_start_write  [Same function!]
  ├─ .end_discard = bitmap_end_write
  ├─ .unplug = bitmap_unplug
  └─ [... 20+ other operations]

  Registered in mddev->bitmap_ops during bitmap creation

  3.3 Page Counter Architecture

  Memory Hierarchy:

  bp[] Array (struct bitmap_page):
  ├─ Each bitmap_page covers 2048 chunks (PAGE_COUNTER_SHIFT=11)
  ├─ bp[page].map → Page of 2048 × 16-bit counters (4KB)
  ├─ bp[page].count → # of dirty chunks in this page
  └─ bp[page].pending → Page has pending flush

  Counter Access Pattern:
  offset (sectors)
    ↓ >> chunkshift (default 18, for 128KB chunks)
  chunk_id
    ↓ >> PAGE_COUNTER_SHIFT (11, for 2048 counters/page)  
  page_index → bp[page_index]
    ↓ & PAGE_COUNTER_MASK (0x7FF)
  counter_offset → bp[page].map[counter_offset]

  Example: 1TB array, 128KB chunks, 64-bit counters
  ├─ Total chunks: 1TB / 128KB = 8M chunks
  ├─ Counters per page: 2048
  ├─ Total pages: 8M / 2048 = 4K pages
  └─ Memory: 4K pages × 4KB/page = 16MB RAM

  ---
  LEVEL 4: Peripheral Systems

  4.1 Bitmap Daemon (Periodic Flush)

  bitmap_daemon_work() - md-bitmap.c:1514

  Scheduled every daemon_sleep seconds (default 5s)

  Algorithm:
  ├─ For each page in bp[]:
  │  ├─ If bp[page].count > 0: Has dirty chunks
  │  ├─ For each counter in page:
  │  │  ├─ bmc = md_bitmap_get_counter(block)
  │  │  ├─ If *bmc == 1 && !need_sync:
  │  │  │  └─ Clear bit, decrement count, md_bitmap_file_clear_bit()
  │  │  └─ If *bmc <= 2: Almost clean, keep tracking
  │  └─ If bp[page].pending: Write page to disk
  │
  ├─ Update events_cleared counter
  └─ Schedule next run: mddev_set_timeout(daemon_sleep)

  Purpose: Periodic cleanup + flush dirty bitmap pages to disk

  4.2 Immediate Flush Path

  __bitmap_unplug() - md-bitmap.c:1241

  Called when I/O completes and immediate flush needed

  Algorithm:
  ├─ For each page:
  │  ├─ If BITMAP_PAGE_DIRTY: Mark as NEEDWRITE
  │  └─ If BITMAP_PAGE_NEEDWRITE: filemap_write_page()
  │
  └─ Wait for writes if sync==true

  Callers:
  ├─ RAID1: raid1_unplug() → unplug bitmap after batch writes
  ├─ RAID5: raid5_unplug() → flush after stripe completion
  └─ Manual: bitmap_store() sysfs trigger

  4.3 Completion Path (Counterpart)

  bitmap_end_write() - md-bitmap.c:1732

  Called when write I/O completes

  Algorithm:
  ├─ bmc = md_bitmap_get_counter(offset, create=0)
  ├─ if (!bmc) return  // Already cleared
  ├─ *bmc -= 1  // Decrement counter
  ├─ if (*bmc == 1):  // Last outstanding write
  │  └─ Wake any threads waiting on overflow_wait
  └─ if (bitmap->overflow_wait.head)
     └─ wake_up(&bitmap->overflow_wait)

  Symmetry:
  ├─ bitmap_start_write: counter++, possibly set bit
  └─ bitmap_end_write: counter--, possibly clear bit (via daemon)

  4.4 Resync Integration

  Bitmap chunks have dual purpose:

  1. Write-Intent Tracking (this function):
     ├─ COUNTER field: # of outstanding writes
     └─ NEEDED bit: Chunk has pending writes

  2. Resync Progress Tracking:
     ├─ RESYNC bit: Chunk currently being resynced
     └─ Used by raid1_sync_request() to skip clean regions

  State Transitions:
  Write starts:    COUNTER=0 → NEEDED=1, COUNTER=2
  Write completes: COUNTER-- → Eventually COUNTER=1
  Daemon cleanup:  COUNTER=1 && all I/O done → COUNTER=0, NEEDED=0
  Resync needed:   NEEDED=1 → Trigger resync via md_do_sync()
  Resync starts:   Set RESYNC=1
  Resync done:     Clear RESYNC, clear on-disk bit

  4.5 Clustered MD Integration

  cluster_slot Mechanism:

  Clustered Arrays (>1 node accessing same storage):
  ├─ Each node has independent bitmap region
  ├─ Offset = cluster_slot × storage.file_pages
  └─ node_offset added to all file_page_index() calculations

  Why: Prevents bitmap conflicts between cluster nodes

  Synchronization:
  ├─ md-cluster.c coordinates bitmap operations
  ├─ copy_from_slot() merges bitmaps from failed nodes
  └─ sync_with_cluster() ensures consistency

  4.6 Memory Pressure Handling

  Hijack Pointer Mechanism (md_bitmap_checkpage):

  Normal Mode:
  ├─ bp[page].map → 4KB page with 2048 × 16-bit counters
  └─ Supports 16383 outstanding writes per chunk

  Hijacked Mode (memory allocation failure):
  ├─ bp[page].hijacked = 1
  ├─ bp[page].map used AS counter storage (pointer value)
  ├─ High 16 bits: Counter for even chunk
  ├─ Low 16 bits: Counter for odd chunk
  └─ MAX 2 counters per page (vs 2048 normally)

  Determination:
  ├─ int hi = (pageoff > PAGE_COUNTER_MASK)
  └─ return &((bitmap_counter_t *)&bitmap->bp[page].map)[hi]

  Degradation: 1024× reduction in tracking granularity

  ---
  CROSS-CUTTING CONCERNS

  Locking Protocol

  bitmap->counts.lock (spinlock_irq):
  ├─ Held: During all counter read/modify operations
  ├─ Released: Before memory allocation (md_bitmap_checkpage)
  ├─ Released: Before sleeping (overflow wait)
  └─ IRQ-safe: Uses spin_lock_irq() to prevent deadlock with bio completion

  Why IRQ-safe: bio_endio() calls bitmap_end_write() in interrupt context

  Performance Characteristics

  Fast Path (common case):
  ├─ spinlock_irq()                    [~10 cycles]
  ├─ md_bitmap_get_counter()           [hash lookup, ~20 cycles]
  ├─ Counter increment                 [1 cycle]
  ├─ spin_unlock_irq()                 [~10 cycles]
  └─ Total: ~50 CPU cycles per write

  Slow Path (first write to chunk):
  ├─ md_bitmap_file_set_bit()          [~200 cycles, kmap + set_bit]
  ├─ md_bitmap_count_page()            [~10 cycles]
  └─ Total: ~250 CPU cycles

  Worst Case (counter overflow):
  ├─ schedule() → Context switch       [~10,000 cycles]
  └─ Only if >16383 concurrent writes to same chunk (extremely rare)

  On-Disk Format

  Bitmap File Layout:
  ├─ Sector 0-7: Bitmap superblock (4KB)
  ├─ Sector 8+: Bitmap data pages
  │  ├─ 1 bit per chunk
  │  ├─ 32768 chunks per 4KB page (4096 bytes × 8 bits)
  │  └─ Byte order: Little-endian or host-endian (BITMAP_HOSTENDIAN flag)
  │
  └─ Size formula: ceil(total_chunks / 32768) × 4KB

  Example: 1TB array, 128KB chunks
  ├─ Chunks: 1TB / 128KB = 8,388,608
  ├─ Pages: ceil(8,388,608 / 32768) = 256 pages
  └─ Bitmap size: 256 × 4KB = 1MB on disk

  ---
  RELATED FUNCTIONS (Same Level)

  Sibling Operations:
  ├─ bitmap_end_write()        [Counterpart - decrements counter]
  ├─ bitmap_start_sync()       [Resync tracking]
  ├─ bitmap_end_sync()         [Resync completion]
  ├─ bitmap_close_sync()       [Resync region closure]
  ├─ llbitmap_start_write()    [Alternative low-latency implementation]
  └─ bitmap_start_behind_write() [Write-behind handling]

  ---
  KEY INSIGHTS

  1. Two-Layer Architecture: In-memory counters (16-bit) vs on-disk bits (1-bit) for space efficiency
  2. Counter Overflow Protection: Blocks new writes if >16,383 outstanding to same chunk (extremely rare)
  3. Deferred Disk Writes: Bit set immediately in memory, flushed periodically (5s default) or on unplug
  4. Memory Pressure Resilient: Hijack mechanism allows operation with 1024× less memory
  5. Lock-Free Fast Path: Most writes just increment counter under spinlock (~50 cycles)
  6. Cluster-Aware: Per-node bitmap regions prevent conflicts in clustered deployments
  7. Resync Integration: Same bitmap tracks both write-intent and resync progress via flag bits
  8. IRQ-Safe: Must handle bio completion callbacks in interrupt context

  ---
  CRITICAL CODE PATHS

  Normal Write Flow

  RAID write → md_account_bio() → md_bitmap_start() → bitmap_start_write()
    ↓
  md_bitmap_get_counter(create=1)
    ↓ [First write to chunk]
  md_bitmap_file_set_bit() → set_bit() → set_page_attr(DIRTY)
    ↓
  Counter: 0 → 2
    ↓ [5 seconds later]
  bitmap_daemon_work() → filemap_write_page() → Flush to disk
    ↓ [Write completes]
  bitmap_end_write() → Counter: 2 → 1 → 0 → Clear bit

  Counter Overflow Recovery

  16,384th concurrent write to same chunk
    ↓
  COUNTER(*bmc) == COUNTER_MAX (16383)
    ↓
  prepare_to_wait(&overflow_wait)
    ↓
  spin_unlock_irq() → schedule() → Sleep
    ↓ [Some write completes]
  bitmap_end_write() → counter-- → wake_up(&overflow_wait)
    ↓
  Wakeup → Retry loop → Success

  Knowledge graph generation complete. This maps all dependencies, key components, and relationships for bitmap_start_write() through
  depth level 4, covering from the core function through the peripheral systems including daemon work, clustering, memory management, and
   I/O completion paths.
````


### RAID1 discard

### write cache for RAID1

- is battery a must for write cache support?

### RAID1 write behind


### RAID1 bitmap

#### How RAID1 Uses Bitmaps for Data Consistency

Answered from AI:

```
RAID1 uses write-intent bitmaps as a mechanism to track which regions of the array might be inconsistent due to incomplete write
operations. Here's how it works:

1. Bitmap Structure and Purpose

The bitmap tracks "dirty" regions of the array that have pending or recently completed write operations. Each bit in the bitmap
corresponds to a chunk of data (typically 64KB-4MB). When a bit is set, it indicates that the corresponding region may be inconsistent
 between mirrors.

2. Write Operation Flow with Bitmap

When a write request comes in (raid1_write_request in /drivers/md/raid1.c:1455), RAID1 follows this sequence:

1. Start Write Tracking (md_write_start at line 1712):
  - Calls mddev->bitmap_ops->start_write() to mark the bitmap region as dirty
  - This happens in bitmap_start_write() which increments a counter for the affected bitmap chunk
2. Write to All Mirrors:
  - RAID1 creates bio clones for each mirror device
  - Submits writes to all working mirrors in parallel
  - Each write has raid1_end_write_request as its completion callback
3. End Write Tracking (close_write() at line 428):
  - Called when all mirror writes complete successfully
  - Calls mddev->bitmap_ops->end_write() to decrement the bitmap counter
  - When counter reaches zero, the bitmap bit can be cleared during the next bitmap sync

3. Key Bitmap Functions

- bitmap_start_write() (md-bitmap.c:1685): Sets the dirty bit and increments a reference counter
- bitmap_end_write() (md-bitmap.c:1739): Decrements the counter; when it reaches zero, marks the chunk as pending for cleanup
- bitmap_dirty_bits(): Explicitly marks regions as dirty during recovery operations

4. Ensuring Consistency

The bitmap ensures consistency in several ways:

1. Write Ordering: Before any data is written to mirrors, the bitmap is updated to mark the region as dirty
2. Reference Counting: Each ongoing write increments a counter; the bit can only be cleared when all writes complete
3. Degraded Mode Handling: If the array is degraded, bitmap bits are marked as "NEEDED" to ensure they're resync'd later
4. Recovery: During resync/recovery, the bitmap identifies which regions need to be synchronized between mirrors

5. Recovery and Resync

When the system restarts after an unclean shutdown:
- bitmap_start_sync() is used during resync to check which regions need synchronization
- Only regions with set bitmap bits are resync'd, making recovery much faster
- The bitmap ensures no data is lost due to partial writes

6. Behind Writes Optimization

RAID1 supports "write-behind" for WriteMostly devices:
- The bitmap tracks behind writes separately (start_behind_write/end_behind_write)
- This allows the array to acknowledge writes before they complete on slower devices
- The bitmap ensures these delayed writes are properly tracked

7. Error Handling

If write errors occur:
- The bitmap keeps the affected regions marked as dirty
- Failed regions will be resync'd during recovery
- This prevents data loss from undetected write failures

The bitmap essentially provides a journal of "in-flight" write operations, ensuring that any region that was being written to during a
 system failure will be properly synchronized during recovery, maintaining data consistency across all mirror devices.

```

#### Questions

- How to deal with sync when creating new raid1? always sync?

- Is counter included in bitmap? No.

one bit per chunk for traditional bitmap, but it becomes one byte for
llbitmap.

- why is bitmap flushed to every underlying disks? why not just flush to
  one disk?

- how to deal with raid1 IO failure?

    -- if one WRITE on single member disk fails, how to handle the failure?

- Why does RAID1 need IO barrier?

    -- for which use cases?


#### How to manage in-memory and on-disk bitmap

- crate bitmap

bitmap_create()     //allocate in-memory bitmap


#### How is bitmap flushed to disk

- Build bitmap data
 
```
md_bitmap_start
    md_clone_bio
        md_account_bio
            raid1_write_request

md_bitmap_end
    md_end_clone_io

.start_write		= bitmap_start_write,
    bitmap_start_write      //update in-memory bitmap and counter
        md_bitmap_file_set_bit

```

- flush bitmap to disk

```
__bitmap_unplug
    md_bitmap_unplug_fn
        INIT_WORK_ONSTACK(&unplug_work.work, md_bitmap_unplug_fn)
            bitmap_unplug_async
                bitmap_unplug
                    .unplug                 = bitmap_unplug
                        mddev->bitmap_ops->unplug
                            bitmap_store
                            raid1_prepare_flush_writes
                                flush_bio_list
                                    flush_pending_writes
                                        freeze_array
                                            raid1_remove_disk
                                                .hot_remove_disk= raid1_remove_disk
                                            handle_read_error
                                                raid1d
                                            raid1_reshape
                                                .check_reshape  = raid1_reshape
                                            raid1_quiesce
                                                .quiesce        = raid1_quiesce
                                        raid1d
                                    raid1_unplug
                                        raid1_add_bio_to_plug(mddev, mbio, raid1_unplug, disks)
                                            raid1_write_request
                                                raid1_make_request
                                flush_pending_writes
                                    freeze_array  //raid1
                                    raid1d
                                    freeze_array  //raid10
                                    raid10d
                                raid10_unplug
                            raid5d
    bitmap_unplug
    bitmap_copy_from_slot
    __bitmap_resize>>


```

#### __bitmap_unplug



## replace raid1 bitmap with simple journal

raid1 uses bitmap for resync/recover in case of non-sync writes, one bit
represents one chunk.

bits are set as 1 during unplug before writing data to underlying disks,
and bits are cleared after writing data to disks successfully.

The recent lockless bitmap improvement increases per-chunk-bits to 8, which
can carry each chunk's status.

When I am looking at this change, I am wondering why this bitmap can't be
replaced with simple journal?

One big drawback for using bitmap this way is that it is really not
friendly for randwrite workloads.  For example, each plug can handle at
most 32 bios from upper layer(FS), if these 32bios belong to 32 chunks,
32 meta IOs can be generated before staring to write data to disks. The
recent lockless bitmap could be worse from this viewpoint because bitmap
size is increased 8 times.

If replacing bitmap with journal, just single sector meta write is required
for randwrite. And the journal usage can be pretty simple, such as,
one entry is 64bit(chunk_seq + nr_chunks) which is enough to cover very big
disk size & any chunk size. Before writing data to underlying disks, all
chunk_seq & nr_chunk from these bios can be built in one buffer, then write
this buffer to journal area in underlying disks. After data write is done,
all these journal entries can be zeroed & written to disks.

resync/recovery can be easy too, just read these entries from journal area,
and run resync.

