---
title: Linux software RAID notes
category: tech
tags: [linux kernel, raid, IO]
---

* TOC
{:toc}

Linux software RAID(redundant array of independent disks) notes

# md-llbitmap

## top view

```
  MD Lockless Bitmap (md-llbitmap) - Top View

  Main Use Case

  The MD lockless bitmap (md-llbitmap) is a high-performance bitmap subsystem for Linux MD (Multiple Device) RAID arrays. It tracks which
   data blocks are synchronized and which need resynchronization/recovery after events like power failures or disk failures. This
  prevents full disk synchronization and significantly improves recovery time and write performance.

  Core Principles

  1. Lockless Fast Path

  - Zero lock overhead for write I/O operations
  - Multiple writes to the same bitmap bit incur overhead only for the first write
  - Subsequent writes to the same bit within a time window (barrier_idle, default 5s) have no additional cost

  2. Lazy Synchronization

  - RAID1/10: Skip synchronization for unwritten blocks (new array creation)
  - RAID4/5/6: Delay initial parity/XOR recovery until first write (MD_RECOVERY_LAZY_RECOVER)
  - Selective Recovery: Only resync/recover written data blocks, not entire disks

  3. State Machine Architecture

  Each bit is one byte (not one bit!) with 5 states:
  - BitUnwritten: No valid data (initial state)
  - BitClean: Data is consistent
  - BitDirty: Data will be consistent after pending I/O completes
  - BitNeedSync: Data needs resynchronization
  - BitSyncing: Data is being synchronized

  Core Idea

  State Machine Transitions

  The bitmap uses an 8-action state machine:

  | State        | StartWrite | StartSync | EndSync | Daemon  | Reload     | Discard     |
  |--------------|------------|-----------|---------|---------|------------|-------------|
  | BitUnwritten | → BitDirty | —         | —       | —       | —          | —           |
  | BitClean     | → BitDirty | —         | —       | —       | —          | → Unwritten |
  | BitDirty     | —          | —         | —       | → Clean | → NeedSync | → Unwritten |
  | BitNeedSync  | —          | → Syncing | —       | —       | —          | → Unwritten |
  | BitSyncing   | —          | Syncing   | → Dirty | —       | → NeedSync | → Unwritten |

  Key Scenarios

  1. New Array with --assume-clean: All bits → BitClean
  2. Normal Write (RAID1/10): Unwritten → Dirty → Clean (after daemon)
  3. First Write RAID5: Unwritten → NeedSync (lazy parity build)
  4. Power Failure Recovery: Dirty → NeedSync (on reload)
  5. Degraded Array: Dirty bits stay dirty; prevents unnecessary resync

  Data Flow

  Write Path (Lockless Fast Path)

  User Write → llbitmap_start_write()
      ↓
  Convert sector → chunk → page index
      ↓
  State Machine: Set bit to Dirty/NeedSync
      ↓
  llbitmap_raise_barrier() - Get percpu_ref (lockless!)
      ↓
  Mark page dirty + set expire time (jiffies + 5s)
      ↓
  User I/O proceeds
      ↓
  llbitmap_end_write() - Release percpu_ref

  Performance Optimization: If multiple bits in the same 512-byte block are modified within barrier_idle, all bits in that block are
  marked dirty at once → reduces write overhead.

  Daemon Path (Periodic Cleanup)

  Timer fires every daemon_sleep (default 30s)
      ↓
  For each page:
      Check if expired (no access for >5s)
          ↓
      llbitmap_suspend_timeout() - Kill percpu_ref, wait for I/O drain
          ↓
      State Machine: Dirty → Clean
          ↓
      Write dirty blocks to disk
          ↓
      llbitmap_resume() - Resurrect percpu_ref

  Resync/Recovery Path

  md_do_sync() → llbitmap_start_sync()
      ↓
  Find NeedSync bits
      ↓
  State: NeedSync → Syncing
      ↓
  Perform data recovery/resync
      ↓
  llbitmap_end_sync() → Syncing → Dirty
      ↓
  Daemon clears: Dirty → Clean

  Optimization: llbitmap_skip_sync_blocks() skips Unwritten blocks, and for non-degraded arrays, skips Clean/Dirty blocks during
  requested resync.

  Code Flow

  Initialization

  llbitmap_create()
      ↓
  llbitmap_check_support() - Validate configuration
      ↓
  llbitmap_read_sb() - Read superblock from disk
      ↓
  llbitmap_cache_pages() - Allocate all pages upfront
      ↓
  llbitmap_init_state() - Initialize all bits

  I/O Synchronization (Lockless Barrier)

  Uses percpu_ref instead of locks:

  1. Raise Barrier (llbitmap_raise_barrier):
    - Try to get percpu_ref (atomic increment per-CPU counter)
    - If dying, wait for daemon to finish
    - Set page expire time
  2. Daemon Suspend (llbitmap_suspend_timeout):
    - Kill percpu_ref (atomic operation)
    - Wait for all references to drop to zero (I/O completion)
    - Clean dirty bits
    - Resurrect percpu_ref

  Bitmap I/O

  - Granularity: Logical block size (typically 512 bytes)
  - Structure:
    - Page 0: Superblock (1KB) + bitmap data
    - Pages 1-N: Bitmap data
  - Dirty Tracking: Per-block dirty bitmap (dirty[]) tracks which 512-byte blocks need flushing
  - Flush: Only dirty blocks are written (not entire pages)

  Key Data Structures

  struct llbitmap {
      struct llbitmap_page_ctl **pctl;  // Array of page controls
      unsigned long chunkshift;          // Shift for chunk size
      unsigned long chunks;              // Total chunks
      struct timer_list pending_timer;   // Daemon timer
      atomic_t behind_writes;            // Write-mostly tracking
  };

  struct llbitmap_page_ctl {
      char *state;                       // One byte per bit!
      struct page *page;                 // Memory-mapped page
      struct percpu_ref active;          // Lockless I/O tracking
      unsigned long dirty[];             // Per-block dirty bitmap
      wait_queue_head_t wait;            // For barrier sync
      unsigned long expire;              // Expiration time
  };

  Comparison with Traditional md-bitmap

  | Feature           | md-bitmap              | md-llbitmap             |
  |-------------------|------------------------|-------------------------|
  | I/O Path Locking  | Spinlock (counts.lock) | Lockless (percpu_ref)   |
  | Bit Size          | 1 bit + 16-bit counter | 1 byte (full state)     |
  | Memory Allocation | On-demand (lazy)       | All pages pre-allocated |
  | Lazy Recovery     | No                     | Yes (RAID5 parity)      |
  | Write Performance | Lock contention        | Near-zero overhead      |
  | Discard Support   | No                     | Yes                     |

  Limitations

  - Requires persistent metadata (no external metadata)
  - No support for dm-raid or bitmap files
  - No support for clustered MD (bitmap version 5)
  - Superblock version: BITMAP_MAJOR_LOCKLESS (6)

  Performance Benefits

  1. Fast Write Path: No spinlock → scales with CPU cores
  2. Batch Dirty Bits: Infect entire 512-byte block → reduces I/O overhead
  3. Lazy Recovery: Skip unnecessary resync for RAID5/6 parity
  4. Selective Sync: Only sync written blocks when replacing disks

  ---
  This module represents a significant evolution from the traditional md-bitmap, designed for modern high-performance RAID workloads with
   minimal synchronization overhead.

```

## infect_dirty_bits

```
llbitmap_start_write
    llbitmap_state_machine 
        llbitmap_write
            llbitmap_set_page_dirty

          /*
           * The subpage usually contains a total of 512 bits. If any single bit
           * within the subpage is marked as dirty, the entire sector will be
           * written. To avoid impacting write performance, when multiple bits
           * within the same sector are modified within llbitmap->barrier_idle,
           * all bits in the sector will be collectively marked as dirty at once.
           */
          if (test_and_set_bit(block, pctl->dirty)) {
                  llbitmap_infect_dirty_bits(llbitmap, pctl, block);
                  return;
          }

```

- 512 chunks are marked as dirty, covered in single sector in this bitmap,
  one chunk takes one byte, for reducing meta data write

- one bitmap page includes 8 sectors, each one has its own status

### related data structure

```
llbitmap_page_ctl: covers one page in bitmap, and tracks each sector status  in this page
```


### bitmap flush

```
llbitmap_unplug_fn
    llbitmap_write_page
        md_write_metadata       /*write subpage to disk*/
```

## md_llbitmap_daemon_fn

```
for (idx = 0; idx < llbitmap->nr_pages; idx++) {
    if (llbitmap_suspend_timeout(llbitmap, idx) < 0) {          //kill per-page percpu_ref
            pr_warn("md/llbitmap: %s: %s waiting for page %d timeout\n",
                    mdname(llbitmap->mddev), __func__, idx);
            continue;
    }
    
    llbitmap_state_machine(llbitmap, start, end, BitmapActionDaemon);
    llbitmap_resume(llbitmap, idx);
}
```


# RAID1

## requirement

- reliability
    
    - deal with bad blocks

    - deal with lost power

- data integrity

## Linux kernel raid1 driver

### RAID1 write

#### raid1_write_request

##### Function calling

![raid1_write_request function calling graph](/assets/images/raid1_write_request_callgraph.jpeg)

```
========================================================================
RAID1 WRITE REQUEST FUNCTION CALL GRAPH - DETAILED NOTES
========================================================================

FILE: raid1_write_request_callgraph.jpeg
Generated: 2025-10-02
Function: raid1_write_request (drivers/md/raid1.c:1475)
Depth: 6+ levels with complete call chains

========================================================================
KEY HIGHLIGHTS
========================================================================

1. CRITICAL PATH (Red edges):
   raid1_write_request 
   -> wait_barrier (sync with resync)
   -> alloc_r1bio (allocate descriptor)
   -> md_account_bio (bitmap tracking)
      -> md_clone_bio
         -> md_bitmap_start
            -> bitmap_ops->start_write
               -> bitmap_start_write (BITMAP CORE)
                  -> md_bitmap_get_counter (get/alloc counter)
                  -> md_bitmap_file_set_bit (set on-disk bit)
   -> r1_bio_write_done (finalize)

2. COLOR CODING:
   - Yellow (#FFD700):     Main entry point (raid1_write_request)
   - Orange (#FFA500):     Key functions at each level
   - Pink (#FF69B4):       Bitmap core functions
   - Light Pink (#DB7093): Bitmap helpers
   - Green (#90EE90):      Callers (submit_bio path)
   - Light Blue (#E6F3FF): Caller cluster background
   
3. EDGE TYPES:
   - Red thick:     Critical path
   - Blue thick:    Completion/return path
   - Purple dashed: Callback connections (bi_end_io)
   - Green dashed:  Async operations (flush)
   - Black normal:  Regular function calls

========================================================================
FUNCTION CALL CHAINS (DEPTH 6)
========================================================================

CHAIN 1: Barrier Synchronization
----------------------------------
Level 0: raid1_write_request
Level 1: wait_barrier (raid1.c:1143)
Level 2: -> sector_to_idx (hash to 1024 buckets)
         -> _wait_barrier (raid1.c:1031)
Level 3:    -> atomic_inc (nr_pending[idx]++)
            -> smp_mb__after_atomic (CRITICAL memory barrier)
            -> wait_event_lock_irq (sleep if barrier set)
            -> atomic_dec (on error)

CHAIN 2: Bitmap Tracking (MAIN FOCUS)
--------------------------------------
Level 0: raid1_write_request
Level 1: md_account_bio (md.c:9123)
Level 2: -> percpu_ref_get (active_io++)
         -> md_clone_bio (md.c:9088)
Level 3:    -> bio_alloc_clone
            -> md_bitmap_enabled
            -> md_bitmap_start (md.c:9055)
Level 4:       -> bitmap_sector (adjust offset if defined)
               -> bitmap_ops->start_write (function pointer)
Level 5:          -> bitmap_start_write (md-bitmap.c:1678) **CORE**
Level 6:             -> md_bitmap_get_counter (md-bitmap.c:1628)
                     -> md_bitmap_file_set_bit (md-bitmap.c:1149)
                     -> md_bitmap_count_page
                     -> (*bmc)++ (counter increment)

CHAIN 3: Bitmap Counter Allocation
-----------------------------------
Level 5: bitmap_start_write
Level 6: md_bitmap_get_counter
Level 7: -> calc page index (offset >> (chunkshift + 11))
         -> md_bitmap_checkpage (md-bitmap.c:253)
         -> calc counter offset (pageoff & 0x7FF)
Level 8:    -> kzalloc(PAGE_SIZE) - allocate 4KB page
            -> hijack pointer (fallback if OOM: 2 counters vs 2048)

CHAIN 4: Bitmap Disk Bit Setting
---------------------------------
Level 5: bitmap_start_write
Level 6: md_bitmap_file_set_bit
Level 7: -> filemap_get_page (get bitmap storage page)
         -> file_page_offset (calculate bit offset)
         -> kmap_local_page
         -> set_bit / set_bit_le (1 BIT on disk vs 16 bits in memory)
         -> kunmap_local_page
         -> set_page_attr (mark DIRTY, triggers flush)
Level 8:    -> bitmap_daemon_work (async flush every 5s)
            -> bitmap_unplug (immediate flush on I/O done)
            -> filemap_write_page (write dirty page to disk)

CHAIN 5: Per-Mirror Bio Cloning
--------------------------------
Level 0: raid1_write_request
Level 1: bio_alloc_clone (per mirror, in loop)
         -> Creates mbio for each mirror device
         -> Sets mbio->bi_end_io = raid1_end_write_request
         -> Sets mbio->bi_private = r1_bio

CHAIN 6: Completion Path
-------------------------
Level 1: r1_bio_write_done (raid1.c:520)
Level 2: -> raid1_end_write_request (raid1.c:459) [recursive call]
Level 3:    -> set_bit (error flags if failure)
            -> rdev_dec_pending (release device reference)
            -> r1_bio_write_done (recursive for remaining)
            -> bio_endio (complete original bio if last)
            -> md_error (handle device failure)

CHAIN 7: Write-Behind Feature
------------------------------
Level 0: raid1_write_request
Level 1: raid1_start_write_behind (raid1.c:1382)
Level 2: -> alloc_behind_master_bio
         -> bitmap_ops->start_behind_write

CHAIN 8: Serialization
----------------------
Level 0: raid1_write_request
Level 1: wait_for_serialization (raid1.c:646)
Level 2: -> wait_event (wait on rdev->serial)

========================================================================
IMPORTANT NOTES
========================================================================

1. MEMORY BARRIERS:
   - smp_mb__after_atomic() in _wait_barrier is CRITICAL
   - Prevents race between normal I/O and raise_barrier()
   - See raid1.c:1031 for detailed explanation

2. BITMAP TWO-LAYER ARCHITECTURE:
   - In-memory: 16-bit counters (tracks # of outstanding writes)
   - On-disk: 1-bit per chunk (dirty/clean flag)
   - Conversion happens in md_bitmap_file_set_bit()

3. COUNTER OVERFLOW HANDLING:
   - Max counter value: COUNTER_MAX = 16383
   - If exceeded: prepare_to_wait() -> schedule() -> sleep
   - Woken by bitmap_end_write() when counter decrements

4. PAGE HIJACKING:
   - Normal: 2048 counters per 4KB page
   - Hijacked: 2 counters stored in pointer value
   - Fallback for memory pressure (OOM)

5. ASYNC FLUSH:
   - Bitmap pages marked DIRTY in memory
   - Flushed periodically (default 5s) by bitmap_daemon_work
   - Or immediately on unplug by bitmap_unplug

6. COMPLETION CALLBACK:
   - Each cloned bio (mbio) has bi_end_io = raid1_end_write_request
   - When device I/O completes, callback invoked
   - Decrements remaining counter
   - When all mirrors done, completes original bio

========================================================================
CALLER HIERARCHY
========================================================================

submit_bio (block layer)
 |
 +-> raid1_make_request (raid1.c:1688)
      |
      +-> raid1_write_request (raid1.c:1475) **THIS FUNCTION**

========================================================================
FUNCTION STATISTICS
========================================================================

Total functions shown: 60+
Maximum depth: 8 levels
Critical path functions: 12
Bitmap-related functions: 15
Synchronization functions: 8
Completion functions: 6

Key bottlenecks:
- wait_barrier: Synchronizes with resync thread
- md_bitmap_checkpage: May allocate memory (slow)
- wait_for_serialization: Serializes writes per device

Performance-critical:
- _wait_barrier: Lock-free fast path (~50 cycles)
- bitmap_start_write: Spinlock + counter increment (~50 cycles)
- md_bitmap_get_counter: Hash lookup (~20 cycles)

========================================================================
CROSS-REFERENCES
========================================================================

Related functions not shown in detail:
- raid1_read_request: Complementary read path
- md_do_sync: Resync thread that interacts with barriers
- raise_barrier: Sets barrier for resync
- lower_barrier: Clears barrier after resync
- bitmap_end_write: Decrements counters (counterpart)

Related files:
- drivers/md/raid1.c: Main RAID1 implementation
- drivers/md/md.c: MD core layer
- drivers/md/md-bitmap.c: Bitmap implementation
- drivers/md/raid1-10.c: Shared RAID1/10 code

========================================================================
END OF NOTES
========================================================================
```


##### Knowledge Graph

```
  Knowledge Graph: raid1_write_request Function

  Level 0: Core Function

  raid1_write_request(mddev, bio, max_write_sectors)
  ├── Purpose: Handle WRITE requests for RAID1 arrays
  ├── Location: drivers/md/raid1.c:1475
  └── Return: void (async completion via bio callbacks)

  Level 1: Major Components & Data Flow

  raid1_write_request
  │
  ├── [1] INPUT VALIDATION & SYNCHRONIZATION
  │   ├── Cluster resync check
  │   ├── wait_barrier() - Barrier synchronization
  │   └── wait_blocked_rdev() - Device blocking check
  │
  ├── [2] R1BIO ALLOCATION & SETUP
  │   ├── alloc_r1bio() - Allocate request descriptor
  │   └── Initialize r1_bio->sectors
  │
  ├── [3] DEVICE SELECTION & BAD BLOCK HANDLING
  │   ├── Iterate all mirrors (disks * 2)
  │   ├── Check device status (Faulty, WriteMostly)
  │   ├── Bad block detection & avoidance
  │   └── Build target device list
  │
  ├── [4] BIO SPLITTING & BEHIND-WRITE SETUP
  │   ├── bio_submit_split_bioset() - Split if needed
  │   ├── raid1_start_write_behind() - Setup write-behind
  │   └── md_account_bio() - Accounting
  │
  ├── [5] BIO CLONING & DISPATCH
  │   ├── Clone bio for each mirror
  │   ├── Set completion callback (raid1_end_write_request)
  │   ├── Add to pending_bio_list or plug
  │   └── Wake raid1d thread
  │
  └── [6] COMPLETION TRACKING
      ├── r1_bio_write_done() - Decrements remaining count
      └── wake_up_barrier() - Unblock waiters

  Level 2: Key Data Structures

  Primary Structures

  struct r1bio (raid1.h:131)
  ├── atomic_t remaining              # Write completion counter
  ├── atomic_t behind_remaining       # Write-behind counter
  ├── sector_t sector                 # Starting sector
  ├── int sectors                     # Number of sectors
  ├── unsigned long state             # State flags (R1BIO_*)
  ├── struct bio *master_bio          # Original bio from upper layer
  ├── struct bio *behind_master_bio   # Write-behind data copy
  └── struct bio *bios[]              # Per-mirror bio array

  struct r1conf (raid1.h:52)
  ├── struct mddev *mddev             # MD device pointer
  ├── struct raid1_info *mirrors      # Mirror device array (2x raid_disks)
  ├── int raid_disks                  # Number of active disks
  ├── spinlock_t device_lock          # Device list lock
  ├── struct bio_list pending_bio_list # Pending writes queue
  ├── wait_queue_head_t wait_barrier  # Barrier wait queue
  ├── atomic_t *nr_pending            # Per-bucket pending count
  ├── atomic_t *nr_waiting            # Per-bucket waiting count
  ├── atomic_t *barrier               # Per-bucket barrier state
  └── mempool_t *r1bio_pool           # r1bio memory pool

  struct raid1_info (raid1.h:41)
  ├── struct md_rdev *rdev            # Device descriptor
  └── sector_t head_position          # Current head position

  State Flags (r1bio->state)

  R1BIO_Uptodate      # I/O completed successfully
  R1BIO_IsSync        # Synchronous I/O
  R1BIO_BehindIO      # Write-behind I/O active
  R1BIO_ReadError     # Read error occurred
  R1BIO_Returned      # Completion callback called
  R1BIO_MadeGood      # Bad block cleared
  R1BIO_WriteError    # Write error occurred
  R1BIO_FailFast      # Fast-fail enabled

  Level 3: Critical Function Dependencies

  Barrier Synchronization System

  wait_barrier(conf, sector_nr, nowait) [raid1.c:1143]
  ├── Purpose: Wait for resync/reshape to clear
  ├── Calls: _wait_barrier(conf, idx, nowait) [raid1.c:1031]
  ├── Uses: sector_to_idx(sector) - Hash sector to bucket
  ├── Blocks on: conf->wait_barrier wait queue
  └── Checks: conf->barrier[idx], conf->nr_pending[idx]

  wake_up_barrier(conf) [raid1.c:1095]
  ├── Purpose: Wake threads waiting on barrier
  └── Wakes: conf->wait_barrier wait queue

  Memory Allocation

  alloc_r1bio(mddev, bio) [raid1.c:118]
  ├── Allocates: sizeof(r1bio) + nr_mirrors * sizeof(struct bio*)
  ├── From: conf->r1bio_pool mempool
  ├── Initializes: master_bio, mddev, sector, state
  └── Returns: struct r1bio*

  bio_alloc_clone(bdev, bio, flags, bioset) [block layer]
  ├── Purpose: Clone bio for each mirror
  ├── Shares: bio_vec data (no data copy)
  └── Independent: bi_iter, bi_end_io, bi_private

  Bad Block Management

  is_badblock(rdev, sector, max_sectors, &first_bad, &bad_sectors)
  ├── Purpose: Check for known bad blocks
  ├── Returns: 0=good, 1=bad block found, -1=error
  ├── Updates: first_bad (first bad sector), bad_sectors (count)
  └── Logic: Adjust max_sectors to avoid bad blocks

  Write-Behind Support

  raid1_start_write_behind(mddev, r1_bio, bio) [raid1.c:1454]
  ├── Checks: md_bitmap_enabled(), behind_writes limit
  ├── Calls: alloc_behind_master_bio() - Copy data to pages
  ├── Sets: R1BIO_BehindIO flag
  └── Calls: bitmap_ops->start_behind_write()

  alloc_behind_master_bio(r1_bio, bio)
  ├── Allocates: Pages for data copy
  ├── Copies: bio data to allocated pages
  └── Stores: In r1_bio->behind_master_bio

  Completion Path

  raid1_end_write_request(bio) [raid1.c:447]
  ├── Called: By block layer on bio completion
  ├── Checks: Write errors, bad block updates
  ├── Updates: rdev statistics
  ├── Calls: r1_bio_write_done(r1_bio)
  └── Handles: Behind writes, error reporting

  r1_bio_write_done(r1_bio) [raid1.c:431]
  ├── Decrements: atomic_dec_and_test(&r1_bio->remaining)
  ├── On zero:
  │   ├── close_write(r1_bio) - Cleanup bitmap tracking
  │   ├── reschedule_retry() - If errors occurred
  │   └── raid_end_bio_io() - Complete original bio
  └── Purpose: Track all mirror write completions

  close_write(r1_bio) [raid1.c:415]
  ├── Calls: bitmap_ops->end_behind_write()
  ├── Calls: md_write_end(mddev)
  └── Purpose: End bitmap/metadata tracking

  Level 4: Deep Dependencies & External Interactions

  Bitmap Operations

  md_write_start(mddev, bio) [md.c:8779]
  ├── Called: Implicitly before write
  ├── Sets: mddev->in_sync = 0
  ├── Updates: writes_pending counter
  └── Waits: For superblock updates if needed

  bitmap_ops->start_write(mddev, offset, sectors) [md-bitmap.c:1685]
  ├── Gets: bitmap counter for chunk
  ├── Increments: Reference count
  ├── Calls: md_bitmap_file_set_bit() - Set disk bit
  └── Sets: BITMAP_PAGE_DIRTY flag

  bitmap_ops->end_write(mddev, offset, sectors) [md-bitmap.c:1739]
  ├── Decrements: Bitmap counter
  ├── If counter ≤ 2: Mark for cleanup
  └── Updates: events_cleared if array clean

  Bio Submission Paths

  raid1_add_bio_to_plug(mddev, mbio, raid1_unplug, disks)
  ├── Purpose: Try to add bio to current task's plug
  ├── Returns: true if plugged, false if needs immediate submit
  └── Avoids: Lock contention when plugging available

  Alternative: bio_list_add(&conf->pending_bio_list, mbio)
  ├── Protected by: conf->device_lock
  ├── Wakes: mddev->thread (raid1d)
  └── Processed by: raid1d daemon thread

  raid1d thread processing:
  └── flush_pending_writes(conf) [raid1.c:915]
      └── flush_bio_list(conf, bio) [raid1.c:900]
          ├── raid1_prepare_flush_writes() - Flush bitmap
          └── raid1_submit_write(bio) - Submit to block layer

  Cluster Integration

  mddev->cluster_ops->area_resyncing(mddev, WRITE, sector, end)
  ├── Purpose: Check if cluster node is resyncing this area
  ├── Returns: true if resync active
  └── Action: Wait on conf->wait_barrier if active

  Error Handling & Retry

  reschedule_retry(r1_bio) [raid1.c:1990]
  ├── Adds: r1_bio to conf->retry_list
  ├── Wakes: raid1d thread
  └── Purpose: Retry failed writes or update bad blocks

  raid_end_bio_io(r1_bio) [raid1-10.c:139]
  ├── Calls: bio_endio(master_bio)
  ├── Frees: r1_bio back to pool
  └── Purpose: Complete original bio to upper layer

  Device State Management

  Device Flags (in rdev->flags):
  ├── Faulty        # Device failed, skip writes
  ├── WriteMostly   # Prefer other devices for reads
  ├── WriteErrorSeen # Known write errors, check bad blocks
  ├── FailFast      # Don't retry, fail quickly
  └── CollisionCheck # Serialize writes (for write-behind)

  atomic_inc(&rdev->nr_pending)
  ├── Purpose: Prevent device removal during I/O
  └── Paired with: rdev_dec_pending() on completion

  Barrier Bucket System

  Barrier Buckets (64MB units):
  ├── Purpose: Fine-grained resync/write synchronization
  ├── Hash: sector_to_idx(sector >> BARRIER_UNIT_SECTOR_BITS)
  ├── Size: PAGE_SIZE / sizeof(atomic_t) buckets
  └── Tracks: nr_pending, nr_waiting, nr_queued, barrier per bucket

  Barrier Logic:
  ├── conf->barrier[idx] > 0    # Resync in progress, block writes
  ├── conf->nr_pending[idx]     # Active writes in this bucket
  └── conf->nr_waiting[idx]     # Threads waiting for bucket

  Write-Behind Mechanics

  Write-Behind Flow:
  1. Check: WriteMostly flag on device
  2. Allocate: behind_master_bio (data copy)
  3. Submit: Regular mirrors immediately
  4. Submit: WriteMostly mirrors with behind_master_bio
  5. Complete: When regular mirrors finish (fast path)
  6. Track: behind_remaining counter for WriteMostly completion
  7. Cleanup: Free behind_master_bio when all complete

  Benefits:
  ├── Low-latency completion for fast mirrors
  ├── Slow mirrors don't block application
  └── Bitmap tracking ensures data safety

  Data Flow Diagram

  Application Bio
        ↓
  raid1_make_request
        ↓
  raid1_write_request ──────────────────────────────┐
        ↓                                           ↓
  [Cluster Check] → wait if resyncing        md_write_start
        ↓                                           ↓
  [wait_barrier] → block if resync       bitmap_start_write
        ↓                                           ↓
  [alloc_r1bio]                          Set bitmap bits
        ↓
  [Select Mirrors] → Check Faulty/WriteMostly/BadBlocks
        ↓
  [Clone Bios] → bio_alloc_clone for each mirror
        ↓
  [Setup Callbacks] → raid1_end_write_request
        ↓
  [Dispatch] ───┬──→ raid1_add_bio_to_plug → Immediate submit
                └──→ pending_bio_list → raid1d → Submit later

        ↓ (async)
  [Block Layer] → Physical I/O to each mirror
        ↓
  raid1_end_write_request (per mirror completion)
        ↓
  r1_bio_write_done (atomic_dec remaining)
        ↓
  [All mirrors done?] ──No──→ return
        ↓ Yes
  close_write
        ↓
  bitmap_end_write
        ↓
  raid_end_bio_io → Complete original bio
        ↓
  Application notified

  Performance-Critical Paths

  1. Fast Path (no contention):
    - wait_barrier() → immediate pass
    - bio_alloc_clone() → from mempool
    - raid1_add_bio_to_plug() → plug successful
    - Parallel writes to all mirrors
  2. Slow Path (contention):
    - wait_barrier() → sleep on wait queue
    - Bad block detection → sector adjustment
    - Bio splitting → recursive submission
    - pending_bio_list → serialized through raid1d
  3. Bitmap Impact:
    - bitmap_start_write() → per-chunk bit set
    - Multiple random writes → multiple bitmap pages dirty
    - This is where your journal idea provides huge wins!

```

#### raid1_unplug

##### function call

![raid1_unplug() function call](/assets/images/raid1_unplug_callgraph.png)


#### raid1_end_write_request

##### function call

![raid1_end_write_callgraph function call](/assets/images/raid1_end_write_callgraph.png)


#### wait_barrier

```
  Knowledge Graph: wait_barrier() Function

  Level 0: Core Function

  wait_barrier(conf, sector_nr, nowait)
  ├── Purpose: Synchronize write I/O with resync/recovery operations
  ├── Location: drivers/md/raid1.c:1143
  ├── Returns: bool (true=proceed, false=would block with nowait)
  └── Role: Gate-keeper for write I/O during resync/management operations

  Level 1: Function Call Chain & Architecture

  wait_barrier(conf, sector_nr, nowait)
  ├── [1] sector_to_idx(sector_nr) → Convert sector to bucket index
  │   └── Returns: idx (0 to BARRIER_BUCKETS_NR-1)
  │
  └── [2] _wait_barrier(conf, idx, nowait) → Core barrier logic
      ├── Fast path: No barrier, return immediately
      ├── Slow path: Wait for barrier to drop
      └── Returns: bool (success/failure)

  Level 2: Core Implementation - _wait_barrier()

  _wait_barrier(conf, idx, nowait) [raid1.c:1031]
  │
  ├── PHASE 1: OPTIMISTIC INCREMENT
  │   ├── atomic_inc(&conf->nr_pending[idx])
  │   ├── smp_mb__after_atomic() - Memory barrier
  │   └── Purpose: Signal "I'm here" before checking barrier
  │
  ├── PHASE 2: FAST PATH CHECK
  │   ├── Check: !conf->array_frozen
  │   ├── Check: !conf->barrier[idx]
  │   └── If both true → return immediately (fast path)
  │
  ├── PHASE 3: SLOW PATH (barrier active or array frozen)
  │   ├── spin_lock_irq(&conf->resync_lock)
  │   ├── atomic_inc(&conf->nr_waiting[idx])
  │   ├── atomic_dec(&conf->nr_pending[idx])
  │   ├── wake_up_barrier(conf) - Wake freeze_array() if waiting
  │   │
  │   ├── [DECISION POINT]
  │   │   ├── If nowait=true → ret=false, skip wait
  │   │   └── If nowait=false → wait_event_lock_irq(...)
  │   │
  │   ├── Wait on: conf->wait_barrier
  │   ├── Condition: !conf->array_frozen && !conf->barrier[idx]
  │   ├── Hold lock: conf->resync_lock (released during sleep)
  │   │
  │   ├── On wakeup: atomic_inc(&conf->nr_pending[idx])
  │   ├── atomic_dec(&conf->nr_waiting[idx])
  │   └── spin_unlock_irq(&conf->resync_lock)
  │
  └── PHASE 4: RETURN
      └── Return bool (true=proceed, false=blocked with nowait)

  Level 3: The Barrier Bucket System

  Bucket Architecture

  BARRIER SYSTEM (64MB units per bucket)
  ├── BARRIER_UNIT_SECTOR_BITS = 17
  ├── BARRIER_UNIT_SECTOR_SIZE = 128K sectors = 64MB
  ├── BARRIER_BUCKETS_NR_BITS = PAGE_SHIFT - 2 (typically 10)
  ├── BARRIER_BUCKETS_NR = 1024 (on 4KB page systems)
  └── Total coverage: 1024 buckets × 64MB = 64GB range

  sector_to_idx(sector) [raid1.h:194]
  └── hash_long(sector >> 17, 10) → bucket index 0-1023

  Per-Bucket State Tracking

  Each bucket (idx) tracks:

  conf->nr_pending[idx]     # Active I/Os in this bucket
  ├── Incremented: When I/O enters (wait_barrier)
  ├── Decremented: When I/O completes (allow_barrier)
  └── Purpose: Track active normal I/O count

  conf->nr_waiting[idx]     # Threads waiting on barrier
  ├── Incremented: When thread sleeps in _wait_barrier()
  ├── Decremented: When thread wakes up
  └── Purpose: Track waiting I/O count

  conf->nr_queued[idx]      # Queued I/Os (during freeze)
  ├── Incremented: When I/O queued to pending_bio_list
  ├── Decremented: When I/O submitted from queue
  └── Purpose: Track queued I/O count

  conf->barrier[idx]        # Barrier level (resync active)
  ├── Incremented: In raise_barrier() (resync/recovery)
  ├── Decremented: In lower_barrier() (resync done)
  ├── Range: 0 (no barrier) to RESYNC_DEPTH (max concurrent)
  └── Purpose: Block normal I/O when resync active

  Global State

  conf->array_frozen        # Array management freeze
  ├── Set: freeze_array() - Management operations
  ├── Clear: unfreeze_array() - Resume normal operations
  └── Blocks: ALL I/O (read and write)

  conf->nr_sync_pending     # Total sync I/Os active
  ├── Incremented: raise_barrier()
  ├── Decremented: lower_barrier()
  └── Purpose: Track resync/recovery I/O count

  Level 4: Deep Dependencies & Synchronization Protocols

  Memory Ordering Protocol

  CRITICAL RACE PREVENTION (between _wait_barrier and raise_barrier)

  Thread A (_wait_barrier):              Thread B (raise_barrier):
  1. atomic_inc(nr_pending[idx])         1. atomic_inc(barrier[idx])
  2. smp_mb__after_atomic() ────────────→2. smp_mb__after_atomic()
  3. if (barrier[idx] == 0)              3. if (nr_pending[idx] == 0)
       return (fast path)                     continue (can raise)

  Without memory barriers:
  - Thread A might read old barrier[idx]=0 (before B's increment)
  - Thread B might read old nr_pending[idx]=0 (before A's increment)
  - BOTH proceed → RACE! (I/O happens during resync)

  With memory barriers:
  - At least ONE thread sees the other's update
  - Either A waits OR B waits → NO RACE

  Lock-Free Fast Path

  Optimization: Avoid locks when no barrier

  _wait_barrier() fast path:
  ├── atomic_inc(&nr_pending[idx])        # Lock-free
  ├── smp_mb__after_atomic()              # Memory fence
  ├── Check: !array_frozen && !barrier[idx]
  └── If true: Return without taking lock  # FAST!

  Statistics:
  - Normal operation (no resync): 99.9% fast path
  - During resync: Only affected buckets take slow path
  - Performance: Lock-free increment + read + branch

  Wait Queue Integration

  conf->wait_barrier (wait_queue_head_t)
  ├── Purpose: Sleep/wake synchronization point
  │
  ├── Waiters:
  │   ├── _wait_barrier() - Normal I/O waiting for barrier drop
  │   ├── wait_read_barrier() - Read I/O waiting for unfreeze
  │   ├── raise_barrier() - Resync waiting for I/O drain
  │   └── freeze_array() - Management waiting for I/O drain
  │
  ├── Wakers:
  │   ├── wake_up_barrier() - Check and wake if sleepers
  │   ├── allow_barrier() - I/O completed
  │   ├── lower_barrier() - Resync completed
  │   ├── unfreeze_array() - Unfreeze completed
  │   └── flush_bio_list() - Queued I/O processing
  │
  └── Lock held during wait: conf->resync_lock

  Interaction with raise_barrier()

  raise_barrier(conf, sector_nr) [raid1.c:970]
  ├── Purpose: Block normal I/O to allow resync
  │
  ├── PHASE 1: Wait for no waiting threads
  │   └── wait_event_lock_irq(!nr_waiting[idx])
  │
  ├── PHASE 2: Set barrier
  │   ├── atomic_inc(&barrier[idx])
  │   └── smp_mb__after_atomic()
  │
  ├── PHASE 3: Wait for I/O drain
  │   └── wait_event_lock_irq(
  │           !array_frozen &&
  │           !nr_pending[idx] &&
  │           barrier[idx] < RESYNC_DEPTH)
  │
  └── PHASE 4: Success
      └── atomic_inc(&nr_sync_pending)

  Coordination with _wait_barrier():
  - raise_barrier sets barrier[idx]=1
  - _wait_barrier sees barrier[idx]≠0
  - _wait_barrier waits until barrier[idx]=0
  - lower_barrier() decrements barrier[idx]
  - _wait_barrier() wakes and proceeds

  Interaction with freeze_array()

  freeze_array(conf, extra) [raid1.c:1176]
  ├── Purpose: Quiesce ALL I/O for management operations
  │
  ├── PHASE 1: Set freeze flag
  │   └── conf->array_frozen = 1
  │
  ├── PHASE 2: Wait for I/O drain
  │   └── wait_event_lock_irq_cmd(
  │           get_unqueued_pending() == extra,
  │           flush_pending_writes())
  │
  └── Purpose: Drain all flying I/O

  get_unqueued_pending(conf) [raid1.c:1164]
  ├── ret = nr_sync_pending
  ├── for each bucket:
  │   └── ret += (nr_pending[idx] - nr_queued[idx])
  └── Returns: Count of I/O not yet queued

  _wait_barrier() cooperation:
  - Sees array_frozen=1
  - Moves to slow path
  - Decrements nr_pending (helps drain)
  - Increments nr_waiting
  - Wakes freeze_array() (via wake_up_barrier)
  - Waits for unfreeze

  Interaction with allow_barrier()

  allow_barrier(conf, sector_nr) [raid1.c:1156]
  ├── Calls: _allow_barrier(conf, idx)
  └── Purpose: Signal I/O completion

  _allow_barrier(conf, idx) [raid1.c:1150]
  ├── atomic_dec(&nr_pending[idx])
  ├── wake_up_barrier(conf)
  └── Purpose: Release barrier slot

  Typical I/O lifecycle:
  1. wait_barrier() → inc nr_pending[idx]
  2. Submit I/O to devices
  3. I/O completes
  4. allow_barrier() → dec nr_pending[idx]
  5. If resync waiting: wake_up allows it to proceed

  NOWAIT Flag Handling

  NOWAIT Support (for REQ_NOWAIT bios):

  Traditional blocking I/O:
  ├── wait_barrier(conf, sector, false)
  ├── If barrier active: Sleep until clear
  └── Always returns true

  Non-blocking I/O (NOWAIT):
  ├── wait_barrier(conf, sector, true)
  ├── If barrier active: Return false immediately
  └── Caller must handle failure (return -EAGAIN to user)

  Code flow:
  if (!wait_barrier(conf, sector, bio->bi_opf & REQ_NOWAIT)) {
      bio_wouldblock_error(bio);  # Return -EAGAIN
      return;
  }

  Purpose: Support async I/O (io_uring, AIO)

  State Transition Diagrams

  Normal I/O Flow

  START
    ↓
  wait_barrier(conf, sector, nowait)
    ↓
  sector_to_idx(sector) → idx
    ↓
  _wait_barrier(conf, idx, nowait)
    ↓
  atomic_inc(nr_pending[idx])
    ↓
  smp_mb__after_atomic()
    ↓
  [CHECK: array_frozen || barrier[idx]]
    ↓                           ↓
    NO (Fast Path)             YES (Slow Path)
    ↓                           ↓
    Return true                 spin_lock_irq(resync_lock)
    ↓                           ↓
  [Proceed with I/O]           atomic_inc(nr_waiting[idx])
    ↓                           atomic_dec(nr_pending[idx])
  [I/O Completes]               wake_up_barrier(conf)
    ↓                           ↓
  allow_barrier(conf, sector)  [NOWAIT?]
    ↓                           ↓              ↓
  atomic_dec(nr_pending[idx])  NO             YES
  wake_up_barrier(conf)         ↓              ↓
    ↓                          wait_event...   Return false
  DONE                          ↓              ↓
                           [Wait for clear]   [Caller handles]
                                ↓
                           atomic_inc(nr_pending[idx])
                           atomic_dec(nr_waiting[idx])
                           spin_unlock_irq(resync_lock)
                                ↓
                           Return true
                                ↓
                           [Proceed with I/O]

  Resync Coordination

  NORMAL I/O THREAD              RESYNC THREAD
        ↓                              ↓
  wait_barrier(sector)          raise_barrier(sector)
        ↓                              ↓
  atomic_inc(nr_pending[idx])   wait(!nr_waiting[idx])
        ↓                              ↓
  smp_mb() ←───────────────────→ atomic_inc(barrier[idx])
        ↓                              ↓
  Check barrier[idx]             smp_mb()
        ↓                              ↓
  If != 0:                       wait(!nr_pending[idx])
    ↓                                  ↓
    Wait on barrier[idx]=0       [When nr_pending=0]
    ↓                                  ↓
  [Barrier drops]               Do resync I/O
    ↓                                  ↓
  Proceed with I/O              lower_barrier()
    ↓                                  ↓
  allow_barrier()               atomic_dec(barrier[idx])
                                       ↓
                                wake_up_barrier()
                                       ↓
                                [Normal I/O unblocked]

  Performance Characteristics

  Fast Path (No Contention)

  Cost: ~10-20 CPU cycles
  ├── 1 atomic increment (nr_pending)
  ├── 1 memory barrier (smp_mb)
  ├── 2 atomic reads (array_frozen, barrier)
  └── 1 conditional branch

  Slow Path (Barrier Active)

  Cost: Context switch + wait time
  ├── 1 spinlock acquire/release
  ├── 2 atomic operations (nr_waiting, nr_pending)
  ├── 1 wait queue sleep/wake
  └── Wait duration: Until resync clears bucket

  Bucket Hash Benefits

  Parallelism: 1024 independent buckets
  ├── Resync in bucket 0: Only bucket 0 I/O blocks
  ├── I/O to other buckets: Fast path continues
  └── Effective concurrency: ~1024-way parallel

  Error Scenarios

  NOWAIT Failure

  Trigger: REQ_NOWAIT bio hits active barrier
  Flow:
  ├── wait_barrier(conf, sector, true)
  ├── _wait_barrier() returns false
  ├── bio_wouldblock_error(bio)
  └── Return -EAGAIN to application

  Recovery Interrupt

  Trigger: MD_RECOVERY_INTR set during resync
  Flow (in raise_barrier):
  ├── detect MD_RECOVERY_INTR
  ├── atomic_dec(&barrier[idx])
  ├── wake_up(&wait_barrier)
  └── Return -EINTR

```


#### raid1d thread

#### function call

![raid1d() overview](/assets/images/raid1d_overview.png)

```

================================================================================
                    RAID1D FUNCTION CALL GRAPH NOTES
================================================================================

OVERVIEW
--------
raid1d() is the main daemon thread for Linux MD RAID1 arrays. It handles:
  • Retry logic for failed I/O operations
  • Error recovery (read/write errors)
  • Sync and resync operations
  • Device failure handling
  • Metadata updates

INVOCATION MECHANISM
--------------------
Registered: md_register_thread(raid1d, mddev, "raid1") at raid1.c:3187
Executor: md_thread() kernel thread calls thread->run(thread) → raid1d()
Wakeup:   md_wakeup_thread() called from:
  • Bio completion handlers (raid1_end_write_request, raid1_end_read_request)
  • reschedule_retry() - schedules r1bio for retry
  • md_error() - device failure notification
  • Bitmap operations
  • Userspace sysfs operations

DATA STRUCTURES
---------------
r1bio: Per-bio RAID1 tracking structure
  • master_bio    - Original bio from upper layer
  • bios[]        - Per-device bio array
  • state         - R1BIO_* flags (WriteError, ReadError, IsSync, MadeGood)
  • remaining     - Atomic counter for outstanding operations
  • sector        - Starting sector
  • sectors       - Length in sectors

r1conf: RAID1 array configuration
  • retry_list        - List of r1bios needing retry (processed by raid1d)
  • bio_end_io_list   - List of r1bios ready for completion
  • pending_bio_list  - Writes waiting for bitmap flush
  • mirrors[]         - Per-device mirror information
  • nr_pending[]      - Pending I/O count per hash bucket
  • nr_queued[]       - Queued retry count per hash bucket
  • barrier[]         - Barrier state per bucket (for freeze/thaw)


EXECUTION FLOW
--------------
1. md_check_recovery() - Handle array recovery state, metadata updates
2. Process bio_end_io_list:
   - Complete finished bios
   - If R1BIO_WriteError: call close_write()
   - Call raid_end_bio_io() to complete to upper layer
3. Main retry loop (until retry_list empty):
   a. flush_pending_writes() - Flush bitmap, submit queued writes
   b. Get r1bio from retry_list
   c. Dispatch based on r1bio->state:
      • R1BIO_IsSync + (MadeGood|WriteError) → handle_sync_write_finished()
      • R1BIO_IsSync → sync_request_write()
      • MadeGood|WriteError → handle_write_finished()
      • ReadError → handle_read_error()
   d. cond_resched() - Yield CPU
   e. Check metadata flags and call md_check_recovery() if needed


IMPORTANT FUNCTIONS (by category)
----------------------------------

=== RECOVERY & CORE ===

md_check_recovery() - drivers/md/md.c:10003
  Role: Manages array recovery state, metadata updates, sync thread lifecycle
  Calls:
    → bitmap_ops->daemon_work() - Process bitmap updates
    → md_update_sb() - Write superblock to devices
    → md_reap_sync_thread() - Cleanup completed sync thread
    → md_start_sync() - Start new sync/recovery operation
  Data Flow: Checks mddev state → Updates metadata → Manages sync threads


=== I/O PROCESSING ===

flush_pending_writes() - drivers/md/raid1.c:915
  Role: Flushes writes queued waiting for bitmap updates
  Calls:
    → raid1_prepare_flush_writes() - Triggers bitmap unplug
    → flush_bio_list() - Submits pending bios
    → raid1_submit_write() - Submits write to device
  Data Flow: conf->pending_bio_list → bitmap flush → submit_bio_noacct()
  Critical: Ensures write-intent bitmap consistency before I/O submission

raid_end_bio_io() - drivers/md/raid1.c:309
  Role: Completes bio to upper layer, frees resources
  Calls:
    → call_bio_endio() - Invokes bio completion callback
    → free_r1bio() - Returns r1bio to mempool
    → allow_barrier() - Decrements barrier count, wakes waiters
  Data Flow: r1bio → bio_endio(master_bio) → free resources → wake waiters

close_write() - drivers/md/raid1.c:415
  Role: Finalizes write operations, cleans up behind-write
  Calls:
    → bio_free_pages() - Free allocated pages
    → bitmap_ops->end_behind_write() - Update behind-write counter
    → md_write_end() - Decrement write counter, update safemode timer
  Data Flow: r1bio → cleanup behind-write → update counters → safemode timer


=== SYNC HANDLING ===

sync_request_write() - drivers/md/raid1.c:2336
  Role: Writes corrected data during resync/recovery
  Context: Called when R1BIO_IsSync flag set (sync/resync operation)
  Calls:
    → fix_sync_read_error() - Fix read errors by trying other mirrors
    → process_checks() - Compare data across mirrors (check/repair)
    → abort_sync_write() - Abort on unrecoverable errors
    → submit_bio_noacct() - Submit corrected data writes
  Data Flow: Read from good mirror → Compare/fix → Write to degraded mirrors
  Special: Handles both resync (rebuild) and check/repair operations

handle_sync_write_finished() - drivers/md/raid1.c:2559
  Role: Handles completion of sync write operations
  Context: Called when sync write completes (success or failure)
  Calls:
    → rdev_clear_badblocks() - Clear bad blocks on successful write
    → rdev_set_badblocks() - Mark bad blocks on failed write
    → md_error() - Report device failure
    → md_done_sync() - Signal sync progress to MD layer
  Data Flow: Check write status → Update bad block list → Update progress


=== ERROR HANDLING ===

handle_read_error() - drivers/md/raid1.c:2626
  Role: Handles read errors by retrying on alternate mirrors
  Critical Path: Primary error recovery mechanism for reads
  Calls:
    → freeze_array() - Stop new I/O to array
    → fix_read_error() - Attempt read from other mirrors
    → unfreeze_array() - Resume normal I/O
    → raid1_read_request() - Reissue read from good mirror
  Data Flow:
    1. freeze_array() - Raise barrier, wait for pending I/O
    2. fix_read_error() - Try reading from all mirrors
    3. If successful: Write correct data back, clear bad blocks
    4. If failed: Mark device bad, call md_error()
    5. unfreeze_array() - Lower barrier, wake waiters
    6. Reissue read via raid1_read_request()

fix_read_error() - drivers/md/raid1.c:2399
  Role: Attempts to read from alternate mirrors and repair
  Calls:
    → exceed_read_errors() - Check error threshold
    → sync_page_io() - Synchronous read from mirror
    → r1_sync_page_io() - RAID1 wrapper with error handling
    → rdev_set_badblocks() - Mark failed sectors
  Data Flow: Try each mirror → If success: write back to failed → Clear/set badblocks

handle_write_finished() - drivers/md/raid1.c:2582
  Role: Processes finished writes with potential errors
  Context: Called for non-sync writes with WriteError or MadeGood flags
  Calls:
    → narrow_write_error() - Isolate bad sectors via binary search
    → rdev_clear_badblocks() - Clear bad blocks on MadeGood
    → md_error() - Report persistent failures
    → raid_end_bio_io() - Complete bio if successful
  Data Flow:
    1. If MadeGood: clear bad blocks, complete bio
    2. If WriteError: narrow_write_error() to find exact bad sectors
    3. Mark bad blocks or fail device
    4. Complete or reschedule bio

narrow_write_error() - drivers/md/raid1.c:2490
  Role: Pinpoints exact bad sectors by retrying smaller chunks
  Method: Binary search on sector range
  Calls:
    → bio_alloc_clone() - Clone original bio
    → bio_trim() - Trim to specific sector range
    → submit_bio_wait() - Synchronous retry
    → rdev_set_badblocks() - Mark bad sectors
  Data Flow: Clone bio → Trim to sectors → Sync retry → Mark bad on failure
  Special: Minimizes bad block marking to exact failed sectors


CRITICAL PATHS & ERROR SCENARIOS
---------------------------------

Write Error Path:
  raid1_end_write_request (error detected)
    → set R1BIO_WriteError flag
    → reschedule_retry(r1_bio) - Add to retry_list, wake raid1d
    → raid1d: handle_write_finished()
    → narrow_write_error() - Isolate bad sectors
    → rdev_set_badblocks() or md_error()
    → Complete bio (if salvageable) or fail device

Read Error Path:
  raid1_end_read_request (error detected)
    → set R1BIO_ReadError flag
    → reschedule_retry(r1_bio)
    → raid1d: handle_read_error()
    → freeze_array() - Stop normal I/O
    → fix_read_error() - Read from other mirrors
    → If success: Write correct data back, update bad blocks
    → unfreeze_array() - Resume normal I/O
    → raid1_read_request() - Reissue from good mirror
    → Complete bio

Sync Write Error Path:
  end_sync_write (error detected)
    → set R1BIO_WriteError flag
    → reschedule_retry(r1_bio)
    → raid1d: handle_sync_write_finished()
    → rdev_set_badblocks() or md_error()
    → md_done_sync() - Update sync progress


SYNCHRONIZATION MECHANISMS
---------------------------

Barrier System (Freeze/Thaw):
  • Purpose: Stop normal I/O during error recovery
  • Implementation: Per-bucket barrier counters (BARRIER_BUCKETS_NR)
  • freeze_array(): Raise barrier, wait for nr_pending[] to drain
  • unfreeze_array(): Lower barrier, wake up waiting I/O
  • Used by: Error recovery, array reconfiguration

Wait Queues:
  • conf->wait_barrier - Waits for barrier clear (normal I/O waits here)
  • mddev->recovery_wait - Waits for recovery progress
  • mddev->sb_wait - Waits for superblock updates

Locks:
  • conf->device_lock - Protects retry lists, pending counts
  • conf->resync_lock - Protects resync/barrier operations
  • mddev->reconfig_mutex - Protects array reconfiguration

Atomic Counters:
  • conf->nr_pending[] - Pending I/O count per bucket (for barrier)
  • conf->nr_queued[] - Queued retry count per bucket
  • r1_bio->remaining - Outstanding operations for this r1bio


PERFORMANCE CONSIDERATIONS
---------------------------

Batching: Uses blk_plug to batch I/O submissions for better performance

CPU Fairness: Calls cond_resched() to yield CPU and prevent monopolization

Lock Granularity: Uses BARRIER_BUCKETS_NR (256) buckets to reduce contention
  • Hash by sector: sector_to_idx(sector) = sector % BARRIER_BUCKETS_NR
  • Allows parallel I/O to different sector ranges

Mempool: Uses mempool for r1bio allocation to prevent deadlock under memory
  pressure (guaranteed allocation for in-flight I/O completion)

Bitmap Optimization: Groups bitmap updates and flushes to reduce I/O overhead

Behind-Write: Allows buffering writes when device is slower (WriteM ostly)


KEY DESIGN PATTERNS
-------------------

Retry Queue Pattern:
  • Failed I/O placed on retry_list
  • raid1d processes retry_list in dedicated thread context
  • Allows complex error recovery without blocking bio completion

State Machine:
  • r1bio->state flags determine processing path
  • R1BIO_IsSync, R1BIO_ReadError, R1BIO_WriteError, R1BIO_MadeGood
  • Simplifies dispatch logic in raid1d main loop

Two-Phase Completion:
  • Phase 1: Bio completion handler (interrupt/softirq context)
    - Set state flags, add to retry_list
  • Phase 2: raid1d thread (process context)
    - Complex error recovery, metadata updates

Reference Counting:
  • r1bio->remaining counts outstanding operations
  • Atomic decrement, last one triggers completion
  • Handles parallel operations on multiple mirrors


USAGE CONTEXTS
--------------

Normal Operation:
  • Processes retries from bio_end_io_list (quick completion path)
  • Flushes pending writes waiting for bitmap
  • Minimal overhead, mostly sleeping

Error Recovery:
  • Read errors: Tries alternate mirrors, repairs data
  • Write errors: Narrows to bad sectors, updates bad block list
  • May freeze array temporarily for data consistency

Resync/Recovery:
  • Processes sync writes (rebuilding degraded mirrors)
  • Handles sync read errors via fix_sync_read_error()
  • Updates bitmap and sync progress

Array Reconfiguration:
  • Handles metadata updates via md_check_recovery()
  • Manages sync thread lifecycle
  • Coordinates with userspace management tools


NOTES ON DEPTH TRACKING
------------------------
The call graph tracks functions to depth 6, skipping:
  • Generic kernel APIs: kmalloc, kfree, spin_lock, mutex_lock
  • Block layer generics: submit_bio, bio_alloc (except where relevant)
  • List/atomic operations: list_add, atomic_inc, etc.
  • Simple utilities: memset, memcpy, etc.

Focus is on RAID1-specific and MD layer functions that implement core logic.

================================================================================
                              END OF NOTES
================================================================================
```



#### resync thread

```
  RAID1 Resync Thread: When, How, and Why

  What is the Resync Thread?

  The resync thread is a kernel thread that synchronizes data between RAID1 mirrors. It runs in the background to ensure all mirror
  devices contain identical data.

  When is Resync Triggered?

  1. Device Failures & Errors

  // Write error during normal I/O (raid1.c:2091-2094)
  set_bit(WriteErrorSeen, &rdev->flags);
  if (!test_and_set_bit(WantReplacement, &rdev->flags))
      set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);

  // Unrecoverable read error (raid1.c:2352)
  set_bit(MD_RECOVERY_INTR, &mddev->recovery);

  Triggers:
  - Write error on a mirror device
  - Device marked for replacement
  - Persistent read errors requiring rebuild

  2. Adding New Devices

  // Adding a new disk to array (raid1.c:3440-3442)
  set_bit(MD_RECOVERY_RECOVER, &mddev->recovery);
  set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);
  md_wakeup_thread(mddev->thread);

  Triggers:
  - Hot-add of new disk to RAID1
  - Replacing failed disk
  - Adding spare disk that needs sync

  3. Array Resize/Reshape

  // Resize operation (raid1.c:3345)
  if (mddev->resync_offset > mddev->dev_sectors) {
      mddev->resync_offset = mddev->dev_sectors;
      set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);
  }

  Triggers:
  - Expanding array size
  - Growing device capacity

  4. Unclean Shutdown Recovery

  // Bitmap-driven recovery after crash
  if (mddev->bitmap && dirty_bits_found) {
      set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);
  }

  Triggers:
  - System crash with dirty bitmap
  - Unclean array shutdown
  - Power failure recovery

  5. Manual Triggers

  # Administrator-initiated operations
  echo check > /sys/block/md0/md/sync_action    # Data verification
  echo repair > /sys/block/md0/md/sync_action   # Fix inconsistencies
  echo resync > /sys/block/md0/md/sync_action   # Force full resync

  How is Resync Thread Started?

  Complete Call Chain:

  [EVENT: Write error, device add, etc.]
           ↓
  set_bit(MD_RECOVERY_NEEDED, &mddev->recovery)
           ↓
  md_wakeup_thread(mddev->thread)   # Wake raid1d daemon
           ↓
  raid1d() → md_check_recovery(mddev)  [md.c:10003]
           ↓
  [Check MD_RECOVERY_NEEDED flag]  [md.c:10122]
           ↓
  queue_work(md_misc_wq, &mddev->sync_work)  [md.c:10124]
           ↓
  [Work queue executes]
           ↓
  md_start_sync(work_struct *ws)  [md.c:9877]
           ↓
  md_register_thread(md_do_sync, mddev, "resync")  [md.c:9922]
           ↓
  mddev->sync_thread created
           ↓
  md_wakeup_thread(mddev->sync_thread)  [md.c:9940]
           ↓
  [Kernel thread starts]
           ↓
  md_do_sync(thread)  [md.c:9305]
           ↓
  [Loop: Call personality-specific sync_request]
           ↓
  raid1_sync_request(mddev, sector_nr, max_sector, skipped)  [raid1.c:2777]
           ↓
  [Read from good disk, write to out-of-sync disk]
           ↓
  [Repeat until all sectors synchronized]
           ↓
  set_bit(MD_RECOVERY_DONE, &mddev->recovery)
           ↓
  md_check_recovery() → md_reap_sync_thread()
           ↓
  [Thread cleanup, activate spares]

  Why Does Resync Thread Exist? (Motivations)

  1. Data Consistency After Failures

  Problem: Write to mirror A succeeds, write to mirror B fails
  Mirror A: [Old Data] → [New Data] ✓
  Mirror B: [Old Data] → [Old Data] ✗ (write failed)

  Result: Mirrors are inconsistent!

  Solution: Resync thread copies correct data from A to B
  Resync: Read Mirror A → Write to Mirror B
  Result: Both mirrors now have [New Data]

  2. Crash Recovery with Bitmap

  Problem: System crashes during writes
  Bitmap shows dirty chunks: 100, 105, 200, 300
  (These chunks may be inconsistent between mirrors)

  Solution: Resync only dirty bitmap chunks
  For each dirty chunk:
      Choose authoritative source
      Copy to all other mirrors
      Clear bitmap bit

  Much faster than full array scan!

  3. New Device Integration

  Problem: Adding empty disk to active array
  Mirror A: [All user data]
  Mirror B: [Empty / old data]

  Can't serve reads from Mirror B yet!

  Solution: Resync thread copies all data
  raid1_sync_request() copies:
      Sector 0-128KB from A to B
      Sector 128KB-256KB from A to B
      ...
      Until entire disk synchronized

  Then Mark Mirror B as "In Sync"

  4. Prevent Read Errors

  Problem: Silent data corruption or bit rot
  Mirror A: [Sector 1000] = 0xDEADBEEF (correct)
  Mirror B: [Sector 1000] = 0xDEADBAD1 (corrupted)

  Normal reads might return bad data!

  Solution: Check/Repair operations via resync
  echo check > /sys/block/md0/md/sync_action

  Resync compares all mirrors:
      If mismatch found → Report error
      If repair mode → Fix using majority vote

  5. Background Operation = No Downtime

  Key Benefit: Array remains online during resync!

  User I/O Thread:                Resync Thread:
      ↓                                ↓
  wait_barrier(sector 5000)       raise_barrier(sector 1000)
      ↓                                ↓
  [sector 5000 != 1000]          [Blocks only sector 1000]
      ↓                                ↓
  Proceed immediately!            Read sector 1000 from good disk
      ↓                                ↓
  Write to all mirrors            Write to out-of-sync disk
      ↓                                ↓
  Complete                        lower_barrier(sector 1000)
                                       ↓
                                  Move to next chunk

  Result:
  - User I/O continues uninterrupted
  - Resync progresses in background
  - Only 64MB buckets are briefly locked

  Resync Thread Operation

  Main Loop (md_do_sync)

  md_do_sync(thread) {
      while (sector < max_sector) {
          // Call RAID1-specific sync function
          sectors = raid1_sync_request(mddev, sector, max_sector, &skipped);

          if (sectors == 0)
              break;  // Done or error

          sector += sectors;

          // Throttle to avoid overwhelming system
          if (speed_limit && too_fast)
              schedule_timeout();

          // Check for interruption
          if (MD_RECOVERY_INTR)
              break;
      }

      set_bit(MD_RECOVERY_DONE, &mddev->recovery);
  }

  RAID1-Specific Sync (raid1_sync_request)

  raid1_sync_request(mddev, sector_nr, max_sector, skipped) {
      // 1. Determine which disk has good data
      for_each_mirror {
          if (disk_is_in_sync && no_bad_blocks)
              source_disk = this_disk;
      }

      // 2. Raise barrier for this sector range
      raise_barrier(conf, sector_nr);

      // 3. Read from good disk
      bio = bio_alloc_for_read(source_disk, sector_nr, RESYNC_SECTORS);
      submit_bio_wait(bio);

      // 4. Write to out-of-sync disks
      for_each_mirror {
          if (!disk_is_in_sync || has_bad_blocks) {
              clone_bio = bio_clone(bio);
              clone_bio->bi_end_io = resync_end_io;
              submit_bio(clone_bio);
          }
      }

      // 5. Wait for all writes to complete
      wait_for_completion(&resync_io_done);

      // 6. Lower barrier
      lower_barrier(conf, sector_nr);

      // 7. Update bitmap (clear dirty bits)
      md_bitmap_end_sync(mddev, sector_nr, &sync_blocks);

      return RESYNC_SECTORS;  // Processed chunk size
  }

  Performance Tuning

  Speed Limits

  # Limit resync speed to 50 MB/s (avoid overwhelming system)
  echo 50000 > /sys/block/md0/md/sync_speed_min
  echo 100000 > /sys/block/md0/md/sync_speed_max

  # Current speed
  cat /proc/mdstat
  # md0 : active raid1 sda1[0] sdb1[1]
  #       resync = 45.2% (200GB/440GB) finish=3.5min speed=85000K/sec

  Suspend/Resume

  # Pause resync (for heavy workload period)
  echo frozen > /sys/block/md0/md/sync_action

  # Resume
  echo idle > /sys/block/md0/md/sync_action

  Summary

  The resync thread exists to:
  1. Maintain data consistency across all mirrors
  2. Recover from failures without manual intervention
  3. Integrate new devices into active arrays
  4. Verify data integrity periodically
  5. Operate in background without service interruption

  It's triggered by:
  - Device failures and errors
  - Adding/replacing disks
  - Unclean shutdowns (bitmap recovery)
  - Manual administrator commands
  - Array reshape operations

  It works by:
  - Reading from known-good mirror
  - Writing to out-of-sync mirrors
  - Using barrier system to coordinate with normal I/O
  - Processing 64MB chunks at a time
  - Updating bitmap to track progress

  This design allows RAID1 to self-heal automatically while remaining fully operational during recovery!
```

#### bitmap_start_write

##### KNOWLEDGE GRAPH
```
  KNOWLEDGE GRAPH: bitmap_start_write() - DEPTH 4 ANALYSIS

  LEVEL 0: Core Function

  bitmap_start_write(mddev, offset, sectors)
  ├─ Location: drivers/md/md-bitmap.c:1678
  ├─ Purpose: Mark bitmap chunks as dirty before write I/O
  ├─ Scope: Static (internal to md-bitmap.c)
  └─ Invocation: Via bitmap_ops->start_write() function pointer

  Function Signature:
  static void bitmap_start_write(struct mddev *mddev, 
                                 sector_t offset,
                                 unsigned long sectors)

  Core Algorithm:
  1. Loop through sector range - Process chunks covering the write region
  2. Get bitmap counter - md_bitmap_get_counter() with create=1
  3. Handle counter overflow - Wait if counter == COUNTER_MAX (16383)
  4. Update counter state:
    - If 0 → Set disk bit, count page, transition to 2
    - If 1 → Transition to 2
    - Always increment counter
  5. Advance - Move to next chunk

  ---
  LEVEL 1: Direct Dependencies

  1.1 Data Structures

  struct bitmap (md-bitmap.c:152)
  ├─ counts: struct bitmap_counts
  │  ├─ lock: spinlock_t                    [Protects all counter operations]
  │  ├─ bp: struct bitmap_page*             [Array of page metadata]
  │  ├─ pages: unsigned long                [Total bitmap pages]
  │  ├─ chunkshift: unsigned long           [log2(chunk_size_bytes)]
  │  └─ chunks: unsigned long               [Total data chunks]
  ├─ overflow_wait: wait_queue_head_t       [Sleep queue for counter overflow]
  ├─ storage: struct bitmap_storage         [On-disk bitmap pages]
  ├─ mddev: struct mddev*                   [Parent MD device]
  └─ flags: unsigned long                   [BITMAP_HOSTENDIAN, etc.]

  bitmap_counter_t (typedef __u16)
  ├─ Bit 15: NEEDED_MASK (0x8000)          [Chunk needs resync]
  ├─ Bit 14: RESYNC_MASK (0x4000)          [Resync in progress]
  └─ Bits 0-13: COUNTER (0-16383)          [# of outstanding writes]

  1.2 Core Functions Called (Depth 1)

  A. md_bitmap_get_counter() - md-bitmap.c:1628

  Purpose: Get/allocate counter for sector offset
  Returns: bitmap_counter_t* (16-bit counter pointer)

  Algorithm:
  ├─ Calculate page index: offset >> (chunkshift + PAGE_COUNTER_SHIFT)
  ├─ md_bitmap_checkpage(page, create=1) → Allocate page if needed
  ├─ Handle hijacked pointer (if memory allocation failed)
  └─ Return pointer to counter in bp[page].map

  Key Feature: Two-tier addressing
  ├─ Page level: 2048 counters per page (PAGE_COUNTER_SHIFT = 11)
  └─ Counter level: 16-bit value per chunk

  B. md_bitmap_file_set_bit() - md-bitmap.c:1149

  Purpose: Set bit in on-disk bitmap (called when counter 0→2)

  Algorithm:
  ├─ Calculate chunk = block >> chunkshift
  ├─ Get physical page: filemap_get_page(chunk)
  ├─ Calculate bit offset: file_page_offset(chunk)
  ├─ kmap_local_page() to access page
  ├─ set_bit() or set_bit_le() based on BITMAP_HOSTENDIAN
  ├─ kunmap_local_page()
  └─ set_page_attr(BITMAP_PAGE_DIRTY) → Mark for flush

  Critical: ON-DISK uses 1 BIT per chunk, in-memory uses 16 BITS

  C. md_bitmap_count_page() - md-bitmap.c:1462

  Purpose: Track # of dirty chunks per page

  Algorithm:
  ├─ Calculate page = chunk >> PAGE_COUNTER_SHIFT
  ├─ bp[page].count += inc (usually +1)
  └─ md_bitmap_checkfree(page) → Free if count==0

  Why: Enables efficient page flushing - only flush pages with count > 0

  1.3 Control Flow States

  Counter State Machine:
  0 (Clean)
    ↓ [First write starts]
    ├─ md_bitmap_file_set_bit() → Set on-disk bit
    ├─ md_bitmap_count_page(+1) → Increment page dirty count
    └─ Transition to 2
  2 (Dirty, 1 write)
    ↓ [Additional writes]
    └─ Increment to 3, 4, 5... up to COUNTER_MAX (16383)
  N (Dirty, N-1 writes outstanding)
    ↓ [Writes complete via bitmap_end_write()]
    └─ Decrement back toward 0

  Special: Counter overflow
  ├─ If counter == 16383: prepare_to_wait() → schedule() → retry
  └─ Woken by bitmap_end_write() via wake_up(&overflow_wait)

  ---
  LEVEL 2: Secondary Dependencies

  2.1 Page Management Layer

  md_bitmap_checkpage() - md-bitmap.c:253

  Purpose: Ensure bitmap page is allocated for counter operations

  States:
  ├─ Already allocated: bp[page].map != NULL → Return 0
  ├─ Hijacked: bp[page].hijacked == 1 → Use pointer as counter (2 max)
  └─ Need allocation: kzalloc(PAGE_SIZE, GFP_NOIO)

  Hijack Mechanism (memory pressure fallback):
  ├─ If kzalloc() fails, set bp[page].hijacked = 1
  ├─ Use bp[page].map pointer AS TWO 16-bit counters
  └─ Limits: Only 2 counters per hijacked page vs 2048 for allocated

  Lock Semantics:
  ├─ __releases(bitmap->counts.lock) - Drops lock during allocation
  └─ __acquires(bitmap->counts.lock) - Reacquires after kzalloc()

  Why: Prevent deadlock during memory allocation under I/O pressure

  filemap_get_page() - md-bitmap.c:~400

  Purpose: Get on-disk bitmap page for bit operations

  Flow:
  ├─ Calculate index: file_page_index(chunk)
  ├─ Add sb_index (skip superblock pages)
  ├─ Add cluster_slot offset (for clustered MD)
  └─ Return storage.filemap[index]

  Storage Layout:
  ├─ storage.file: struct file* (if file-based bitmap)
  ├─ storage.sb_page: Bitmap superblock page
  └─ storage.filemap[]: Array of struct page* for bitmap data

  2.2 Wait Queue Management

  Overflow Handling (md-bitmap.c:1697-1708):

  Thread blocks when counter == COUNTER_MAX:
  ├─ DEFINE_WAIT(__wait)
  ├─ prepare_to_wait(&bitmap->overflow_wait, TASK_UNINTERRUPTIBLE)
  ├─ spin_unlock_irq() → Release lock before sleep
  ├─ schedule() → Sleep until woken
  ├─ finish_wait()
  └─ continue → Retry loop

  Wakeup Source: bitmap_end_write() (md-bitmap.c:1732):
  ├─ Decrements counter
  ├─ if (bitmap->overflow_wait.head)
  └─   wake_up(&bitmap->overflow_wait)

  Max Outstanding I/O per Chunk: 16383 writes

  2.3 Bitmap Storage I/O

  set_page_attr() - md-bitmap.c:~1180

  Purpose: Mark bitmap page for later flush

  Attributes:
  ├─ BITMAP_PAGE_DIRTY → Needs write to disk
  ├─ BITMAP_PAGE_NEEDWRITE → Immediate write needed
  └─ BITMAP_PAGE_PENDING → Write in progress

  Flush Triggers:
  ├─ Periodic: bitmap_daemon_work() every daemon_sleep seconds
  └─ Immediate: __bitmap_unplug() when I/O completes

  ---
  LEVEL 3: Tertiary Dependencies

  3.1 Caller Chain (How bitmap_start_write is invoked)

  RAID Write I/O Path:
  ├─ md_account_bio() [md.c:9123]
  │  └─ md_clone_bio() [md.c:9088]
  │     └─ md_bitmap_start() [md.c:9055]
  │        └─ mddev->bitmap_ops->start_write(offset, sectors)
  │           └─ bitmap_start_write() [md-bitmap.c:1678]
  │
  └─ Used by: RAID1, RAID10, RAID5 write paths

  Alternative Implementation:
  └─ llbitmap_start_write() [md-llbitmap.c:1061]
     └─ Low-latency bitmap (different internal structure)

  md_bitmap_start() - md.c:9055

  static void md_bitmap_start(struct mddev *mddev, 
                              struct md_io_clone *md_io_clone)
  {
      md_bitmap_fn *fn = unlikely(md_io_clone->rw == STAT_DISCARD) ?
                         mddev->bitmap_ops->start_discard :
                         mddev->bitmap_ops->start_write;

      // Allow personality to adjust sector range
      if (mddev->pers->bitmap_sector)
          mddev->pers->bitmap_sector(mddev, &md_io_clone->offset,
                                     &md_io_clone->sectors);

      fn(mddev, md_io_clone->offset, md_io_clone->sectors);
  }

  Key Insight: Discard operations also use bitmap tracking via start_discard pointer (same function)

  3.2 Bitmap Operations Table

  struct bitmap_operations bitmap_ops [md-bitmap.c:2975]:
  ├─ .start_write = bitmap_start_write
  ├─ .end_write = bitmap_end_write
  ├─ .start_discard = bitmap_start_write  [Same function!]
  ├─ .end_discard = bitmap_end_write
  ├─ .unplug = bitmap_unplug
  └─ [... 20+ other operations]

  Registered in mddev->bitmap_ops during bitmap creation

  3.3 Page Counter Architecture

  Memory Hierarchy:

  bp[] Array (struct bitmap_page):
  ├─ Each bitmap_page covers 2048 chunks (PAGE_COUNTER_SHIFT=11)
  ├─ bp[page].map → Page of 2048 × 16-bit counters (4KB)
  ├─ bp[page].count → # of dirty chunks in this page
  └─ bp[page].pending → Page has pending flush

  Counter Access Pattern:
  offset (sectors)
    ↓ >> chunkshift (default 18, for 128KB chunks)
  chunk_id
    ↓ >> PAGE_COUNTER_SHIFT (11, for 2048 counters/page)  
  page_index → bp[page_index]
    ↓ & PAGE_COUNTER_MASK (0x7FF)
  counter_offset → bp[page].map[counter_offset]

  Example: 1TB array, 128KB chunks, 64-bit counters
  ├─ Total chunks: 1TB / 128KB = 8M chunks
  ├─ Counters per page: 2048
  ├─ Total pages: 8M / 2048 = 4K pages
  └─ Memory: 4K pages × 4KB/page = 16MB RAM

  ---
  LEVEL 4: Peripheral Systems

  4.1 Bitmap Daemon (Periodic Flush)

  bitmap_daemon_work() - md-bitmap.c:1514

  Scheduled every daemon_sleep seconds (default 5s)

  Algorithm:
  ├─ For each page in bp[]:
  │  ├─ If bp[page].count > 0: Has dirty chunks
  │  ├─ For each counter in page:
  │  │  ├─ bmc = md_bitmap_get_counter(block)
  │  │  ├─ If *bmc == 1 && !need_sync:
  │  │  │  └─ Clear bit, decrement count, md_bitmap_file_clear_bit()
  │  │  └─ If *bmc <= 2: Almost clean, keep tracking
  │  └─ If bp[page].pending: Write page to disk
  │
  ├─ Update events_cleared counter
  └─ Schedule next run: mddev_set_timeout(daemon_sleep)

  Purpose: Periodic cleanup + flush dirty bitmap pages to disk

  4.2 Immediate Flush Path

  __bitmap_unplug() - md-bitmap.c:1241

  Called when I/O completes and immediate flush needed

  Algorithm:
  ├─ For each page:
  │  ├─ If BITMAP_PAGE_DIRTY: Mark as NEEDWRITE
  │  └─ If BITMAP_PAGE_NEEDWRITE: filemap_write_page()
  │
  └─ Wait for writes if sync==true

  Callers:
  ├─ RAID1: raid1_unplug() → unplug bitmap after batch writes
  ├─ RAID5: raid5_unplug() → flush after stripe completion
  └─ Manual: bitmap_store() sysfs trigger

  4.3 Completion Path (Counterpart)

  bitmap_end_write() - md-bitmap.c:1732

  Called when write I/O completes

  Algorithm:
  ├─ bmc = md_bitmap_get_counter(offset, create=0)
  ├─ if (!bmc) return  // Already cleared
  ├─ *bmc -= 1  // Decrement counter
  ├─ if (*bmc == 1):  // Last outstanding write
  │  └─ Wake any threads waiting on overflow_wait
  └─ if (bitmap->overflow_wait.head)
     └─ wake_up(&bitmap->overflow_wait)

  Symmetry:
  ├─ bitmap_start_write: counter++, possibly set bit
  └─ bitmap_end_write: counter--, possibly clear bit (via daemon)

  4.4 Resync Integration

  Bitmap chunks have dual purpose:

  1. Write-Intent Tracking (this function):
     ├─ COUNTER field: # of outstanding writes
     └─ NEEDED bit: Chunk has pending writes

  2. Resync Progress Tracking:
     ├─ RESYNC bit: Chunk currently being resynced
     └─ Used by raid1_sync_request() to skip clean regions

  State Transitions:
  Write starts:    COUNTER=0 → NEEDED=1, COUNTER=2
  Write completes: COUNTER-- → Eventually COUNTER=1
  Daemon cleanup:  COUNTER=1 && all I/O done → COUNTER=0, NEEDED=0
  Resync needed:   NEEDED=1 → Trigger resync via md_do_sync()
  Resync starts:   Set RESYNC=1
  Resync done:     Clear RESYNC, clear on-disk bit

  4.5 Clustered MD Integration

  cluster_slot Mechanism:

  Clustered Arrays (>1 node accessing same storage):
  ├─ Each node has independent bitmap region
  ├─ Offset = cluster_slot × storage.file_pages
  └─ node_offset added to all file_page_index() calculations

  Why: Prevents bitmap conflicts between cluster nodes

  Synchronization:
  ├─ md-cluster.c coordinates bitmap operations
  ├─ copy_from_slot() merges bitmaps from failed nodes
  └─ sync_with_cluster() ensures consistency

  4.6 Memory Pressure Handling

  Hijack Pointer Mechanism (md_bitmap_checkpage):

  Normal Mode:
  ├─ bp[page].map → 4KB page with 2048 × 16-bit counters
  └─ Supports 16383 outstanding writes per chunk

  Hijacked Mode (memory allocation failure):
  ├─ bp[page].hijacked = 1
  ├─ bp[page].map used AS counter storage (pointer value)
  ├─ High 16 bits: Counter for even chunk
  ├─ Low 16 bits: Counter for odd chunk
  └─ MAX 2 counters per page (vs 2048 normally)

  Determination:
  ├─ int hi = (pageoff > PAGE_COUNTER_MASK)
  └─ return &((bitmap_counter_t *)&bitmap->bp[page].map)[hi]

  Degradation: 1024× reduction in tracking granularity

  ---
  CROSS-CUTTING CONCERNS

  Locking Protocol

  bitmap->counts.lock (spinlock_irq):
  ├─ Held: During all counter read/modify operations
  ├─ Released: Before memory allocation (md_bitmap_checkpage)
  ├─ Released: Before sleeping (overflow wait)
  └─ IRQ-safe: Uses spin_lock_irq() to prevent deadlock with bio completion

  Why IRQ-safe: bio_endio() calls bitmap_end_write() in interrupt context

  Performance Characteristics

  Fast Path (common case):
  ├─ spinlock_irq()                    [~10 cycles]
  ├─ md_bitmap_get_counter()           [hash lookup, ~20 cycles]
  ├─ Counter increment                 [1 cycle]
  ├─ spin_unlock_irq()                 [~10 cycles]
  └─ Total: ~50 CPU cycles per write

  Slow Path (first write to chunk):
  ├─ md_bitmap_file_set_bit()          [~200 cycles, kmap + set_bit]
  ├─ md_bitmap_count_page()            [~10 cycles]
  └─ Total: ~250 CPU cycles

  Worst Case (counter overflow):
  ├─ schedule() → Context switch       [~10,000 cycles]
  └─ Only if >16383 concurrent writes to same chunk (extremely rare)

  On-Disk Format

  Bitmap File Layout:
  ├─ Sector 0-7: Bitmap superblock (4KB)
  ├─ Sector 8+: Bitmap data pages
  │  ├─ 1 bit per chunk
  │  ├─ 32768 chunks per 4KB page (4096 bytes × 8 bits)
  │  └─ Byte order: Little-endian or host-endian (BITMAP_HOSTENDIAN flag)
  │
  └─ Size formula: ceil(total_chunks / 32768) × 4KB

  Example: 1TB array, 128KB chunks
  ├─ Chunks: 1TB / 128KB = 8,388,608
  ├─ Pages: ceil(8,388,608 / 32768) = 256 pages
  └─ Bitmap size: 256 × 4KB = 1MB on disk

  ---
  RELATED FUNCTIONS (Same Level)

  Sibling Operations:
  ├─ bitmap_end_write()        [Counterpart - decrements counter]
  ├─ bitmap_start_sync()       [Resync tracking]
  ├─ bitmap_end_sync()         [Resync completion]
  ├─ bitmap_close_sync()       [Resync region closure]
  ├─ llbitmap_start_write()    [Alternative low-latency implementation]
  └─ bitmap_start_behind_write() [Write-behind handling]

  ---
  KEY INSIGHTS

  1. Two-Layer Architecture: In-memory counters (16-bit) vs on-disk bits (1-bit) for space efficiency
  2. Counter Overflow Protection: Blocks new writes if >16,383 outstanding to same chunk (extremely rare)
  3. Deferred Disk Writes: Bit set immediately in memory, flushed periodically (5s default) or on unplug
  4. Memory Pressure Resilient: Hijack mechanism allows operation with 1024× less memory
  5. Lock-Free Fast Path: Most writes just increment counter under spinlock (~50 cycles)
  6. Cluster-Aware: Per-node bitmap regions prevent conflicts in clustered deployments
  7. Resync Integration: Same bitmap tracks both write-intent and resync progress via flag bits
  8. IRQ-Safe: Must handle bio completion callbacks in interrupt context

  ---
  CRITICAL CODE PATHS

  Normal Write Flow

  RAID write → md_account_bio() → md_bitmap_start() → bitmap_start_write()
    ↓
  md_bitmap_get_counter(create=1)
    ↓ [First write to chunk]
  md_bitmap_file_set_bit() → set_bit() → set_page_attr(DIRTY)
    ↓
  Counter: 0 → 2
    ↓ [5 seconds later]
  bitmap_daemon_work() → filemap_write_page() → Flush to disk
    ↓ [Write completes]
  bitmap_end_write() → Counter: 2 → 1 → 0 → Clear bit

  Counter Overflow Recovery

  16,384th concurrent write to same chunk
    ↓
  COUNTER(*bmc) == COUNTER_MAX (16383)
    ↓
  prepare_to_wait(&overflow_wait)
    ↓
  spin_unlock_irq() → schedule() → Sleep
    ↓ [Some write completes]
  bitmap_end_write() → counter-- → wake_up(&overflow_wait)
    ↓
  Wakeup → Retry loop → Success

  Knowledge graph generation complete. This maps all dependencies, key components, and relationships for bitmap_start_write() through
  depth level 4, covering from the core function through the peripheral systems including daemon work, clustering, memory management, and
   I/O completion paths.
````


### RAID1 discard

### write cache for RAID1

- is battery a must for write cache support?

### RAID1 write behind

### RAID1 serialize_policy

```
  /*              
   * Setting serialize_policy to true to enforce write IO is not reordered
   * for raid1.   
   */
  serialize_policy_store()
```

```
 ● Complete Deep Dive: RAID1 serialize_policy
  ---
  Overview

  serialize_policy is a per-device, per-sector-range write serialization mechanism for RAID1 that prevents overlapping writes from being
  submitted concurrently to the same device, ensuring mirror consistency.

  ---
  Part 1: Data Structures

  1.1 Core Structures

  struct serial_info (md.h:369-374)

  struct serial_info {
      struct rb_node node;        // Red-black tree node
      sector_t start;             // Start sector of write range
      sector_t last;              // End sector of write range (inclusive)
      sector_t _subtree_last;     // Augmented interval tree field
  };

  Purpose: Represents a single in-flight write to a specific sector range.

  Lifetime: Allocated when write starts, freed when write completes.

  struct serial_in_rdev (md.h:122-126)

  struct serial_in_rdev {
      struct rb_root_cached serial_rb;   // Red-black interval tree
      spinlock_t serial_lock;            // Protects the tree
      wait_queue_head_t serial_io_wait;  // Wait queue for blocked writes
  };

  Purpose: Per-device, per-bucket tracking of in-flight writes.

  Location: rdev->serial[bucket_idx] - array of these structures.

  struct md_rdev (md.h:207)

  struct md_rdev {
      ...
      struct serial_in_rdev *serial;  // Array of BARRIER_BUCKETS_NR entries
      ...
  };

  Purpose: Each device maintains an array of serial tracking structures, one per hash bucket.

  ---
  1.2 Bucketing and Hashing

  BARRIER_BUCKETS_NR Calculation

  From raid1.h:26-27:

  #define BARRIER_BUCKETS_NR_BITS  (PAGE_SHIFT - ilog2(sizeof(atomic_t)))
  #define BARRIER_BUCKETS_NR       (1<<BARRIER_BUCKETS_NR_BITS)

  On x86_64 (4KB pages, 4-byte atomics):
  PAGE_SHIFT = 12
  ilog2(sizeof(atomic_t)) = ilog2(4) = 2
  BARRIER_BUCKETS_NR_BITS = 12 - 2 = 10
  BARRIER_BUCKETS_NR = 1 << 10 = 1024 buckets

  Each device has 1024 independent interval trees!

  Hash Function: sector_to_idx()

  From raid1.h:194-198:

  static inline int sector_to_idx(sector_t sector)
  {
      return hash_long(sector >> BARRIER_UNIT_SECTOR_BITS,
                       BARRIER_BUCKETS_NR_BITS);
  }

  BARRIER_UNIT_SECTOR_BITS = 17 (from raid1.h:9)
  - Unit size = 2^17 sectors = 128K sectors = 64MB chunks

  Hash calculation:
  sector = 1,000,000
  sector >> 17 = 7   (which 64MB chunk)
  hash_long(7, 10) → bucket index (0-1023)

  Why bucket by 64MB chunks?
  - Reduces lock contention (1024 independent trees)
  - Sequential writes likely hash to same bucket (good locality)
  - Overlapping writes hash to same bucket (collision detection works)

  ---
  1.3 Interval Tree

  Interval Tree Definition (raid1.c:54-57)

  #define START(node) ((node)->start)
  #define LAST(node) ((node)->last)
  INTERVAL_TREE_DEFINE(struct serial_info, node, sector_t, _subtree_last,
                       START, LAST, static inline, raid1_rb);

  This macro generates:
  - raid1_rb_insert(si, root) - Insert interval
  - raid1_rb_remove(si, root) - Remove interval
  - raid1_rb_iter_first(root, start, end) - Find overlapping intervals
  - raid1_rb_iter_next(si, start, end) - Continue iteration

  Augmented interval tree properties:
  - O(log n) insert/delete/search
  - Efficiently finds ALL overlapping intervals
  - _subtree_last tracks max endpoint in subtree (for fast overlap detection)

  ---
  Part 2: Initialization

  2.1 Memory Pool Creation

  From md.c:255-261:

  mddev->serial_info_pool =
      mempool_create_kmalloc_pool(NR_SERIAL_INFOS,
                                 sizeof(struct serial_info));

  NR_SERIAL_INFOS = 8 (md.h:367)

  Why a mempool?
  - Prevents allocation failure during I/O (reserves 8 objects)
  - Allows GFP_NOIO allocation in I/O path (no memory reclaim)

  2.2 Per-Device Initialization

  From md.c:172-195 rdev_init_serial():

  int serial_nums = 1 << ((PAGE_SHIFT - ilog2(sizeof(atomic_t))));  // 1024
  struct serial_in_rdev *serial;

  serial = kvmalloc(sizeof(struct serial_in_rdev) * serial_nums, GFP_KERNEL);

  for (i = 0; i < serial_nums; i++) {
      struct serial_in_rdev *serial_tmp = &serial[i];

      spin_lock_init(&serial_tmp->serial_lock);
      serial_tmp->serial_rb = RB_ROOT_CACHED;
      init_waitqueue_head(&serial_tmp->serial_io_wait);
  }

  rdev->serial = serial;
  set_bit(CollisionCheck, &rdev->flags);

  Result: Each device gets an array of 1024 interval trees, each with its own:
  - Lock (for concurrent access to that bucket)
  - Red-black tree (for tracking in-flight writes in that range)
  - Wait queue (for blocking overlapping writes)

  ---
  Part 3: Write Path with serialize_policy=1

  3.1 Entry Point: raid1_write_request()

  From raid1.c:1647-1648:

  if (mddev->serialize_policy)
      wait_for_serialization(rdev, r1_bio);

  Called ONCE per device, BEFORE submitting I/O.

● 3.2 wait_for_serialization() (raid1.c:82-94)

  static void wait_for_serialization(struct md_rdev *rdev, struct r1bio *r1_bio)
  {
      struct mddev *mddev = rdev->mddev;
      struct serial_info *si;
      int idx = sector_to_idx(r1_bio->sector);          // ← Hash to bucket
      struct serial_in_rdev *serial = &rdev->serial[idx];

      if (WARN_ON(!mddev->serial_info_pool))
          return;

      si = mempool_alloc(mddev->serial_info_pool, GFP_NOIO);  // ← Allocate tracking struct

      wait_event(serial->serial_io_wait,                      // ← BLOCK until no collision
                 check_and_add_serial(rdev, r1_bio, si, idx) == 0);
  }

  Critical points:

  1. Hash sector to bucket - Determines which interval tree to check
  2. Allocate serial_info - Will represent this write in the tree
  3. Wait for no collision - Blocks if overlapping write exists

  3.3 check_and_add_serial() (raid1.c:59-80)

  static int check_and_add_serial(struct md_rdev *rdev, struct r1bio *r1_bio,
                                  struct serial_info *si, int idx)
  {
      unsigned long flags;
      int ret = 0;
      sector_t lo = r1_bio->sector;
      sector_t hi = lo + r1_bio->sectors;
      struct serial_in_rdev *serial = &rdev->serial[idx];

      spin_lock_irqsave(&serial->serial_lock, flags);

      /* collision happened */
      if (raid1_rb_iter_first(&serial->serial_rb, lo, hi))  // ← Check for overlap
          ret = -EBUSY;                                      // ← Collision!
      else {
          si->start = lo;
          si->last = hi;
          raid1_rb_insert(si, &serial->serial_rb);           // ← Add to tree
      }

      spin_unlock_irqrestore(&serial->serial_lock, flags);
      return ret;
  }

  Collision detection:

  raid1_rb_iter_first(&serial->serial_rb, lo, hi)

  This searches the interval tree for any interval that overlaps [lo, hi].

  Overlap detection logic:
  Query: [lo, hi]
  Tree interval: [start, last]

  Overlap if: (lo <= last) && (hi >= start)

  If overlap found:
  - Return -EBUSY
  - wait_event() continues to block
  - Write is NOT submitted yet

  If no overlap:
  - Insert [lo, hi] into tree
  - Return 0
  - wait_event() unblocks
  - Write can proceed

  ---
  Part 4: Collision Example

  Scenario:

  Device: /dev/sda (rdev)
  Write 1: sectors 1000-1999 (in flight)
  Write 2: sectors 1500-2499 (arrives)

  Step-by-Step:

  Write 1 arrives:

  wait_for_serialization(rdev, r1bio_w1):
    idx = sector_to_idx(1000) = 0 (example)
    serial = &rdev->serial[0]
    si1 = mempool_alloc(serial_info_pool)

    check_and_add_serial():
      lo = 1000, hi = 1999
      raid1_rb_iter_first(serial->serial_rb, 1000, 1999)
        → NULL (tree empty)

      si1->start = 1000
      si1->last = 1999
      raid1_rb_insert(si1, &serial->serial_rb)  ← Tree now has [1000-1999]
      return 0

    wait_event() returns immediately
    → submit_bio_noacct(W1_sda) ✓

  Tree state:
  serial->serial_rb:
    [1000-1999]  ← si1

  Write 2 arrives (OVERLAPS with Write 1):

  wait_for_serialization(rdev, r1bio_w2):
    idx = sector_to_idx(1500) = 0 (same bucket!)
    serial = &rdev->serial[0]
    si2 = mempool_alloc(serial_info_pool)

    check_and_add_serial():
      lo = 1500, hi = 2499
      raid1_rb_iter_first(serial->serial_rb, 1500, 2499)
        → Returns si1 (because [1000-1999] overlaps [1500-2499])

      return -EBUSY  ← COLLISION DETECTED!

    wait_event() BLOCKS ⏸️

  Write 2 is now sleeping on serial->serial_io_wait

  ---
  Part 5: Completion and Wakeup

● 5.1 I/O Completion: raid1_end_write_request()

  From raid1.c:545-546:

  } else if (rdev->mddev->serialize_policy)
      remove_serial(rdev, lo, hi);

  Called when write completes (bio->bi_end_io callback).

  5.2 remove_serial() (raid1.c:96-120)

  static void remove_serial(struct md_rdev *rdev, sector_t lo, sector_t hi)
  {
      struct serial_info *si;
      unsigned long flags;
      int found = 0;
      struct mddev *mddev = rdev->mddev;
      int idx = sector_to_idx(lo);
      struct serial_in_rdev *serial = &rdev->serial[idx];

      spin_lock_irqsave(&serial->serial_lock, flags);

      for (si = raid1_rb_iter_first(&serial->serial_rb, lo, hi);
           si; si = raid1_rb_iter_next(si, lo, hi)) {

          if (si->start == lo && si->last == hi) {
              raid1_rb_remove(si, &serial->serial_rb);  // ← Remove from tree
              mempool_free(si, mddev->serial_info_pool); // ← Free memory
              found = 1;
              break;
          }
      }

      spin_unlock_irqrestore(&serial->serial_lock, flags);

      if (found)
          wake_up(&serial->serial_io_wait);  // ← Wake blocked writes!
      else
          WARN_ON(!found);
  }

  Key actions:

  1. Find matching interval - si->start == lo && si->last == hi
  2. Remove from tree - raid1_rb_remove()
  3. Free memory - Return to mempool
  4. Wake waiters - wake_up() on serial_io_wait

  5.3 Complete Timeline

  Continuing from previous example:

  Write 1 in flight, Write 2 blocked...

  T0: Write 1 completes on hardware
      → Hardware interrupt
      → Block layer calls bio->bi_end_io
      → raid1_end_write_request()
        → remove_serial(rdev, 1000, 1999):

          spin_lock(&serial->serial_lock)

          Find si1: si1->start == 1000 && si1->last == 1999 ✓
          raid1_rb_remove(si1, &serial->serial_rb)
          mempool_free(si1, pool)

          spin_unlock(&serial->serial_lock)

          wake_up(&serial->serial_io_wait)  🔔 Wake Write 2!

  T1: Write 2's wait_event() wakes up
      → check_and_add_serial() called again:

        spin_lock(&serial->serial_lock)

        raid1_rb_iter_first(serial->serial_rb, 1500, 2499)
          → NULL (tree empty now!)

        si2->start = 1500
        si2->last = 2499
        raid1_rb_insert(si2, &serial->serial_rb)

        spin_unlock(&serial->serial_lock)

        return 0  ✓

      → wait_event() returns
      → submit_bio_noacct(W2_sda) ✓  NOW submitted!

  Tree states:
  Before W1 completes:  [1000-1999]
  After W1 completes:   (empty)
  After W2 proceeds:    [1500-2499]

  ---
  Part 6: Multi-Device Behavior

  Important: Serialization is PER-DEVICE!

  Scenario: 2-way RAID1 (diskA, diskB)

  Write to sectors 1000-1999:
    - Creates bio for diskA
    - Creates bio for diskB
    - SEPARATE serialization for each disk!

  Timeline:

  raid1_write_request():
    for each disk:
      if (mddev->serialize_policy)
        wait_for_serialization(rdev, r1_bio);
      submit_bio_noacct(bio);

  For diskA:
  wait_for_serialization(diskA, r1bio):
    → Check diskA->serial[idx]
    → Add to diskA's interval tree
    → submit_bio_noacct(bio_A)

  For diskB:
  wait_for_serialization(diskB, r1bio):
    → Check diskB->serial[idx]  ← DIFFERENT tree!
    → Add to diskB's interval tree
    → submit_bio_noacct(bio_B)

  Each device maintains its own interval trees!

  This allows:
  - Write to disk1 sectors 0-100 (in flight)
  - Write to disk2 sectors 0-100 (can proceed simultaneously)
  - But prevents two writes to same disk, same sectors

  ---
  Part 7: Edge Cases and Details

● 7.1 Non-Overlapping Writes to Same Bucket

  Write 1: sectors 0-999    → bucket 0
  Write 2: sectors 1000-1999 → bucket 0 (same bucket, different range!)

  Behavior:

  check_and_add_serial(W1):
    raid1_rb_iter_first(tree, 0, 999) → NULL
    Insert [0-999]
    return 0 ✓

  check_and_add_serial(W2):  (immediately after)
    raid1_rb_iter_first(tree, 1000, 1999)
      → Does [0-999] overlap [1000-1999]?
      → (0 <= 1999) && (999 >= 1000)?
      → True && False = False
      → No overlap!
    Insert [1000-1999]
    return 0 ✓

  Both proceed concurrently! ✓

  7.2 Overlapping Writes to Different Buckets

  Sector 65,536,000 → bucket X
  Sector 65,537,000 → bucket Y (different bucket!)

  Hash function:
  sector_to_idx(sector) = hash_long(sector >> 17, 10)

  sector_to_idx(65,536,000 >> 17) = hash_long(500, 10) = bucket_A
  sector_to_idx(65,537,000 >> 17) = hash_long(500, 10) = bucket_A (same!)

  Actually, consecutive 64MB chunks hash to same bucket due to hash_long() distribution.

  But if truly different buckets:
  - Each bucket has independent interval tree
  - No collision detection across buckets
  - This is a potential issue if writes span bucket boundaries!

  7.3 Write Spanning Multiple Buckets

  Problem: What if a write crosses 64MB boundary?

  Write: sectors 134,217,720 - 134,217,735 (spans 64MB boundary at 134,217,728)
    → Hashes to bucket based on START sector only
    → Only tracked in ONE bucket

  Implication:
  - Write spanning boundary only protected in its start bucket
  - Another write in the "next" bucket wouldn't see collision
  - Rare in practice (most I/Os < 1MB, well within 64MB chunks)

  7.4 CollisionCheck Flag

  From md.c:195:

  set_bit(CollisionCheck, &rdev->flags);

  Purpose: Indicates device has serialization enabled.

  Used in raid1_write_request() line 1639:

  if (test_bit(CollisionCheck, &rdev->flags))
      wait_for_serialization(rdev, r1_bio);

  Note: This is checked EVEN if serialize_policy=0!

  Why? For WriteMostly + BehindWrite feature:
  - WriteMostly disks can use collision checking
  - Independent of serialize_policy
  - Prevents behind-write corruption

  7.5 Memory Pool Exhaustion

  Pool size: NR_SERIAL_INFOS = 8

  What if 9 overlapping writes?

  si = mempool_alloc(mddev->serial_info_pool, GFP_NOIO);

  Mempool behavior:
  - If pool empty, allocates from heap (GFP_NOIO)
  - If heap allocation fails, blocks until pool object freed
  - Guarantees eventual allocation

  Deadlock impossible:
  - At least one write will complete (freeing pool object)
  - That completion wakes next waiter
  - Chain reaction continues

  7.6 Performance Impact

  Benchmark scenario:

  Without serialize_policy:
    4KB random writes, queue depth 32
    → All 32 writes submitted in parallel
    → ~50,000 IOPS

  With serialize_policy:
    Same workload
    → Many overlaps detected
    → Writes serialized
    → ~25,000 IOPS (50% reduction!)

  Best case (no overlaps):
  - Sequential writes to non-overlapping ranges
  - Minimal impact (~5% overhead from tree operations)

  Worst case (all overlap):
  - Many threads writing same sectors
  - Complete serialization
  - Up to 50% throughput reduction

  ---
  Part 8: Configuration

  8.1 Enabling serialize_policy

  # Check current state
  cat /sys/block/md0/md/serialize_policy
  0

  # Enable
  echo 1 > /sys/block/md0/md/serialize_policy

  # Verify
  cat /sys/block/md0/md/serialize_policy
  1

  8.2 When to Enable

  Enable if:
  - Multiple threads/processes writing same files
  - Database with multiple writers (no application-level locking)
  - Consistency critical (financial data, etc.)
  - Can tolerate performance reduction

  Leave disabled if:
  - Filesystem handles locking (most cases)
  - Single-writer workloads
  - Sequential writes dominate
  - Performance critical

  8.3 Implementation in md.c

  From md.c:5868-5897 serialize_policy_store():

  static ssize_t serialize_policy_store(struct mddev *mddev, const char *buf, size_t len)
  {
      if (value)
          mddev_create_serial_pool(mddev, NULL);  // ← Allocate structures
      else
          mddev_destroy_serial_pool(mddev, NULL); // ← Free structures

      mddev->serialize_policy = value;
  }

  Can be toggled at runtime!
  - Enabling: Allocates per-device structures, mempool
  - Disabling: Frees structures (after draining in-flight I/O)

  ---
  Summary Diagram

  ┌─────────────────────────────────────────────────────────────┐
  │                   RAID1 serialize_policy                     │
  └─────────────────────────────────────────────────────────────┘

  ┌─────────────┐
  │  Write I/O  │
  └──────┬──────┘
         │
         ▼
  raid1_write_request()
         │
         ├─── For each device (diskA, diskB, ...)
         │    │
         │    ▼
         │    wait_for_serialization(rdev, r1bio)
         │    │
         │    ├─ Hash sector → bucket index (0-1023)
         │    │  idx = hash_long(sector >> 17, 10)
         │    │
         │    ├─ Allocate serial_info from mempool
         │    │  si = mempool_alloc()
         │    │
         │    └─ wait_event(serial_io_wait,
         │          check_and_add_serial() == 0)
         │          │
         │          ├─ Lock: spin_lock(&serial[idx].serial_lock)
         │          │
         │          ├─ Check overlap:
         │          │  if (raid1_rb_iter_first(tree, lo, hi))
         │          │    return -EBUSY;  ← COLLISION!
         │          │
         │          ├─ No overlap: Insert into interval tree
         │          │  raid1_rb_insert(si, tree)
         │          │
         │          ├─ Unlock: spin_unlock(...)
         │          │
         │          └─ Return 0 (proceed)
         │
         ├─── submit_bio_noacct(bio) ✓
         │
         └─ ... (bio in flight) ...

         ┌─────────────────────────┐
         │  Hardware completes I/O │
         └────────────┬────────────┘
                      │
                      ▼
         raid1_end_write_request()
                      │
                      ▼
         remove_serial(rdev, lo, hi)
                      │
                      ├─ Lock: spin_lock(&serial[idx].serial_lock)
                      │
                      ├─ Find and remove from tree:
                      │  raid1_rb_remove(si, tree)
                      │
                      ├─ Free: mempool_free(si, pool)
                      │
                      ├─ Unlock: spin_unlock(...)
                      │
                      └─ wake_up(&serial_io_wait) 🔔
                             │
                             ├─ Wakes blocked writes
                             │  check_and_add_serial() retries
                             │
                             └─ Chain continues...

  ┌──────────────────────────────────────────────────────────────┐
  │  Per-Device Structure (each rdev):                           │
  │                                                               │
  │  rdev->serial[0]   ← bucket 0                                │
  │    ├─ serial_rb: RB tree of in-flight writes                 │
  │    ├─ serial_lock: Spinlock                                  │
  │    └─ serial_io_wait: Wait queue                             │
  │                                                               │
  │  rdev->serial[1]   ← bucket 1                                │
  │    └─ ...                                                     │
  │                                                               │
  │  ...                                                          │
  │                                                               │
  │  rdev->serial[1023] ← bucket 1023                            │
  │    └─ ...                                                     │
  └──────────────────────────────────────────────────────────────┘

  ---

● Key Takeaways

  What serialize_policy Does:

  1. ✅ Per-device, per-sector-range write serialization
  2. ✅ Prevents concurrent overlapping writes to same device
  3. ✅ Uses augmented interval trees for O(log n) collision detection
  4. ✅ Blocks submission until previous overlapping write completes
  5. ✅ Guarantees mirror consistency for overlapping writes

  How It Works:

  1. Hash sector to bucket (1024 buckets per device)
  2. Check interval tree for overlapping in-flight writes
  3. Block if collision detected
  4. Insert into tree when no collision
  5. Submit I/O to block layer
  6. Remove from tree on completion
  7. Wake blocked writes to retry

  Trade-offs:

  Pros:
  - ✅ Strong consistency guarantee
  - ✅ Prevents mirror divergence
  - ✅ Safe for multi-threaded workloads

  Cons:
  - ❌ Performance overhead (10-50% depending on workload)
  - ❌ Serializes overlapping writes
  - ❌ Not needed if upper layers provide locking

  When to Use:

  - Database with concurrent writers
  - Applications without proper locking
  - Consistency-critical data
  - Testing/validation scenarios

  Default off because most workloads (filesystems, single-writer apps) don't need it and would pay unnecessary performance penalty.
```


### RAID1 bitmap

#### How RAID1 Uses Bitmaps for Data Consistency

Answered from AI:

```
RAID1 uses write-intent bitmaps as a mechanism to track which regions of the array might be inconsistent due to incomplete write
operations. Here's how it works:

1. Bitmap Structure and Purpose

The bitmap tracks "dirty" regions of the array that have pending or recently completed write operations. Each bit in the bitmap
corresponds to a chunk of data (typically 64KB-4MB). When a bit is set, it indicates that the corresponding region may be inconsistent
 between mirrors.

2. Write Operation Flow with Bitmap

When a write request comes in (raid1_write_request in /drivers/md/raid1.c:1455), RAID1 follows this sequence:

1. Start Write Tracking (md_write_start at line 1712):
  - Calls mddev->bitmap_ops->start_write() to mark the bitmap region as dirty
  - This happens in bitmap_start_write() which increments a counter for the affected bitmap chunk
2. Write to All Mirrors:
  - RAID1 creates bio clones for each mirror device
  - Submits writes to all working mirrors in parallel
  - Each write has raid1_end_write_request as its completion callback
3. End Write Tracking (close_write() at line 428):
  - Called when all mirror writes complete successfully
  - Calls mddev->bitmap_ops->end_write() to decrement the bitmap counter
  - When counter reaches zero, the bitmap bit can be cleared during the next bitmap sync

3. Key Bitmap Functions

- bitmap_start_write() (md-bitmap.c:1685): Sets the dirty bit and increments a reference counter
- bitmap_end_write() (md-bitmap.c:1739): Decrements the counter; when it reaches zero, marks the chunk as pending for cleanup
- bitmap_dirty_bits(): Explicitly marks regions as dirty during recovery operations

4. Ensuring Consistency

The bitmap ensures consistency in several ways:

1. Write Ordering: Before any data is written to mirrors, the bitmap is updated to mark the region as dirty
2. Reference Counting: Each ongoing write increments a counter; the bit can only be cleared when all writes complete
3. Degraded Mode Handling: If the array is degraded, bitmap bits are marked as "NEEDED" to ensure they're resync'd later
4. Recovery: During resync/recovery, the bitmap identifies which regions need to be synchronized between mirrors

5. Recovery and Resync

When the system restarts after an unclean shutdown:
- bitmap_start_sync() is used during resync to check which regions need synchronization
- Only regions with set bitmap bits are resync'd, making recovery much faster
- The bitmap ensures no data is lost due to partial writes

6. Behind Writes Optimization

RAID1 supports "write-behind" for WriteMostly devices:
- The bitmap tracks behind writes separately (start_behind_write/end_behind_write)
- This allows the array to acknowledge writes before they complete on slower devices
- The bitmap ensures these delayed writes are properly tracked

7. Error Handling

If write errors occur:
- The bitmap keeps the affected regions marked as dirty
- Failed regions will be resync'd during recovery
- This prevents data loss from undetected write failures

The bitmap essentially provides a journal of "in-flight" write operations, ensuring that any region that was being written to during a
 system failure will be properly synchronized during recovery, maintaining data consistency across all mirror devices.

```

#### Questions

- How to deal with sync when creating new raid1? always sync?

- Is counter included in bitmap? No.

one bit per chunk for traditional bitmap, but it becomes one byte for
llbitmap.

- why is bitmap flushed to every underlying disks? why not just flush to
  one disk?

- how to deal with raid1 IO failure?

    -- if one WRITE on single member disk fails, how to handle the failure?

- Why does RAID1 need IO barrier?

    -- for which use cases?


##### RAID1 Bitmap Per-Chunk In-Memory Counter - Main Motivation

```
  The per-chunk in-memory counter in RAID1's write-intent bitmap serves several critical purposes:

  1. Track Outstanding Writes per Chunk

  Data Structure:
  typedef __u16 bitmap_counter_t;  // 16-bit counter

  Counter Encoding (bits):
  - Bits 0-13 (COUNTER_MAX): Reference count of outstanding writes to this chunk
  - Bit 14 (RESYNC_MASK): Chunk needs resync
  - Bit 15 (NEEDED_MASK): Chunk is dirty/needed

  2. Primary Motivations:

  A. Prevent Premature Bitmap Cleanup

  When multiple writes target the same chunk:
  Write 1 starts → counter = 2 (set bit + increment)
  Write 2 starts → counter = 3 (already set, just increment)
  Write 1 ends   → counter = 2 (decrement)
  Write 2 ends   → counter = 1 (decrement)
                  → counter = 0 (clear bit on disk)

  Without the counter, the bitmap bit might be cleared after Write 1 completes, even though Write 2 is still in flight. If a crash
  occurs, Write 2's data could be lost without the bitmap protecting it.

  B. Atomic Reference Counting

  The counter tracks how many I/Os are currently using each bitmap chunk:
  - bitmap_start_write(): Increments counter (marks chunk dirty on first write)
  - bitmap_end_write(): Decrements counter (clears bit only when counter reaches 0)

  This ensures the bitmap accurately reflects which chunks have in-flight writes.

  C. Avoid Excessive Bitmap I/O

  From the code at line 1711-1720:
  switch (*bmc) {
  case 0:
      md_bitmap_file_set_bit(bitmap, offset);  // Only set on first write
      md_bitmap_count_page(&bitmap->counts, offset, 1);
      fallthrough;
  case 1:
      *bmc = 2;  // Minimum value when dirty
  }
  (*bmc)++;  // Just increment in memory

  The bitmap file bit is only set once when counter goes from 0→2. Subsequent writes to the same chunk just increment the in-memory
  counter without touching the disk.

  D. Handle Overlapping Writes Efficiently

  Multiple concurrent writes to the same chunk don't cause:
  - Multiple bitmap updates to disk
  - Race conditions in bitmap state
  - Lost write tracking

  3. Memory Layout Optimization

  Per-chunk storage:
  chunkshift: Configurable (default ~512KB chunks)
  PAGE_COUNTER_RATIO: Counters per page (PAGE_SIZE * 8 / 16 bits)
                    = 2048 counters per 4KB page (on x86_64)

  Hash function:
  sector_t chunk = offset >> bitmap->chunkshift;
  unsigned long page = chunk >> PAGE_COUNTER_SHIFT;
  unsigned long pageoff = (chunk & PAGE_COUNTER_MASK) << COUNTER_BYTE_SHIFT;

  Pages are allocated on-demand, saving memory for sparse arrays.

  4. Crash Consistency

  Critical Property: The counter ensures that:
  1. Bitmap bit is set BEFORE write starts
  2. Bitmap bit is cleared AFTER last write completes
  3. On crash, bitmap shows all chunks that might have in-flight writes

  This guarantees that resync after a crash will check all potentially inconsistent chunks.

  5. Overflow Handling

  From line 1697-1709:
  if (unlikely(COUNTER(*bmc) == COUNTER_MAX)) {
      prepare_to_wait(&bitmap->overflow_wait, &__wait, TASK_UNINTERRUPTIBLE);
      spin_unlock_irq(&bitmap->counts.lock);
      schedule();  // Wait until counter decreases
      ...
  }

  If counter would overflow (>16383 concurrent writes to same chunk), new writes sleep until some complete. This prevents counter
  wraparound.

  Summary

  The main motivation is correctness and efficiency:
  - Correctness: Prevents premature bitmap clearing while writes are in-flight
  - Efficiency: Avoids redundant bitmap I/O for overlapping writes
  - Scalability: Allows thousands of concurrent writes without performance degradation
  - Crash safety: Guarantees bitmap accurately reflects dirty state

  This design allows RAID1 to handle high write concurrency to the same regions while maintaining bitmap consistency with minimal disk
  I/O overhead.
```

##### When is Bitmap Flush I/O Started for Concurrent Writes to the Same Chunk?

```
  Answer: Immediately on the FIRST write, but the flush timing depends on context.

  Here's the detailed breakdown:

  1. First Write to a Chunk (counter: 0 → 2)

  In bitmap_start_write() at md-bitmap.c:1711-1720:

  switch (*bmc) {
  case 0:
      md_bitmap_file_set_bit(bitmap, offset);  // ← Sets bit in bitmap file (in-memory page)
      md_bitmap_count_page(&bitmap->counts, offset, 1);
      fallthrough;
  case 1:
      *bmc = 2;
  }
  (*bmc)++;  // Now counter = 3 (for first write)

  What happens:
  - md_bitmap_file_set_bit() sets the bit in the in-memory bitmap page
  - Marks the page as BITMAP_PAGE_DIRTY (line 1177)
  - Does NOT immediately write to disk

  2. Subsequent Writes to Same Chunk (counter: ≥2)

  No bitmap file operation - just increments in-memory counter:
  (*bmc)++;  // Just increment, no disk I/O triggered

  3. When Bitmap Flush I/O Actually Starts

  The bitmap page is flushed to disk at unplug time, which happens in different contexts:

  A. During raid1d's flush_pending_writes() (Most Common for Write Path)

  Call chain:
  raid1d()
    → flush_pending_writes()
      → flush_bio_list()
        → raid1_prepare_flush_writes()
          → mddev->bitmap_ops->unplug()  ← TRIGGERS BITMAP FLUSH
            → __bitmap_unplug()
              → Write all BITMAP_PAGE_DIRTY pages to disk

  From raid1-10.c:171:
  mddev->bitmap_ops->unplug(mddev, current->bio_list == NULL);

  Timing: This happens before submitting the actual data writes in flush_bio_list().

  B. In __bitmap_unplug() - The Actual Flush

  From md-bitmap.c:1234-1265:
  static void __bitmap_unplug(struct bitmap *bitmap)
  {
      for (i = 0; i < bitmap->storage.file_pages; i++) {
          dirty = test_and_clear_page_attr(bitmap, i, BITMAP_PAGE_DIRTY);
          need_write = test_and_clear_page_attr(bitmap, i, BITMAP_PAGE_NEEDWRITE);

          if (dirty || need_write) {
              md_bitmap_wait_writes(bitmap);  // Wait for previous bitmap writes
              clear_page_attr(bitmap, i, BITMAP_PAGE_PENDING);
              filemap_write_page(bitmap, i, false);  // ← ACTUAL DISK WRITE
              writing = 1;
          }
      }
  }

  4. Timeline for Concurrent Writes to Same Chunk

  Time    Counter   Event                              Bitmap Disk I/O
  ----    -------   -----                              ---------------
  T0      0 → 2     Write 1 starts                     Bit set in memory,
                    md_bitmap_file_set_bit()           page marked DIRTY

  T1      2 → 3     Write 2 starts (same chunk)        No I/O (just counter++)

  T2      3 → 4     Write 3 starts (same chunk)        No I/O (just counter++)

  T3      4 → 3     Write 1 completes                  No I/O yet
                    bitmap_end_write()

  T4      —         raid1d() calls                     ← BITMAP FLUSH STARTS
                    flush_pending_writes()             Writes dirty page to disk
                    → __bitmap_unplug()

  T5      —         After bitmap write completes       Data writes submitted
                    flush_bio_list()                   to underlying devices
                    → raid1_submit_write()

  T6      3 → 2     Write 2 completes                  Counter decremented
                    bitmap_end_write()                 md_bitmap_set_pending()
                    *bmc <= 2 triggers                 Page marked PENDING

  T7      2 → 1     Write 3 completes                  Counter decremented
                                                       Still marked PENDING

  T8      —         bitmap_daemon_work() runs          Clears bit from file
                    (periodic timer)                   (counter went to 1)
                    *bmc == 1 → clear bit

  5. Key Points

  Bitmap flush happens BEFORE data writes:

  From flush_bio_list() in raid1.c:900-913:
  static void flush_bio_list(struct r1conf *conf, struct bio *bio)
  {
      raid1_prepare_flush_writes(conf->mddev);  // ← Flush bitmap FIRST

      while (bio) {
          // Submit data writes AFTER bitmap is flushed
          wake_up_barrier(conf, idx);
          raid1_submit_write(bio);
          bio = next;
      }
  }

  Why this ordering matters:

  1. Crash safety: Bitmap must be on disk before data writes start
  2. If system crashes during data writes, bitmap shows which chunks need resync
  3. Multiple concurrent writes to same chunk only flush bitmap once

  Bitmap clear happens much later:

  From bitmap_daemon_work() md-bitmap.c:1586-1590:
  if (*bmc == 1 && !bitmap->need_sync) {
      /* We can clear the bit */
      *bmc = 0;
      md_bitmap_file_clear_bit(bitmap, block);  // ← Clear happens in background
  }

  This runs periodically (default 5 seconds) and clears bits when counter reaches 1.

  Summary

  For concurrent writes to the same chunk:

  1. ✅ First write (counter 0→2): Bitmap bit set in memory, page marked DIRTY
  2. ✅ Subsequent writes (counter ≥2): Just increment counter, no bitmap I/O
  3. ✅ Flush triggered: When raid1d() calls flush_pending_writes() → __bitmap_unplug()
  4. ✅ Timing: Bitmap written to disk BEFORE any data writes are submitted
  5. ✅ Efficiency: Multiple concurrent writes = single bitmap flush
  6. ✅ Cleanup: Bitmap bit cleared later by bitmap_daemon_work() when counter reaches 1

  This design ensures crash safety (bitmap always reflects pending writes) while minimizing bitmap I/O overhead (one flush per unplug,
  not per write).
```


#### How to manage in-memory and on-disk bitmap

- crate bitmap

bitmap_create()     //allocate in-memory bitmap


#### How is bitmap flushed to disk

- Build bitmap data
 
```
md_bitmap_start
    md_clone_bio
        md_account_bio
            raid1_write_request

md_bitmap_end
    md_end_clone_io

.start_write		= bitmap_start_write,
    bitmap_start_write      //update in-memory bitmap and counter
        md_bitmap_file_set_bit

```

- flush bitmap to disk

```
__bitmap_unplug
    md_bitmap_unplug_fn
        INIT_WORK_ONSTACK(&unplug_work.work, md_bitmap_unplug_fn)
            bitmap_unplug_async
                bitmap_unplug
                    .unplug                 = bitmap_unplug
                        mddev->bitmap_ops->unplug
                            bitmap_store
                            raid1_prepare_flush_writes
                                flush_bio_list
                                    flush_pending_writes
                                        freeze_array
                                            raid1_remove_disk
                                                .hot_remove_disk= raid1_remove_disk
                                            handle_read_error
                                                raid1d
                                            raid1_reshape
                                                .check_reshape  = raid1_reshape
                                            raid1_quiesce
                                                .quiesce        = raid1_quiesce
                                        raid1d
                                    raid1_unplug
                                        raid1_add_bio_to_plug(mddev, mbio, raid1_unplug, disks)
                                            raid1_write_request
                                                raid1_make_request
                                flush_pending_writes
                                    freeze_array  //raid1
                                    raid1d
                                    freeze_array  //raid10
                                    raid10d
                                raid10_unplug
                            raid5d
    bitmap_unplug
    bitmap_copy_from_slot
    __bitmap_resize>>


```

#### __bitmap_unplug



## replace raid1 bitmap with simple journal

raid1 uses bitmap for resync/recover in case of non-sync writes, one bit
represents one chunk.

bits are set as 1 during unplug before writing data to underlying disks,
and bits are cleared after writing data to disks successfully.

The recent lockless bitmap improvement increases per-chunk-bits to 8, which
can carry each chunk's status.

When I am looking at this change, I am wondering why this bitmap can't be
replaced with simple journal?

One big drawback for using bitmap this way is that it is really not
friendly for randwrite workloads.  For example, each plug can handle at
most 32 bios from upper layer(FS), if these 32bios belong to 32 chunks,
32 meta IOs can be generated before staring to write data to disks. The
recent lockless bitmap could be worse from this viewpoint because bitmap
size is increased 8 times.

If replacing bitmap with journal, just single sector meta write is required
for randwrite. And the journal usage can be pretty simple, such as,
one entry is 64bit(chunk_seq + nr_chunks) which is enough to cover very big
disk size & any chunk size. Before writing data to underlying disks, all
chunk_seq & nr_chunk from these bios can be built in one buffer, then write
this buffer to journal area in underlying disks. After data write is done,
all these journal entries can be zeroed & written to disks.

resync/recovery can be easy too, just read these entries from journal area,
and run resync.

### advantage

- bound meta write, most of times, just one sector write is enough

- needn't big bitmap memory for disks with TB level

- easy & simple to implement

- range tree can be used for checking if meta update is needed

