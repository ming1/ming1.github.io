---
title: Linux software RAID notes
category: tech
tags: [linux kernel, raid, IO]
---

* TOC
{:toc}

Linux software RAID(redundant array of independent disks) notes

# RAID1

## requirement

- reliability
    
    - deal with bad blocks

    - deal with lost power

- data integrity

## Linux kernel raid1 driver

### RAID1 write

```
  Knowledge Graph: raid1_write_request Function

  Level 0: Core Function

  raid1_write_request(mddev, bio, max_write_sectors)
  ├── Purpose: Handle WRITE requests for RAID1 arrays
  ├── Location: drivers/md/raid1.c:1475
  └── Return: void (async completion via bio callbacks)

  Level 1: Major Components & Data Flow

  raid1_write_request
  │
  ├── [1] INPUT VALIDATION & SYNCHRONIZATION
  │   ├── Cluster resync check
  │   ├── wait_barrier() - Barrier synchronization
  │   └── wait_blocked_rdev() - Device blocking check
  │
  ├── [2] R1BIO ALLOCATION & SETUP
  │   ├── alloc_r1bio() - Allocate request descriptor
  │   └── Initialize r1_bio->sectors
  │
  ├── [3] DEVICE SELECTION & BAD BLOCK HANDLING
  │   ├── Iterate all mirrors (disks * 2)
  │   ├── Check device status (Faulty, WriteMostly)
  │   ├── Bad block detection & avoidance
  │   └── Build target device list
  │
  ├── [4] BIO SPLITTING & BEHIND-WRITE SETUP
  │   ├── bio_submit_split_bioset() - Split if needed
  │   ├── raid1_start_write_behind() - Setup write-behind
  │   └── md_account_bio() - Accounting
  │
  ├── [5] BIO CLONING & DISPATCH
  │   ├── Clone bio for each mirror
  │   ├── Set completion callback (raid1_end_write_request)
  │   ├── Add to pending_bio_list or plug
  │   └── Wake raid1d thread
  │
  └── [6] COMPLETION TRACKING
      ├── r1_bio_write_done() - Decrements remaining count
      └── wake_up_barrier() - Unblock waiters

  Level 2: Key Data Structures

  Primary Structures

  struct r1bio (raid1.h:131)
  ├── atomic_t remaining              # Write completion counter
  ├── atomic_t behind_remaining       # Write-behind counter
  ├── sector_t sector                 # Starting sector
  ├── int sectors                     # Number of sectors
  ├── unsigned long state             # State flags (R1BIO_*)
  ├── struct bio *master_bio          # Original bio from upper layer
  ├── struct bio *behind_master_bio   # Write-behind data copy
  └── struct bio *bios[]              # Per-mirror bio array

  struct r1conf (raid1.h:52)
  ├── struct mddev *mddev             # MD device pointer
  ├── struct raid1_info *mirrors      # Mirror device array (2x raid_disks)
  ├── int raid_disks                  # Number of active disks
  ├── spinlock_t device_lock          # Device list lock
  ├── struct bio_list pending_bio_list # Pending writes queue
  ├── wait_queue_head_t wait_barrier  # Barrier wait queue
  ├── atomic_t *nr_pending            # Per-bucket pending count
  ├── atomic_t *nr_waiting            # Per-bucket waiting count
  ├── atomic_t *barrier               # Per-bucket barrier state
  └── mempool_t *r1bio_pool           # r1bio memory pool

  struct raid1_info (raid1.h:41)
  ├── struct md_rdev *rdev            # Device descriptor
  └── sector_t head_position          # Current head position

  State Flags (r1bio->state)

  R1BIO_Uptodate      # I/O completed successfully
  R1BIO_IsSync        # Synchronous I/O
  R1BIO_BehindIO      # Write-behind I/O active
  R1BIO_ReadError     # Read error occurred
  R1BIO_Returned      # Completion callback called
  R1BIO_MadeGood      # Bad block cleared
  R1BIO_WriteError    # Write error occurred
  R1BIO_FailFast      # Fast-fail enabled

  Level 3: Critical Function Dependencies

  Barrier Synchronization System

  wait_barrier(conf, sector_nr, nowait) [raid1.c:1143]
  ├── Purpose: Wait for resync/reshape to clear
  ├── Calls: _wait_barrier(conf, idx, nowait) [raid1.c:1031]
  ├── Uses: sector_to_idx(sector) - Hash sector to bucket
  ├── Blocks on: conf->wait_barrier wait queue
  └── Checks: conf->barrier[idx], conf->nr_pending[idx]

  wake_up_barrier(conf) [raid1.c:1095]
  ├── Purpose: Wake threads waiting on barrier
  └── Wakes: conf->wait_barrier wait queue

  Memory Allocation

  alloc_r1bio(mddev, bio) [raid1.c:118]
  ├── Allocates: sizeof(r1bio) + nr_mirrors * sizeof(struct bio*)
  ├── From: conf->r1bio_pool mempool
  ├── Initializes: master_bio, mddev, sector, state
  └── Returns: struct r1bio*

  bio_alloc_clone(bdev, bio, flags, bioset) [block layer]
  ├── Purpose: Clone bio for each mirror
  ├── Shares: bio_vec data (no data copy)
  └── Independent: bi_iter, bi_end_io, bi_private

  Bad Block Management

  is_badblock(rdev, sector, max_sectors, &first_bad, &bad_sectors)
  ├── Purpose: Check for known bad blocks
  ├── Returns: 0=good, 1=bad block found, -1=error
  ├── Updates: first_bad (first bad sector), bad_sectors (count)
  └── Logic: Adjust max_sectors to avoid bad blocks

  Write-Behind Support

  raid1_start_write_behind(mddev, r1_bio, bio) [raid1.c:1454]
  ├── Checks: md_bitmap_enabled(), behind_writes limit
  ├── Calls: alloc_behind_master_bio() - Copy data to pages
  ├── Sets: R1BIO_BehindIO flag
  └── Calls: bitmap_ops->start_behind_write()

  alloc_behind_master_bio(r1_bio, bio)
  ├── Allocates: Pages for data copy
  ├── Copies: bio data to allocated pages
  └── Stores: In r1_bio->behind_master_bio

  Completion Path

  raid1_end_write_request(bio) [raid1.c:447]
  ├── Called: By block layer on bio completion
  ├── Checks: Write errors, bad block updates
  ├── Updates: rdev statistics
  ├── Calls: r1_bio_write_done(r1_bio)
  └── Handles: Behind writes, error reporting

  r1_bio_write_done(r1_bio) [raid1.c:431]
  ├── Decrements: atomic_dec_and_test(&r1_bio->remaining)
  ├── On zero:
  │   ├── close_write(r1_bio) - Cleanup bitmap tracking
  │   ├── reschedule_retry() - If errors occurred
  │   └── raid_end_bio_io() - Complete original bio
  └── Purpose: Track all mirror write completions

  close_write(r1_bio) [raid1.c:415]
  ├── Calls: bitmap_ops->end_behind_write()
  ├── Calls: md_write_end(mddev)
  └── Purpose: End bitmap/metadata tracking

  Level 4: Deep Dependencies & External Interactions

  Bitmap Operations

  md_write_start(mddev, bio) [md.c:8779]
  ├── Called: Implicitly before write
  ├── Sets: mddev->in_sync = 0
  ├── Updates: writes_pending counter
  └── Waits: For superblock updates if needed

  bitmap_ops->start_write(mddev, offset, sectors) [md-bitmap.c:1685]
  ├── Gets: bitmap counter for chunk
  ├── Increments: Reference count
  ├── Calls: md_bitmap_file_set_bit() - Set disk bit
  └── Sets: BITMAP_PAGE_DIRTY flag

  bitmap_ops->end_write(mddev, offset, sectors) [md-bitmap.c:1739]
  ├── Decrements: Bitmap counter
  ├── If counter ≤ 2: Mark for cleanup
  └── Updates: events_cleared if array clean

  Bio Submission Paths

  raid1_add_bio_to_plug(mddev, mbio, raid1_unplug, disks)
  ├── Purpose: Try to add bio to current task's plug
  ├── Returns: true if plugged, false if needs immediate submit
  └── Avoids: Lock contention when plugging available

  Alternative: bio_list_add(&conf->pending_bio_list, mbio)
  ├── Protected by: conf->device_lock
  ├── Wakes: mddev->thread (raid1d)
  └── Processed by: raid1d daemon thread

  raid1d thread processing:
  └── flush_pending_writes(conf) [raid1.c:915]
      └── flush_bio_list(conf, bio) [raid1.c:900]
          ├── raid1_prepare_flush_writes() - Flush bitmap
          └── raid1_submit_write(bio) - Submit to block layer

  Cluster Integration

  mddev->cluster_ops->area_resyncing(mddev, WRITE, sector, end)
  ├── Purpose: Check if cluster node is resyncing this area
  ├── Returns: true if resync active
  └── Action: Wait on conf->wait_barrier if active

  Error Handling & Retry

  reschedule_retry(r1_bio) [raid1.c:1990]
  ├── Adds: r1_bio to conf->retry_list
  ├── Wakes: raid1d thread
  └── Purpose: Retry failed writes or update bad blocks

  raid_end_bio_io(r1_bio) [raid1-10.c:139]
  ├── Calls: bio_endio(master_bio)
  ├── Frees: r1_bio back to pool
  └── Purpose: Complete original bio to upper layer

  Device State Management

  Device Flags (in rdev->flags):
  ├── Faulty        # Device failed, skip writes
  ├── WriteMostly   # Prefer other devices for reads
  ├── WriteErrorSeen # Known write errors, check bad blocks
  ├── FailFast      # Don't retry, fail quickly
  └── CollisionCheck # Serialize writes (for write-behind)

  atomic_inc(&rdev->nr_pending)
  ├── Purpose: Prevent device removal during I/O
  └── Paired with: rdev_dec_pending() on completion

  Barrier Bucket System

  Barrier Buckets (64MB units):
  ├── Purpose: Fine-grained resync/write synchronization
  ├── Hash: sector_to_idx(sector >> BARRIER_UNIT_SECTOR_BITS)
  ├── Size: PAGE_SIZE / sizeof(atomic_t) buckets
  └── Tracks: nr_pending, nr_waiting, nr_queued, barrier per bucket

  Barrier Logic:
  ├── conf->barrier[idx] > 0    # Resync in progress, block writes
  ├── conf->nr_pending[idx]     # Active writes in this bucket
  └── conf->nr_waiting[idx]     # Threads waiting for bucket

  Write-Behind Mechanics

  Write-Behind Flow:
  1. Check: WriteMostly flag on device
  2. Allocate: behind_master_bio (data copy)
  3. Submit: Regular mirrors immediately
  4. Submit: WriteMostly mirrors with behind_master_bio
  5. Complete: When regular mirrors finish (fast path)
  6. Track: behind_remaining counter for WriteMostly completion
  7. Cleanup: Free behind_master_bio when all complete

  Benefits:
  ├── Low-latency completion for fast mirrors
  ├── Slow mirrors don't block application
  └── Bitmap tracking ensures data safety

  Data Flow Diagram

  Application Bio
        ↓
  raid1_make_request
        ↓
  raid1_write_request ──────────────────────────────┐
        ↓                                           ↓
  [Cluster Check] → wait if resyncing        md_write_start
        ↓                                           ↓
  [wait_barrier] → block if resync       bitmap_start_write
        ↓                                           ↓
  [alloc_r1bio]                          Set bitmap bits
        ↓
  [Select Mirrors] → Check Faulty/WriteMostly/BadBlocks
        ↓
  [Clone Bios] → bio_alloc_clone for each mirror
        ↓
  [Setup Callbacks] → raid1_end_write_request
        ↓
  [Dispatch] ───┬──→ raid1_add_bio_to_plug → Immediate submit
                └──→ pending_bio_list → raid1d → Submit later

        ↓ (async)
  [Block Layer] → Physical I/O to each mirror
        ↓
  raid1_end_write_request (per mirror completion)
        ↓
  r1_bio_write_done (atomic_dec remaining)
        ↓
  [All mirrors done?] ──No──→ return
        ↓ Yes
  close_write
        ↓
  bitmap_end_write
        ↓
  raid_end_bio_io → Complete original bio
        ↓
  Application notified

  Performance-Critical Paths

  1. Fast Path (no contention):
    - wait_barrier() → immediate pass
    - bio_alloc_clone() → from mempool
    - raid1_add_bio_to_plug() → plug successful
    - Parallel writes to all mirrors
  2. Slow Path (contention):
    - wait_barrier() → sleep on wait queue
    - Bad block detection → sector adjustment
    - Bio splitting → recursive submission
    - pending_bio_list → serialized through raid1d
  3. Bitmap Impact:
    - bitmap_start_write() → per-chunk bit set
    - Multiple random writes → multiple bitmap pages dirty
    - This is where your journal idea provides huge wins!

```

#### wait_barrier

```
  Knowledge Graph: wait_barrier() Function

  Level 0: Core Function

  wait_barrier(conf, sector_nr, nowait)
  ├── Purpose: Synchronize write I/O with resync/recovery operations
  ├── Location: drivers/md/raid1.c:1143
  ├── Returns: bool (true=proceed, false=would block with nowait)
  └── Role: Gate-keeper for write I/O during resync/management operations

  Level 1: Function Call Chain & Architecture

  wait_barrier(conf, sector_nr, nowait)
  ├── [1] sector_to_idx(sector_nr) → Convert sector to bucket index
  │   └── Returns: idx (0 to BARRIER_BUCKETS_NR-1)
  │
  └── [2] _wait_barrier(conf, idx, nowait) → Core barrier logic
      ├── Fast path: No barrier, return immediately
      ├── Slow path: Wait for barrier to drop
      └── Returns: bool (success/failure)

  Level 2: Core Implementation - _wait_barrier()

  _wait_barrier(conf, idx, nowait) [raid1.c:1031]
  │
  ├── PHASE 1: OPTIMISTIC INCREMENT
  │   ├── atomic_inc(&conf->nr_pending[idx])
  │   ├── smp_mb__after_atomic() - Memory barrier
  │   └── Purpose: Signal "I'm here" before checking barrier
  │
  ├── PHASE 2: FAST PATH CHECK
  │   ├── Check: !conf->array_frozen
  │   ├── Check: !conf->barrier[idx]
  │   └── If both true → return immediately (fast path)
  │
  ├── PHASE 3: SLOW PATH (barrier active or array frozen)
  │   ├── spin_lock_irq(&conf->resync_lock)
  │   ├── atomic_inc(&conf->nr_waiting[idx])
  │   ├── atomic_dec(&conf->nr_pending[idx])
  │   ├── wake_up_barrier(conf) - Wake freeze_array() if waiting
  │   │
  │   ├── [DECISION POINT]
  │   │   ├── If nowait=true → ret=false, skip wait
  │   │   └── If nowait=false → wait_event_lock_irq(...)
  │   │
  │   ├── Wait on: conf->wait_barrier
  │   ├── Condition: !conf->array_frozen && !conf->barrier[idx]
  │   ├── Hold lock: conf->resync_lock (released during sleep)
  │   │
  │   ├── On wakeup: atomic_inc(&conf->nr_pending[idx])
  │   ├── atomic_dec(&conf->nr_waiting[idx])
  │   └── spin_unlock_irq(&conf->resync_lock)
  │
  └── PHASE 4: RETURN
      └── Return bool (true=proceed, false=blocked with nowait)

  Level 3: The Barrier Bucket System

  Bucket Architecture

  BARRIER SYSTEM (64MB units per bucket)
  ├── BARRIER_UNIT_SECTOR_BITS = 17
  ├── BARRIER_UNIT_SECTOR_SIZE = 128K sectors = 64MB
  ├── BARRIER_BUCKETS_NR_BITS = PAGE_SHIFT - 2 (typically 10)
  ├── BARRIER_BUCKETS_NR = 1024 (on 4KB page systems)
  └── Total coverage: 1024 buckets × 64MB = 64GB range

  sector_to_idx(sector) [raid1.h:194]
  └── hash_long(sector >> 17, 10) → bucket index 0-1023

  Per-Bucket State Tracking

  Each bucket (idx) tracks:

  conf->nr_pending[idx]     # Active I/Os in this bucket
  ├── Incremented: When I/O enters (wait_barrier)
  ├── Decremented: When I/O completes (allow_barrier)
  └── Purpose: Track active normal I/O count

  conf->nr_waiting[idx]     # Threads waiting on barrier
  ├── Incremented: When thread sleeps in _wait_barrier()
  ├── Decremented: When thread wakes up
  └── Purpose: Track waiting I/O count

  conf->nr_queued[idx]      # Queued I/Os (during freeze)
  ├── Incremented: When I/O queued to pending_bio_list
  ├── Decremented: When I/O submitted from queue
  └── Purpose: Track queued I/O count

  conf->barrier[idx]        # Barrier level (resync active)
  ├── Incremented: In raise_barrier() (resync/recovery)
  ├── Decremented: In lower_barrier() (resync done)
  ├── Range: 0 (no barrier) to RESYNC_DEPTH (max concurrent)
  └── Purpose: Block normal I/O when resync active

  Global State

  conf->array_frozen        # Array management freeze
  ├── Set: freeze_array() - Management operations
  ├── Clear: unfreeze_array() - Resume normal operations
  └── Blocks: ALL I/O (read and write)

  conf->nr_sync_pending     # Total sync I/Os active
  ├── Incremented: raise_barrier()
  ├── Decremented: lower_barrier()
  └── Purpose: Track resync/recovery I/O count

  Level 4: Deep Dependencies & Synchronization Protocols

  Memory Ordering Protocol

  CRITICAL RACE PREVENTION (between _wait_barrier and raise_barrier)

  Thread A (_wait_barrier):              Thread B (raise_barrier):
  1. atomic_inc(nr_pending[idx])         1. atomic_inc(barrier[idx])
  2. smp_mb__after_atomic() ────────────→2. smp_mb__after_atomic()
  3. if (barrier[idx] == 0)              3. if (nr_pending[idx] == 0)
       return (fast path)                     continue (can raise)

  Without memory barriers:
  - Thread A might read old barrier[idx]=0 (before B's increment)
  - Thread B might read old nr_pending[idx]=0 (before A's increment)
  - BOTH proceed → RACE! (I/O happens during resync)

  With memory barriers:
  - At least ONE thread sees the other's update
  - Either A waits OR B waits → NO RACE

  Lock-Free Fast Path

  Optimization: Avoid locks when no barrier

  _wait_barrier() fast path:
  ├── atomic_inc(&nr_pending[idx])        # Lock-free
  ├── smp_mb__after_atomic()              # Memory fence
  ├── Check: !array_frozen && !barrier[idx]
  └── If true: Return without taking lock  # FAST!

  Statistics:
  - Normal operation (no resync): 99.9% fast path
  - During resync: Only affected buckets take slow path
  - Performance: Lock-free increment + read + branch

  Wait Queue Integration

  conf->wait_barrier (wait_queue_head_t)
  ├── Purpose: Sleep/wake synchronization point
  │
  ├── Waiters:
  │   ├── _wait_barrier() - Normal I/O waiting for barrier drop
  │   ├── wait_read_barrier() - Read I/O waiting for unfreeze
  │   ├── raise_barrier() - Resync waiting for I/O drain
  │   └── freeze_array() - Management waiting for I/O drain
  │
  ├── Wakers:
  │   ├── wake_up_barrier() - Check and wake if sleepers
  │   ├── allow_barrier() - I/O completed
  │   ├── lower_barrier() - Resync completed
  │   ├── unfreeze_array() - Unfreeze completed
  │   └── flush_bio_list() - Queued I/O processing
  │
  └── Lock held during wait: conf->resync_lock

  Interaction with raise_barrier()

  raise_barrier(conf, sector_nr) [raid1.c:970]
  ├── Purpose: Block normal I/O to allow resync
  │
  ├── PHASE 1: Wait for no waiting threads
  │   └── wait_event_lock_irq(!nr_waiting[idx])
  │
  ├── PHASE 2: Set barrier
  │   ├── atomic_inc(&barrier[idx])
  │   └── smp_mb__after_atomic()
  │
  ├── PHASE 3: Wait for I/O drain
  │   └── wait_event_lock_irq(
  │           !array_frozen &&
  │           !nr_pending[idx] &&
  │           barrier[idx] < RESYNC_DEPTH)
  │
  └── PHASE 4: Success
      └── atomic_inc(&nr_sync_pending)

  Coordination with _wait_barrier():
  - raise_barrier sets barrier[idx]=1
  - _wait_barrier sees barrier[idx]≠0
  - _wait_barrier waits until barrier[idx]=0
  - lower_barrier() decrements barrier[idx]
  - _wait_barrier() wakes and proceeds

  Interaction with freeze_array()

  freeze_array(conf, extra) [raid1.c:1176]
  ├── Purpose: Quiesce ALL I/O for management operations
  │
  ├── PHASE 1: Set freeze flag
  │   └── conf->array_frozen = 1
  │
  ├── PHASE 2: Wait for I/O drain
  │   └── wait_event_lock_irq_cmd(
  │           get_unqueued_pending() == extra,
  │           flush_pending_writes())
  │
  └── Purpose: Drain all flying I/O

  get_unqueued_pending(conf) [raid1.c:1164]
  ├── ret = nr_sync_pending
  ├── for each bucket:
  │   └── ret += (nr_pending[idx] - nr_queued[idx])
  └── Returns: Count of I/O not yet queued

  _wait_barrier() cooperation:
  - Sees array_frozen=1
  - Moves to slow path
  - Decrements nr_pending (helps drain)
  - Increments nr_waiting
  - Wakes freeze_array() (via wake_up_barrier)
  - Waits for unfreeze

  Interaction with allow_barrier()

  allow_barrier(conf, sector_nr) [raid1.c:1156]
  ├── Calls: _allow_barrier(conf, idx)
  └── Purpose: Signal I/O completion

  _allow_barrier(conf, idx) [raid1.c:1150]
  ├── atomic_dec(&nr_pending[idx])
  ├── wake_up_barrier(conf)
  └── Purpose: Release barrier slot

  Typical I/O lifecycle:
  1. wait_barrier() → inc nr_pending[idx]
  2. Submit I/O to devices
  3. I/O completes
  4. allow_barrier() → dec nr_pending[idx]
  5. If resync waiting: wake_up allows it to proceed

  NOWAIT Flag Handling

  NOWAIT Support (for REQ_NOWAIT bios):

  Traditional blocking I/O:
  ├── wait_barrier(conf, sector, false)
  ├── If barrier active: Sleep until clear
  └── Always returns true

  Non-blocking I/O (NOWAIT):
  ├── wait_barrier(conf, sector, true)
  ├── If barrier active: Return false immediately
  └── Caller must handle failure (return -EAGAIN to user)

  Code flow:
  if (!wait_barrier(conf, sector, bio->bi_opf & REQ_NOWAIT)) {
      bio_wouldblock_error(bio);  # Return -EAGAIN
      return;
  }

  Purpose: Support async I/O (io_uring, AIO)

  State Transition Diagrams

  Normal I/O Flow

  START
    ↓
  wait_barrier(conf, sector, nowait)
    ↓
  sector_to_idx(sector) → idx
    ↓
  _wait_barrier(conf, idx, nowait)
    ↓
  atomic_inc(nr_pending[idx])
    ↓
  smp_mb__after_atomic()
    ↓
  [CHECK: array_frozen || barrier[idx]]
    ↓                           ↓
    NO (Fast Path)             YES (Slow Path)
    ↓                           ↓
    Return true                 spin_lock_irq(resync_lock)
    ↓                           ↓
  [Proceed with I/O]           atomic_inc(nr_waiting[idx])
    ↓                           atomic_dec(nr_pending[idx])
  [I/O Completes]               wake_up_barrier(conf)
    ↓                           ↓
  allow_barrier(conf, sector)  [NOWAIT?]
    ↓                           ↓              ↓
  atomic_dec(nr_pending[idx])  NO             YES
  wake_up_barrier(conf)         ↓              ↓
    ↓                          wait_event...   Return false
  DONE                          ↓              ↓
                           [Wait for clear]   [Caller handles]
                                ↓
                           atomic_inc(nr_pending[idx])
                           atomic_dec(nr_waiting[idx])
                           spin_unlock_irq(resync_lock)
                                ↓
                           Return true
                                ↓
                           [Proceed with I/O]

  Resync Coordination

  NORMAL I/O THREAD              RESYNC THREAD
        ↓                              ↓
  wait_barrier(sector)          raise_barrier(sector)
        ↓                              ↓
  atomic_inc(nr_pending[idx])   wait(!nr_waiting[idx])
        ↓                              ↓
  smp_mb() ←───────────────────→ atomic_inc(barrier[idx])
        ↓                              ↓
  Check barrier[idx]             smp_mb()
        ↓                              ↓
  If != 0:                       wait(!nr_pending[idx])
    ↓                                  ↓
    Wait on barrier[idx]=0       [When nr_pending=0]
    ↓                                  ↓
  [Barrier drops]               Do resync I/O
    ↓                                  ↓
  Proceed with I/O              lower_barrier()
    ↓                                  ↓
  allow_barrier()               atomic_dec(barrier[idx])
                                       ↓
                                wake_up_barrier()
                                       ↓
                                [Normal I/O unblocked]

  Performance Characteristics

  Fast Path (No Contention)

  Cost: ~10-20 CPU cycles
  ├── 1 atomic increment (nr_pending)
  ├── 1 memory barrier (smp_mb)
  ├── 2 atomic reads (array_frozen, barrier)
  └── 1 conditional branch

  Slow Path (Barrier Active)

  Cost: Context switch + wait time
  ├── 1 spinlock acquire/release
  ├── 2 atomic operations (nr_waiting, nr_pending)
  ├── 1 wait queue sleep/wake
  └── Wait duration: Until resync clears bucket

  Bucket Hash Benefits

  Parallelism: 1024 independent buckets
  ├── Resync in bucket 0: Only bucket 0 I/O blocks
  ├── I/O to other buckets: Fast path continues
  └── Effective concurrency: ~1024-way parallel

  Error Scenarios

  NOWAIT Failure

  Trigger: REQ_NOWAIT bio hits active barrier
  Flow:
  ├── wait_barrier(conf, sector, true)
  ├── _wait_barrier() returns false
  ├── bio_wouldblock_error(bio)
  └── Return -EAGAIN to application

  Recovery Interrupt

  Trigger: MD_RECOVERY_INTR set during resync
  Flow (in raise_barrier):
  ├── detect MD_RECOVERY_INTR
  ├── atomic_dec(&barrier[idx])
  ├── wake_up(&wait_barrier)
  └── Return -EINTR

```

#### resync thread

```
  RAID1 Resync Thread: When, How, and Why

  What is the Resync Thread?

  The resync thread is a kernel thread that synchronizes data between RAID1 mirrors. It runs in the background to ensure all mirror
  devices contain identical data.

  When is Resync Triggered?

  1. Device Failures & Errors

  // Write error during normal I/O (raid1.c:2091-2094)
  set_bit(WriteErrorSeen, &rdev->flags);
  if (!test_and_set_bit(WantReplacement, &rdev->flags))
      set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);

  // Unrecoverable read error (raid1.c:2352)
  set_bit(MD_RECOVERY_INTR, &mddev->recovery);

  Triggers:
  - Write error on a mirror device
  - Device marked for replacement
  - Persistent read errors requiring rebuild

  2. Adding New Devices

  // Adding a new disk to array (raid1.c:3440-3442)
  set_bit(MD_RECOVERY_RECOVER, &mddev->recovery);
  set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);
  md_wakeup_thread(mddev->thread);

  Triggers:
  - Hot-add of new disk to RAID1
  - Replacing failed disk
  - Adding spare disk that needs sync

  3. Array Resize/Reshape

  // Resize operation (raid1.c:3345)
  if (mddev->resync_offset > mddev->dev_sectors) {
      mddev->resync_offset = mddev->dev_sectors;
      set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);
  }

  Triggers:
  - Expanding array size
  - Growing device capacity

  4. Unclean Shutdown Recovery

  // Bitmap-driven recovery after crash
  if (mddev->bitmap && dirty_bits_found) {
      set_bit(MD_RECOVERY_NEEDED, &mddev->recovery);
  }

  Triggers:
  - System crash with dirty bitmap
  - Unclean array shutdown
  - Power failure recovery

  5. Manual Triggers

  # Administrator-initiated operations
  echo check > /sys/block/md0/md/sync_action    # Data verification
  echo repair > /sys/block/md0/md/sync_action   # Fix inconsistencies
  echo resync > /sys/block/md0/md/sync_action   # Force full resync

  How is Resync Thread Started?

  Complete Call Chain:

  [EVENT: Write error, device add, etc.]
           ↓
  set_bit(MD_RECOVERY_NEEDED, &mddev->recovery)
           ↓
  md_wakeup_thread(mddev->thread)   # Wake raid1d daemon
           ↓
  raid1d() → md_check_recovery(mddev)  [md.c:10003]
           ↓
  [Check MD_RECOVERY_NEEDED flag]  [md.c:10122]
           ↓
  queue_work(md_misc_wq, &mddev->sync_work)  [md.c:10124]
           ↓
  [Work queue executes]
           ↓
  md_start_sync(work_struct *ws)  [md.c:9877]
           ↓
  md_register_thread(md_do_sync, mddev, "resync")  [md.c:9922]
           ↓
  mddev->sync_thread created
           ↓
  md_wakeup_thread(mddev->sync_thread)  [md.c:9940]
           ↓
  [Kernel thread starts]
           ↓
  md_do_sync(thread)  [md.c:9305]
           ↓
  [Loop: Call personality-specific sync_request]
           ↓
  raid1_sync_request(mddev, sector_nr, max_sector, skipped)  [raid1.c:2777]
           ↓
  [Read from good disk, write to out-of-sync disk]
           ↓
  [Repeat until all sectors synchronized]
           ↓
  set_bit(MD_RECOVERY_DONE, &mddev->recovery)
           ↓
  md_check_recovery() → md_reap_sync_thread()
           ↓
  [Thread cleanup, activate spares]

  Why Does Resync Thread Exist? (Motivations)

  1. Data Consistency After Failures

  Problem: Write to mirror A succeeds, write to mirror B fails
  Mirror A: [Old Data] → [New Data] ✓
  Mirror B: [Old Data] → [Old Data] ✗ (write failed)

  Result: Mirrors are inconsistent!

  Solution: Resync thread copies correct data from A to B
  Resync: Read Mirror A → Write to Mirror B
  Result: Both mirrors now have [New Data]

  2. Crash Recovery with Bitmap

  Problem: System crashes during writes
  Bitmap shows dirty chunks: 100, 105, 200, 300
  (These chunks may be inconsistent between mirrors)

  Solution: Resync only dirty bitmap chunks
  For each dirty chunk:
      Choose authoritative source
      Copy to all other mirrors
      Clear bitmap bit

  Much faster than full array scan!

  3. New Device Integration

  Problem: Adding empty disk to active array
  Mirror A: [All user data]
  Mirror B: [Empty / old data]

  Can't serve reads from Mirror B yet!

  Solution: Resync thread copies all data
  raid1_sync_request() copies:
      Sector 0-128KB from A to B
      Sector 128KB-256KB from A to B
      ...
      Until entire disk synchronized

  Then Mark Mirror B as "In Sync"

  4. Prevent Read Errors

  Problem: Silent data corruption or bit rot
  Mirror A: [Sector 1000] = 0xDEADBEEF (correct)
  Mirror B: [Sector 1000] = 0xDEADBAD1 (corrupted)

  Normal reads might return bad data!

  Solution: Check/Repair operations via resync
  echo check > /sys/block/md0/md/sync_action

  Resync compares all mirrors:
      If mismatch found → Report error
      If repair mode → Fix using majority vote

  5. Background Operation = No Downtime

  Key Benefit: Array remains online during resync!

  User I/O Thread:                Resync Thread:
      ↓                                ↓
  wait_barrier(sector 5000)       raise_barrier(sector 1000)
      ↓                                ↓
  [sector 5000 != 1000]          [Blocks only sector 1000]
      ↓                                ↓
  Proceed immediately!            Read sector 1000 from good disk
      ↓                                ↓
  Write to all mirrors            Write to out-of-sync disk
      ↓                                ↓
  Complete                        lower_barrier(sector 1000)
                                       ↓
                                  Move to next chunk

  Result:
  - User I/O continues uninterrupted
  - Resync progresses in background
  - Only 64MB buckets are briefly locked

  Resync Thread Operation

  Main Loop (md_do_sync)

  md_do_sync(thread) {
      while (sector < max_sector) {
          // Call RAID1-specific sync function
          sectors = raid1_sync_request(mddev, sector, max_sector, &skipped);

          if (sectors == 0)
              break;  // Done or error

          sector += sectors;

          // Throttle to avoid overwhelming system
          if (speed_limit && too_fast)
              schedule_timeout();

          // Check for interruption
          if (MD_RECOVERY_INTR)
              break;
      }

      set_bit(MD_RECOVERY_DONE, &mddev->recovery);
  }

  RAID1-Specific Sync (raid1_sync_request)

  raid1_sync_request(mddev, sector_nr, max_sector, skipped) {
      // 1. Determine which disk has good data
      for_each_mirror {
          if (disk_is_in_sync && no_bad_blocks)
              source_disk = this_disk;
      }

      // 2. Raise barrier for this sector range
      raise_barrier(conf, sector_nr);

      // 3. Read from good disk
      bio = bio_alloc_for_read(source_disk, sector_nr, RESYNC_SECTORS);
      submit_bio_wait(bio);

      // 4. Write to out-of-sync disks
      for_each_mirror {
          if (!disk_is_in_sync || has_bad_blocks) {
              clone_bio = bio_clone(bio);
              clone_bio->bi_end_io = resync_end_io;
              submit_bio(clone_bio);
          }
      }

      // 5. Wait for all writes to complete
      wait_for_completion(&resync_io_done);

      // 6. Lower barrier
      lower_barrier(conf, sector_nr);

      // 7. Update bitmap (clear dirty bits)
      md_bitmap_end_sync(mddev, sector_nr, &sync_blocks);

      return RESYNC_SECTORS;  // Processed chunk size
  }

  Performance Tuning

  Speed Limits

  # Limit resync speed to 50 MB/s (avoid overwhelming system)
  echo 50000 > /sys/block/md0/md/sync_speed_min
  echo 100000 > /sys/block/md0/md/sync_speed_max

  # Current speed
  cat /proc/mdstat
  # md0 : active raid1 sda1[0] sdb1[1]
  #       resync = 45.2% (200GB/440GB) finish=3.5min speed=85000K/sec

  Suspend/Resume

  # Pause resync (for heavy workload period)
  echo frozen > /sys/block/md0/md/sync_action

  # Resume
  echo idle > /sys/block/md0/md/sync_action

  Summary

  The resync thread exists to:
  1. Maintain data consistency across all mirrors
  2. Recover from failures without manual intervention
  3. Integrate new devices into active arrays
  4. Verify data integrity periodically
  5. Operate in background without service interruption

  It's triggered by:
  - Device failures and errors
  - Adding/replacing disks
  - Unclean shutdowns (bitmap recovery)
  - Manual administrator commands
  - Array reshape operations

  It works by:
  - Reading from known-good mirror
  - Writing to out-of-sync mirrors
  - Using barrier system to coordinate with normal I/O
  - Processing 64MB chunks at a time
  - Updating bitmap to track progress

  This design allows RAID1 to self-heal automatically while remaining fully operational during recovery!
```


### RAID1 discard

### write cache for RAID1

- is battery a must for write cache support?

### RAID1 write behind


### RAID1 bitmap

#### How RAID1 Uses Bitmaps for Data Consistency

Answered from AI:

```
RAID1 uses write-intent bitmaps as a mechanism to track which regions of the array might be inconsistent due to incomplete write
operations. Here's how it works:

1. Bitmap Structure and Purpose

The bitmap tracks "dirty" regions of the array that have pending or recently completed write operations. Each bit in the bitmap
corresponds to a chunk of data (typically 64KB-4MB). When a bit is set, it indicates that the corresponding region may be inconsistent
 between mirrors.

2. Write Operation Flow with Bitmap

When a write request comes in (raid1_write_request in /drivers/md/raid1.c:1455), RAID1 follows this sequence:

1. Start Write Tracking (md_write_start at line 1712):
  - Calls mddev->bitmap_ops->start_write() to mark the bitmap region as dirty
  - This happens in bitmap_start_write() which increments a counter for the affected bitmap chunk
2. Write to All Mirrors:
  - RAID1 creates bio clones for each mirror device
  - Submits writes to all working mirrors in parallel
  - Each write has raid1_end_write_request as its completion callback
3. End Write Tracking (close_write() at line 428):
  - Called when all mirror writes complete successfully
  - Calls mddev->bitmap_ops->end_write() to decrement the bitmap counter
  - When counter reaches zero, the bitmap bit can be cleared during the next bitmap sync

3. Key Bitmap Functions

- bitmap_start_write() (md-bitmap.c:1685): Sets the dirty bit and increments a reference counter
- bitmap_end_write() (md-bitmap.c:1739): Decrements the counter; when it reaches zero, marks the chunk as pending for cleanup
- bitmap_dirty_bits(): Explicitly marks regions as dirty during recovery operations

4. Ensuring Consistency

The bitmap ensures consistency in several ways:

1. Write Ordering: Before any data is written to mirrors, the bitmap is updated to mark the region as dirty
2. Reference Counting: Each ongoing write increments a counter; the bit can only be cleared when all writes complete
3. Degraded Mode Handling: If the array is degraded, bitmap bits are marked as "NEEDED" to ensure they're resync'd later
4. Recovery: During resync/recovery, the bitmap identifies which regions need to be synchronized between mirrors

5. Recovery and Resync

When the system restarts after an unclean shutdown:
- bitmap_start_sync() is used during resync to check which regions need synchronization
- Only regions with set bitmap bits are resync'd, making recovery much faster
- The bitmap ensures no data is lost due to partial writes

6. Behind Writes Optimization

RAID1 supports "write-behind" for WriteMostly devices:
- The bitmap tracks behind writes separately (start_behind_write/end_behind_write)
- This allows the array to acknowledge writes before they complete on slower devices
- The bitmap ensures these delayed writes are properly tracked

7. Error Handling

If write errors occur:
- The bitmap keeps the affected regions marked as dirty
- Failed regions will be resync'd during recovery
- This prevents data loss from undetected write failures

The bitmap essentially provides a journal of "in-flight" write operations, ensuring that any region that was being written to during a
 system failure will be properly synchronized during recovery, maintaining data consistency across all mirror devices.

```

#### Questions

- How to deal with sync when creating new raid1? always sync?

- Is counter included in bitmap? No.

one bit per chunk for traditional bitmap, but it becomes one byte for
llbitmap.

- why is bitmap flushed to every underlying disks? why not just flush to
  one disk?

- how to deal with raid1 IO failure?

    -- if one WRITE on single member disk fails, how to handle the failure?

- Why does RAID1 need IO barrier?

    -- for which use cases?


#### How to manage in-memory and on-disk bitmap

- crate bitmap

bitmap_create()     //allocate in-memory bitmap


#### How is bitmap flushed to disk

- Build bitmap data
 
```
md_bitmap_start
    md_clone_bio
        md_account_bio
            raid1_write_request

md_bitmap_end
    md_end_clone_io

.start_write		= bitmap_start_write,
    bitmap_start_write      //update in-memory bitmap and counter
        md_bitmap_file_set_bit

```

- flush bitmap to disk

```
__bitmap_unplug
    md_bitmap_unplug_fn
        INIT_WORK_ONSTACK(&unplug_work.work, md_bitmap_unplug_fn)
            bitmap_unplug_async
                bitmap_unplug
                    .unplug                 = bitmap_unplug
                        mddev->bitmap_ops->unplug
                            bitmap_store
                            raid1_prepare_flush_writes
                                flush_bio_list
                                    flush_pending_writes
                                        freeze_array
                                            raid1_remove_disk
                                                .hot_remove_disk= raid1_remove_disk
                                            handle_read_error
                                                raid1d
                                            raid1_reshape
                                                .check_reshape  = raid1_reshape
                                            raid1_quiesce
                                                .quiesce        = raid1_quiesce
                                        raid1d
                                    raid1_unplug
                                        raid1_add_bio_to_plug(mddev, mbio, raid1_unplug, disks)
                                            raid1_write_request
                                                raid1_make_request
                                flush_pending_writes
                                    freeze_array  //raid1
                                    raid1d
                                    freeze_array  //raid10
                                    raid10d
                                raid10_unplug
                            raid5d
    bitmap_unplug
    bitmap_copy_from_slot
    __bitmap_resize>>


```

#### __bitmap_unplug



## replace raid1 bitmap with simple journal

raid1 uses bitmap for resync/recover in case of non-sync writes, one bit
represents one chunk.

bits are set as 1 during unplug before writing data to underlying disks,
and bits are cleared after writing data to disks successfully.

The recent lockless bitmap improvement increases per-chunk-bits to 8, which
can carry each chunk's status.

When I am looking at this change, I am wondering why this bitmap can't be
replaced with simple journal?

One big drawback for using bitmap this way is that it is really not
friendly for randwrite workloads.  For example, each plug can handle at
most 32 bios from upper layer(FS), if these 32bios belong to 32 chunks,
32 meta IOs can be generated before staring to write data to disks. The
recent lockless bitmap could be worse from this viewpoint because bitmap
size is increased 8 times.

If replacing bitmap with journal, just single sector meta write is required
for randwrite. And the journal usage can be pretty simple, such as,
one entry is 64bit(chunk_seq + nr_chunks) which is enough to cover very big
disk size & any chunk size. Before writing data to underlying disks, all
chunk_seq & nr_chunk from these bios can be built in one buffer, then write
this buffer to journal area in underlying disks. After data write is done,
all these journal entries can be zeroed & written to disks.

resync/recovery can be easy too, just read these entries from journal area,
and run resync.

