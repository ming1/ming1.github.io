---
title: Linux software RAID notes
category: tech
tags: [linux kernel, raid, IO]
---

* TOC
{:toc}

Linux software RAID(redundant array of independent disks) notes

# RAID1

## requirement

- reliability
    
    - deal with bad blocks

    - deal with lost power

- data integrity

## Linux kernel raid1 driver

### RAID1 write

### RAID1 discard

### write cache for RAID1

- is battery a must for write cache support?

### RAID1 write behind


### RAID1 bitmap

#### How RAID1 Uses Bitmaps for Data Consistency

Answered from AI:

```
RAID1 uses write-intent bitmaps as a mechanism to track which regions of the array might be inconsistent due to incomplete write
operations. Here's how it works:

1. Bitmap Structure and Purpose

The bitmap tracks "dirty" regions of the array that have pending or recently completed write operations. Each bit in the bitmap
corresponds to a chunk of data (typically 64KB-4MB). When a bit is set, it indicates that the corresponding region may be inconsistent
 between mirrors.

2. Write Operation Flow with Bitmap

When a write request comes in (raid1_write_request in /drivers/md/raid1.c:1455), RAID1 follows this sequence:

1. Start Write Tracking (md_write_start at line 1712):
  - Calls mddev->bitmap_ops->start_write() to mark the bitmap region as dirty
  - This happens in bitmap_start_write() which increments a counter for the affected bitmap chunk
2. Write to All Mirrors:
  - RAID1 creates bio clones for each mirror device
  - Submits writes to all working mirrors in parallel
  - Each write has raid1_end_write_request as its completion callback
3. End Write Tracking (close_write() at line 428):
  - Called when all mirror writes complete successfully
  - Calls mddev->bitmap_ops->end_write() to decrement the bitmap counter
  - When counter reaches zero, the bitmap bit can be cleared during the next bitmap sync

3. Key Bitmap Functions

- bitmap_start_write() (md-bitmap.c:1685): Sets the dirty bit and increments a reference counter
- bitmap_end_write() (md-bitmap.c:1739): Decrements the counter; when it reaches zero, marks the chunk as pending for cleanup
- bitmap_dirty_bits(): Explicitly marks regions as dirty during recovery operations

4. Ensuring Consistency

The bitmap ensures consistency in several ways:

1. Write Ordering: Before any data is written to mirrors, the bitmap is updated to mark the region as dirty
2. Reference Counting: Each ongoing write increments a counter; the bit can only be cleared when all writes complete
3. Degraded Mode Handling: If the array is degraded, bitmap bits are marked as "NEEDED" to ensure they're resync'd later
4. Recovery: During resync/recovery, the bitmap identifies which regions need to be synchronized between mirrors

5. Recovery and Resync

When the system restarts after an unclean shutdown:
- bitmap_start_sync() is used during resync to check which regions need synchronization
- Only regions with set bitmap bits are resync'd, making recovery much faster
- The bitmap ensures no data is lost due to partial writes

6. Behind Writes Optimization

RAID1 supports "write-behind" for WriteMostly devices:
- The bitmap tracks behind writes separately (start_behind_write/end_behind_write)
- This allows the array to acknowledge writes before they complete on slower devices
- The bitmap ensures these delayed writes are properly tracked

7. Error Handling

If write errors occur:
- The bitmap keeps the affected regions marked as dirty
- Failed regions will be resync'd during recovery
- This prevents data loss from undetected write failures

The bitmap essentially provides a journal of "in-flight" write operations, ensuring that any region that was being written to during a
 system failure will be properly synchronized during recovery, maintaining data consistency across all mirror devices.

```

#### Questions

- How to deal with sync when creating new raid1? always sync?

- Is counter included in bitmap? No.

one bit per chunk for traditional bitmap, but it becomes one byte for
llbitmap.

- why is bitmap flushed to every underlying disks? why not just flush to
  one disk?

- how to deal with raid1 IO failure?

    -- if one WRITE on single member disk fails, how to handle the failure?

- Why does RAID1 need IO barrier?

    -- for which use cases?


#### How to manage in-memory and on-disk bitmap

- crate bitmap

bitmap_create()     //allocate in-memory bitmap


#### How is bitmap flushed to disk

- Build bitmap data
 
```
md_bitmap_start
    md_clone_bio
        md_account_bio
            raid1_write_request

md_bitmap_end
    md_end_clone_io

.start_write		= bitmap_start_write,
    bitmap_start_write      //update in-memory bitmap and counter
        md_bitmap_file_set_bit

```

- flush bitmap to disk

```
__bitmap_unplug
    md_bitmap_unplug_fn
        INIT_WORK_ONSTACK(&unplug_work.work, md_bitmap_unplug_fn)
            bitmap_unplug_async
                bitmap_unplug
                    .unplug                 = bitmap_unplug
                        mddev->bitmap_ops->unplug
                            bitmap_store
                            raid1_prepare_flush_writes
                                flush_bio_list
                                    flush_pending_writes
                                        freeze_array
                                            raid1_remove_disk
                                                .hot_remove_disk= raid1_remove_disk
                                            handle_read_error
                                                raid1d
                                            raid1_reshape
                                                .check_reshape  = raid1_reshape
                                            raid1_quiesce
                                                .quiesce        = raid1_quiesce
                                        raid1d
                                    raid1_unplug
                                        raid1_add_bio_to_plug(mddev, mbio, raid1_unplug, disks)
                                            raid1_write_request
                                                raid1_make_request
                                flush_pending_writes
                                    freeze_array  //raid1
                                    raid1d
                                    freeze_array  //raid10
                                    raid10d
                                raid10_unplug
                            raid5d
    bitmap_unplug
    bitmap_copy_from_slot
    __bitmap_resize>>


```

#### __bitmap_unplug



## replace raid1 bitmap with simple journal

raid1 uses bitmap for resync/recover in case of non-sync writes, one bit
represents one chunk.

bits are set as 1 during unplug before writing data to underlying disks,
and bits are cleared after writing data to disks successfully.

The recent lockless bitmap improvement increases per-chunk-bits to 8, which
can carry each chunk's status.

When I am looking at this change, I am wondering why this bitmap can't be
replaced with simple journal?

One big drawback for using bitmap this way is that it is really not
friendly for randwrite workloads.  For example, each plug can handle at
most 32 bios from upper layer(FS), if these 32bios belong to 32 chunks,
32 meta IOs can be generated before staring to write data to disks. The
recent lockless bitmap could be worse from this viewpoint because bitmap
size is increased 8 times.

If replacing bitmap with journal, just single sector meta write is required
for randwrite. And the journal usage can be pretty simple, such as,
one entry is 64bit(chunk_seq + nr_chunks) which is enough to cover very big
disk size & any chunk size. Before writing data to underlying disks, all
chunk_seq & nr_chunk from these bios can be built in one buffer, then write
this buffer to journal area in underlying disks. After data write is done,
all these journal entries can be zeroed & written to disks.

resync/recovery can be easy too, just read these entries from journal area,
and run resync.

