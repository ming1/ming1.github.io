---
title: Understanding GPU
category: tech
tags: [gpu, programming, memory, AI, storage]
---

title:  Understanding GPU

* TOC
{:toc}


# GPU overview

## overview


## GPU memory

[A synthetic benchmarking tool to measure peak capabilities of opencl devices](https://github.com/krrishnarraj/clpeak)


## platform & software eco


# GPU Performance Background User's Guide

[GPU Performance Background User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html)

## Overview

The basic structure of a GPU (GPU Architecture Fundamentals)

How operations are divided and executed in parallel (GPU Execution Model)

How to estimate performance limitations with arithmetic intensity (Understanding Performance)

Loose categories of deep learning operations and the performance limitations that tend to apply to each (DNN Operation Categories)


## GPU Architecture Fundamentals

![Simplified view of the GPU architecture](/assets/images/simple-gpu-arch.svg)

```
The GPU is a highly parallel processor architecture, composed of processing 
elements and a memory hierarchy. At a high level, NVIDIA® GPUs consist of a 
number of Streaming Multiprocessors (SMs), on-chip L2 cache, and high-bandwidth 
DRAM. Arithmetic and other instructions are executed by the SMs; data and code 
are accessed from DRAM via the L2 cache. As an example, an NVIDIA A100 GPU contains 
108 SMs, a 40 MB L2 cache, and up to 2039 GB/s bandwidth from 80 GB of HBM2 memory.
```

![Multiply-add operations per clock per SM](/assets/images/multi-add-op.svg)

```
Each SM has its own instruction schedulers and various instruction execution 
pipelines. Multiply-add is the most frequent operation in modern neural networks, 
acting as a building block for fully-connected and convolutional layers, both of 
which can be viewed as a collection of vector dot-products. The following table 
shows a single SM’s multiply-add operations per clock for various data types on 
NVIDIA’s recent GPU architectures. Each multiply-add comprises two operations, 
thus one would multiply the throughput in the table by 2 to get FLOP counts per 
clock. To get the FLOPS rate for GPU one would then multiply these by the number 
of SMs and SM clock rate. For example, an A100 GPU with 108 SMs and 1.41 GHz clock 
rate has peak dense throughputs of 156 TF32 TFLOPS and 312 FP16 TFLOPS (throughputs 
achieved by applications depend on a number of factors discussed throughout this 
document).
```


## GPU Execution Model


# Roadmap: Understanding GPU Architecture

[Roadmap: Understanding GPU Architecture](https://cvw.cac.cornell.edu/gpu-architecture)

## GPU Characteristics

The hardware design for graphics processing units (GPUs) is optimized for highly parallel processing. 


### Design: GPU vs. CPU

GPUs were originally designed to render graphics. They work very well for shading, texturing, and
rendering the thousands of independent polygons that comprise a 3D object. CPUs, on the other hand,
are meant to control the logical flow of any general-purpose program, where lots of number crunching
may (or may not) be involved. Due to these very different roles, GPUs are characterized by many
more processing units and higher aggregate memory bandwidth, while CPUs feature more sophisticated
instruction processing and faster clock speed.


- CPU

CPUs can handle more complex workflows compared to GPUs.

CPUs don't have as many arithmetic logic units or floating point units as GPUs (the small green boxes
above, roughly speaking), but the ALUs and FPUs in a CPU core are individually more capable.

CPUs have more cache memory than GPUs.

### Performance: GPU vs. CPU

GPUs and CPUs are intended for fundamentally different types of workloads. CPUs are typically designed
for multitasking and fast serial processing, while GPUs are designed to produce high computational
throughput using their massively parallel architectures.

### Heterogeneous Applications

It turns out that almost any application that relies on ***huge amounts of floating-point operations
and simple data access patterns*** can gain a significant speedup using GPUs. This is sometimes referred
to as GPGPU, or General-Purpose computing on Graphics Processing Units.

The following are some of the scientific and engineering fields that have successfully used CUDA and
NVIDIA GPUs to accelerate the performance of important applications:

```
Deep Learning
Computational Fluid Dynamics
Computational Structural Mechanics
Seismic Processing
Bioinformatics
Materials Science
Molecular Dynamics
Quantum Chemistry
Computational Physics
```

Of course, GPUs are hosted on CPU-based systems. Given a heterogeneous computer containing
both CPUs and GPUs, it may be a good strategy to offload the massively parallel and numerically
intensive tasks to one or more GPUs. Since most HPC applications contain both highly parallel
and less-parallel parts, adopting a heterogeneous programming model is frequently the best way
to utilize the strengths of both GPUs and CPUs. It allows the application to take advantage
of the highly parallel GPU hardware to produce higher overall computational throughput.

### Threads and Cores Redefined

- For GPU

    -- thread

    The stream of instructions and data that is assigned to one CUDA core; note, a Single
    Instruction applies to Multiple Threads, acting on multiple data (SIMT)

    -- CUDA core (vector lane)

    Unit that processes one data item after another, to execute its portion of a SIMT
    instruction stream

    -- warp (CPU: vector)

    Group of 32 threads that executes the same stream of instructions together, on
    different data

    -- kernel  (CPU: threads)

    Function that runs on the device; a kernel may be subdivided into thread blocks

    -- SM,streaming multiprocessor  (CPU: core)

    Unit capable of executing a thread block of a kernel; multiple SMs may work together
    on a kernel

### SIMT and Warps

- SIMT

As you might expect, the NVIDIA term "Single Instruction Multiple Threads" (SIMT) is
closely related to a better known term, Single Instruction Multiple Data (SIMD). What's
the difference? In pure SIMD, a single instruction acts upon all the data in exactly
the same way. In SIMT, this restriction is loosened a bit: selected threads can be
activated or deactivated, so that instructions and data are processed only on the
active threads, while the local data remain unchanged on inactive threads.





- Warps

### kernel and SMs

- Kernels (in software)

A function that is meant to be executed in parallel on an attached GPU is called a kernel.
In CUDA, a kernel is usually identified by the presence of the __global__ specifier in
front of an otherwise normal-looking C++ function declaration. The designation __global__
means the kernel may be called from either the host or the device, but it will execute
on the device.

Instead of being executed only once, a kernel is executed N times in parallel by N different
threads on the GPU. Each thread is assigned a unique ID (in effect, an index) that it can
use to compute memory addresses and make control decisions.

Accordingly, kernel calls must supply special arguments specifying how many threads to use
on the GPU. They do this using CUDA's "execution configuration" syntax, which looks like
this: fun<<<1, N>>>(x, y, z). Note that the first entry in the configuration (1, in this
case) gives the number of blocks of N threads that will be launched.

- Streaming multiprocessors (in hardware)

On the GPU, a kernel call is executed by one or more streaming multiprocessors, or SMs.
The SMs are the hardware homes of the CUDA cores that execute the threads. The CUDA
cores in each SM are always arranged in sets of 32 so that the SM can use them to
execute full warps of threads. The exact number of SMs available in a device depends
on its NVIDIA processor family (Volta, Turing, etc.), as well as the specific model
number of the processor. Thus, the Volta chip in the Tesla V100 has 80 SMs in total,
while the more recent Turing chip in the Quadro RTX 5000 has just 48.


## GPU Memory

Just like a CPU, the GPU relies on a memory hierarchy—from RAM, through cache levels—to
ensure that its processing engines are kept supplied with the data they need to do
useful work. And just like the cores in a CPU, the streaming multiprocessors (SMs)
in a GPU ultimately require the data to be in registers to be available for computations.
This topic looks at the sizes and properties of the different elements of the GPU's
memory hierarchy and how they compare to those found in CPUs.





