---
title: Understanding GPU
category: tech
tags: [gpu, programming, memory, AI, storage]
---

title:  Understanding GPU

* TOC
{:toc}


# GPU overview

## overview

## GPU memory

[A synthetic benchmarking tool to measure peak capabilities of opencl devices](https://github.com/krrishnarraj/clpeak)


## platform & software eco


# Roadmap: Understanding GPU Architecture

[Roadmap: Understanding GPU Architecture](https://cvw.cac.cornell.edu/gpu-architecture)

## GPU Characteristics

The hardware design for graphics processing units (GPUs) is optimized for highly parallel processing. 


### Design: GPU vs. CPU

GPUs were originally designed to render graphics. They work very well for shading, texturing, and
rendering the thousands of independent polygons that comprise a 3D object. CPUs, on the other hand,
are meant to control the logical flow of any general-purpose program, where lots of number crunching
may (or may not) be involved. Due to these very different roles, GPUs are characterized by many
more processing units and higher aggregate memory bandwidth, while CPUs feature more sophisticated
instruction processing and faster clock speed.


- CPU

CPUs can handle more complex workflows compared to GPUs.

CPUs don't have as many arithmetic logic units or floating point units as GPUs (the small green boxes
above, roughly speaking), but the ALUs and FPUs in a CPU core are individually more capable.

CPUs have more cache memory than GPUs.

### Performance: GPU vs. CPU

GPUs and CPUs are intended for fundamentally different types of workloads. CPUs are typically designed
for multitasking and fast serial processing, while GPUs are designed to produce high computational
throughput using their massively parallel architectures.

### Heterogeneous Applications

It turns out that almost any application that relies on ***huge amounts of floating-point operations
and simple data access patterns*** can gain a significant speedup using GPUs. This is sometimes referred
to as GPGPU, or General-Purpose computing on Graphics Processing Units.

The following are some of the scientific and engineering fields that have successfully used CUDA and
NVIDIA GPUs to accelerate the performance of important applications:

```
Deep Learning
Computational Fluid Dynamics
Computational Structural Mechanics
Seismic Processing
Bioinformatics
Materials Science
Molecular Dynamics
Quantum Chemistry
Computational Physics
```

Of course, GPUs are hosted on CPU-based systems. Given a heterogeneous computer containing
both CPUs and GPUs, it may be a good strategy to offload the massively parallel and numerically
intensive tasks to one or more GPUs. Since most HPC applications contain both highly parallel
and less-parallel parts, adopting a heterogeneous programming model is frequently the best way
to utilize the strengths of both GPUs and CPUs. It allows the application to take advantage
of the highly parallel GPU hardware to produce higher overall computational throughput.

### Threads and Cores Redefined

- For GPU

    -- thread

    The stream of instructions and data that is assigned to one CUDA core; note, a Single
    Instruction applies to Multiple Threads, acting on multiple data (SIMT)

    -- CUDA core (vector lane)

    Unit that processes one data item after another, to execute its portion of a SIMT
    instruction stream

    -- warp (CPU: vector)

    Group of 32 threads that executes the same stream of instructions together, on
    different data

    -- kernel  (CPU: threads)

    Function that runs on the device; a kernel may be subdivided into thread blocks

    -- SM,streaming multiprocessor  (CPU: core)

    Unit capable of executing a thread block of a kernel; multiple SMs may work together
    on a kernel

### SIMT and Warps

- SIMT

- Warps

### kernel and SMs

- Kernels (in software)

A function that is meant to be executed in parallel on an attached GPU is called a kernel.
In CUDA, a kernel is usually identified by the presence of the __global__ specifier in
front of an otherwise normal-looking C++ function declaration. The designation __global__
means the kernel may be called from either the host or the device, but it will execute
on the device.

Instead of being executed only once, a kernel is executed N times in parallel by N different
threads on the GPU. Each thread is assigned a unique ID (in effect, an index) that it can
use to compute memory addresses and make control decisions.

Accordingly, kernel calls must supply special arguments specifying how many threads to use
on the GPU. They do this using CUDA's "execution configuration" syntax, which looks like
this: fun<<<1, N>>>(x, y, z). Note that the first entry in the configuration (1, in this
case) gives the number of blocks of N threads that will be launched.

- Streaming multiprocessors (in hardware)

On the GPU, a kernel call is executed by one or more streaming multiprocessors, or SMs.
The SMs are the hardware homes of the CUDA cores that execute the threads. The CUDA
cores in each SM are always arranged in sets of 32 so that the SM can use them to
execute full warps of threads. The exact number of SMs available in a device depends
on its NVIDIA processor family (Volta, Turing, etc.), as well as the specific model
number of the processor. Thus, the Volta chip in the Tesla V100 has 80 SMs in total,
while the more recent Turing chip in the Quadro RTX 5000 has just 48.


## GPU Memory

Just like a CPU, the GPU relies on a memory hierarchy—from RAM, through cache levels—to
ensure that its processing engines are kept supplied with the data they need to do
useful work. And just like the cores in a CPU, the streaming multiprocessors (SMs)
in a GPU ultimately require the data to be in registers to be available for computations.
This topic looks at the sizes and properties of the different elements of the GPU's
memory hierarchy and how they compare to those found in CPUs.





