---
title: Understanding GPU
category: tech
tags: [gpu, programming, memory, AI, storage]
---

title:  Understanding GPU

* TOC
{:toc}


# GPU overview

## overview

### Tensor

```
● A tensor in GPU/LLM training is a multi-dimensional array of numbers - essentially a generalization of scalars, vectors, and matrices to arbitrary dimensions.

  Conceptual Hierarchy

  - Scalar (0D tensor): Single number → 5
  - Vector (1D tensor): Array of numbers → [1, 2, 3]
  - Matrix (2D tensor): Grid of numbers → [[1,2], [3,4]]
  - Tensor (3D+ tensor): Multi-dimensional array → [[[...]]]

  In LLM/GPU Context

  Why tensors?
  1. Batch processing: Dimension 0 often represents batch size
  2. Sequence data: Dimension 1 might be sequence length (tokens)
  3. Features: Dimension 2 could be embedding dimensions
  4. Efficiency: GPUs are optimized for parallel operations on large arrays

  Example - LLM forward pass:
  Input tensor shape: [batch_size, sequence_length, embedding_dim]
                      [32, 512, 768]
                      ↓
  Attention weights:  [32, 8, 512, 512]  # 8 attention heads
                      ↓
  Output tensor:      [32, 512, 768]

  Key Properties

  - Shape: Dimensions of the array (e.g., [2, 3, 4])
  - Dtype: Data type (float32, float16, bfloat16, int8)
  - Device: Where stored (CPU, GPU, TPU)
  - Gradients: Can track derivatives for backpropagation

  GPU Optimization

  GPUs excel at tensor operations because:
  - Thousands of cores perform element-wise operations in parallel
  - Matrix multiplications (tensor cores) are hardware-accelerated
  - Memory layout optimized for coalesced access patterns

  Think of tensors as the fundamental data structure for neural networks, just like arrays are for traditional programming.

```



## GPU memory

[A synthetic benchmarking tool to measure peak capabilities of opencl devices](https://github.com/krrishnarraj/clpeak)


## platform & software eco


# GPU Performance Background User's Guide

[GPU Performance Background User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html)

## Overview

The basic structure of a GPU (GPU Architecture Fundamentals)

How operations are divided and executed in parallel (GPU Execution Model)

How to estimate performance limitations with arithmetic intensity (Understanding Performance)

Loose categories of deep learning operations and the performance limitations that tend to apply to each (DNN Operation Categories)


## GPU Architecture Fundamentals

![Simplified view of the GPU architecture](/assets/images/simple-gpu-arch.svg)

```
The GPU is a highly parallel processor architecture, composed of processing 
elements and a memory hierarchy. At a high level, NVIDIA® GPUs consist of a 
number of Streaming Multiprocessors (SMs), on-chip L2 cache, and high-bandwidth 
DRAM. Arithmetic and other instructions are executed by the SMs; data and code 
are accessed from DRAM via the L2 cache. As an example, an NVIDIA A100 GPU contains 
108 SMs, a 40 MB L2 cache, and up to 2039 GB/s bandwidth from 80 GB of HBM2 memory.
```

![Multiply-add operations per clock per SM](/assets/images/multi-add-op.svg)

```
Each SM has its own instruction schedulers and various instruction execution 
pipelines. Multiply-add is the most frequent operation in modern neural networks, 
acting as a building block for fully-connected and convolutional layers, both of 
which can be viewed as a collection of vector dot-products. The following table 
shows a single SM’s multiply-add operations per clock for various data types on 
NVIDIA’s recent GPU architectures. Each multiply-add comprises two operations, 
thus one would multiply the throughput in the table by 2 to get FLOP counts per 
clock. To get the FLOPS rate for GPU one would then multiply these by the number 
of SMs and SM clock rate. For example, an A100 GPU with 108 SMs and 1.41 GHz clock 
rate has peak dense throughputs of 156 TF32 TFLOPS and 312 FP16 TFLOPS (throughputs 
achieved by applications depend on a number of factors discussed throughout this 
document).
```

```
Furthermore, the NVIDIA Turing™ architecture can execute INT8 operations in either 
Tensor Cores or CUDA cores. Tensor Cores were introduced in the NVIDIA Volta™ GPU 
architecture to accelerate matrix multiply and accumulate operations for machine 
learning and scientific applications. These instructions operate on small matrix 
blocks (for example, 4x4 blocks). Note that Tensor Cores can compute and accumulate 
products in higher precision than the inputs. For example, during training with FP16 
inputs, Tensor Cores can compute products without loss of precision and accumulate 
in FP32. When math operations cannot be formulated in terms of matrix blocks they 
are executed in other CUDA cores. For example, the element-wise addition of two 
half-precision tensors would be performed by CUDA cores, rather than Tensor Cores.
```


## GPU Execution Model

To utilize their parallel resources, GPUs execute many threads concurrently. 

There are two concepts critical to understanding how thread count relates to GPU performance: 

- GPUs execute functions using a 2-level hierarchy of threads. A given function’s threads 
are grouped into equally-sized thread blocks, and a set of thread blocks are launched to 
execute the function. 


- GPUs hide dependent instruction latency by switching to the execution of other threads. 
Thus, the number of threads needed to effectively utilize a GPU is much higher than the number 
of cores or instruction pipelines.

The 2-level thread hierarchy is a result of GPUs having many SMs, each of which in turn 
has pipelines for executing many threads and enables its threads to communicate via 
shared memory and synchronization. 

At runtime, a thread block is placed on an SM for execution, enabling all threads in a 
thread block to communicate and synchronize efficiently. Launching a function with a 
single thread block would only give work to a single SM, therefore to fully utilize a 
GPU with multiple SMs one needs to launch many thread blocks. Since an SM can execute 
multiple thread blocks concurrently, typically one wants the number of thread blocks to 
be several times higher than the number of SMs. The reason for this is to minimize 
the “tail” effect, where at the end of a function execution only a few active thread 
blocks remain, thus underutilizing the GPU for that period of time as illustrated in
Figure 3.

We use the term wave to refer to a set of thread blocks that run concurrently. It is 
most efficient to launch functions that execute in several waves of thread blocks - 
a smaller percentage of time is spent in the tail wave, minimizing the tail effect 
and thus the need to do anything about it. For the higher-end GPUs, typically only 
launches with fewer than 300 thread blocks should be examined for tail effects.


## DNN Operation Categories

While modern neural networks are built from a variety of layers, their operations 
fall into three main categories according to the nature of computation.


### Elementwise Operations

Elementwise operations may be unary or binary operations; the key is that layers 
in this category perform mathematical operations on each element independently of 
all other elements in the tensor.

Layers in this category include most non-linearities (sigmoid, tanh, etc.), scale, 
bias, add, and others. These layers tend to be memory-limited, as they perform 
few operations per byte accessed. Further details on activations, in particular, 
can be found within the Activations section in the Optimizing Memory-Bound Layers 
User's Guide.


### Reduction Operations

Reduction operations produce values computed over a range of input tensor values.

For example, pooling layers compute values over some neighborhoods in the input tensor. 
Batch normalization computes the mean and standard deviation over a tensor before using 
them in operations for each output element. In addition to pooling and normalization 
layers, SoftMax also falls into the reduction category. Typical reduction operations 
have a low arithmetic intensity and thus are memory limited. Further details on pooling 
layers can be found within Pooling.


### Dot-Product Operations

Operations in this category can be expressed as dot-products of elements from two tensors, 
usually a weight (learned parameter) tensor and an activation tensor. 


These include fully-connected layers, occurring on their own and as building blocks of 
recurrent and attention cells. Fully-connected layers are naturally expressed as 
matrix-vector and matrix-matrix multiplies. Convolutions can also be expressed as 
collections of dot-products - one vector is the set of parameters for a given filter, 
the other is an “unrolled” activation region to which that filter is being applied. 
Since filters are applied in multiple locations, convolutions too can be viewed as 
matrix-vector or matrix-matrix multiply operations (refer to Convolution Algorithms).

Operations in the dot-product category can be math-limited if the corresponding matrices 
are large enough.


# CUDA C++ Programming Guide

[CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)


## Introduction

### The Benefits of Using GPUs

The Graphics Processing Unit (GPU)1 provides much higher instruction throughput and memory 
bandwidth than the CPU within a similar price and power envelope. Many applications leverage 
these higher capabilities to run faster on the GPU than on the CPU (see GPU Applications). 
Other computing devices, like FPGAs, are also very energy efficient, but offer much less 
programming flexibility than GPUs.

This difference in capabilities between the GPU and the CPU exists because they are designed 
with different goals in mind. While the CPU is designed to excel at executing a sequence 
of operations, called a thread, as fast as possible and can execute a few tens of these 
threads in parallel, the GPU is designed to excel at executing thousands of them in 
parallel (amortizing the slower single-thread performance to achieve greater throughput).

The GPU is specialized for highly parallel computations and therefore designed such that 
more transistors are devoted to data processing rather than data caching and flow control. 

### CUDA®: A General-Purpose Parallel Computing Platform and Programming Model

CUDA comes with a software environment that allows developers to use C++ as a high-level 
programming language. As illustrated by Figure 2, other languages, application programming 
interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC.


### A Scalable Programming Model

The CUDA parallel programming model is designed to overcome this challenge while maintaining a 
low learning curve for programmers familiar with standard programming languages such as C.

At its core are three key abstractions — a hierarchy of thread groups, shared memories, and 
barrier synchronization — that are simply exposed to the programmer as a minimal set of language extensions.

These abstractions provide fine-grained data parallelism and thread parallelism, nested within 
coarse-grained data parallelism and task parallelism. They guide the programmer to partition 
the problem into coarse sub-problems that can be solved independently in parallel by blocks of 
threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by 
all threads within the block.

This decomposition preserves language expressivity by allowing threads to cooperate when 
solving each sub-problem, and at the same time enables automatic scalability. Indeed, each 
block of threads can be scheduled on any of the available multiprocessors within a GPU, in 
any order, concurrently or sequentially, so that a compiled CUDA program can execute on any 
number of multiprocessors as illustrated by Figure 3, and only the runtime system needs to 
know the physical multiprocessor count.

This scalable programming model allows the GPU architecture to span a wide market range by 
simply scaling the number of multiprocessors and memory partitions: from the high-performance 
enthusiast GeForce GPUs and professional Quadro and Tesla computing products to a variety of 
inexpensive, mainstream GeForce GPUs (see CUDA-Enabled GPUs for a list of all CUDA-enabled GPUs).

![Automatic Scalability](/assets/images/automatic-scalability.png)

```
A GPU is built around an array of Streaming Multiprocessors (SMs) (see Hardware Implementation 
for more details). A multithreaded program is partitioned into blocks of threads that execute 
independently from each other, so that a GPU with more multiprocessors will automatically 
execute the program in less time than a GPU with fewer multiprocessors.
```

## Programming Model

### Kernels

CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels,
that, when called, are executed N times in parallel by N different CUDA threads, as opposed 
to only once like regular C++ functions.

A kernel is defined using the __global__ declaration specifier and the number of CUDA 
threads that execute that kernel for a given kernel call is specified using a new <<<...>>>execution 
configuration syntax (see Execution Configuration). Each thread that executes the kernel 
is given a unique thread ID that is accessible within the kernel through built-in variables.

As an illustration, the following sample code, using the built-in variable threadIdx, adds 
two vectors A and B of size N and stores the result into vector C.

```
// Kernel definition
__global__ void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

int main()
{
    ...
    // Kernel invocation with N threads
    VecAdd<<<1, N>>>(A, B, C);
    ...
}
```

### 5.2. Thread Hierarchy

For convenience, threadIdx is a 3-component vector, so that threads can be identified using a
one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, 
two-dimensional, or three-dimensional block of threads, called a thread block. This provides a 
natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume.

The index of a thread and its thread ID relate to each other in a straightforward way: For a 
one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy), the 
thread ID of a thread of index (x, y) is (x + y Dx); for a three-dimensional block of size (Dx, Dy, Dz), the 
thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy).

As an example, the following code adds two matrices A and B of size NxN and stores the result into matrix C.

```
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
                       float C[N][N])
{
    int i = threadIdx.x;
    int j = threadIdx.y;
    C[i][j] = A[i][j] + B[i][j];
}

int main()
{
    ...
    // Kernel invocation with one block of N * N * 1 threads
    int numBlocks = 1;
    dim3 threadsPerBlock(N, N);
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
    ...
}
```








# Roadmap: Understanding GPU Architecture

[Roadmap: Understanding GPU Architecture](https://cvw.cac.cornell.edu/gpu-architecture)

## GPU Characteristics

The hardware design for graphics processing units (GPUs) is optimized for highly parallel processing. 


### Design: GPU vs. CPU

GPUs were originally designed to render graphics. They work very well for shading, texturing, and
rendering the thousands of independent polygons that comprise a 3D object. CPUs, on the other hand,
are meant to control the logical flow of any general-purpose program, where lots of number crunching
may (or may not) be involved. Due to these very different roles, GPUs are characterized by many
more processing units and higher aggregate memory bandwidth, while CPUs feature more sophisticated
instruction processing and faster clock speed.


- CPU

CPUs can handle more complex workflows compared to GPUs.

CPUs don't have as many arithmetic logic units or floating point units as GPUs (the small green boxes
above, roughly speaking), but the ALUs and FPUs in a CPU core are individually more capable.

CPUs have more cache memory than GPUs.

### Performance: GPU vs. CPU

GPUs and CPUs are intended for fundamentally different types of workloads. CPUs are typically designed
for multitasking and fast serial processing, while GPUs are designed to produce high computational
throughput using their massively parallel architectures.

### Heterogeneous Applications

It turns out that almost any application that relies on ***huge amounts of floating-point operations
and simple data access patterns*** can gain a significant speedup using GPUs. This is sometimes referred
to as GPGPU, or General-Purpose computing on Graphics Processing Units.

The following are some of the scientific and engineering fields that have successfully used CUDA and
NVIDIA GPUs to accelerate the performance of important applications:

```
Deep Learning
Computational Fluid Dynamics
Computational Structural Mechanics
Seismic Processing
Bioinformatics
Materials Science
Molecular Dynamics
Quantum Chemistry
Computational Physics
```

Of course, GPUs are hosted on CPU-based systems. Given a heterogeneous computer containing
both CPUs and GPUs, it may be a good strategy to offload the massively parallel and numerically
intensive tasks to one or more GPUs. Since most HPC applications contain both highly parallel
and less-parallel parts, adopting a heterogeneous programming model is frequently the best way
to utilize the strengths of both GPUs and CPUs. It allows the application to take advantage
of the highly parallel GPU hardware to produce higher overall computational throughput.

### Threads and Cores Redefined

- For GPU

    -- thread

    The stream of instructions and data that is assigned to one CUDA core; note, a Single
    Instruction applies to Multiple Threads, acting on multiple data (SIMT)

    -- CUDA core (vector lane)

    Unit that processes one data item after another, to execute its portion of a SIMT
    instruction stream

    -- warp (CPU: vector)

    Group of 32 threads that executes the same stream of instructions together, on
    different data

    -- kernel  (CPU: threads)

    Function that runs on the device; a kernel may be subdivided into thread blocks

    -- SM,streaming multiprocessor  (CPU: core)

    Unit capable of executing a thread block of a kernel; multiple SMs may work together
    on a kernel

### SIMT and Warps

- SIMT

As you might expect, the NVIDIA term "Single Instruction Multiple Threads" (SIMT) is
closely related to a better known term, Single Instruction Multiple Data (SIMD). What's
the difference? In pure SIMD, a single instruction acts upon all the data in exactly
the same way. In SIMT, this restriction is loosened a bit: selected threads can be
activated or deactivated, so that instructions and data are processed only on the
active threads, while the local data remain unchanged on inactive threads.





- Warps

### kernel and SMs

- Kernels (in software)

A function that is meant to be executed in parallel on an attached GPU is called a kernel.
In CUDA, a kernel is usually identified by the presence of the __global__ specifier in
front of an otherwise normal-looking C++ function declaration. The designation __global__
means the kernel may be called from either the host or the device, but it will execute
on the device.

Instead of being executed only once, a kernel is executed N times in parallel by N different
threads on the GPU. Each thread is assigned a unique ID (in effect, an index) that it can
use to compute memory addresses and make control decisions.

Accordingly, kernel calls must supply special arguments specifying how many threads to use
on the GPU. They do this using CUDA's "execution configuration" syntax, which looks like
this: fun<<<1, N>>>(x, y, z). Note that the first entry in the configuration (1, in this
case) gives the number of blocks of N threads that will be launched.

- Streaming multiprocessors (in hardware)

On the GPU, a kernel call is executed by one or more streaming multiprocessors, or SMs.
The SMs are the hardware homes of the CUDA cores that execute the threads. The CUDA
cores in each SM are always arranged in sets of 32 so that the SM can use them to
execute full warps of threads. The exact number of SMs available in a device depends
on its NVIDIA processor family (Volta, Turing, etc.), as well as the specific model
number of the processor. Thus, the Volta chip in the Tesla V100 has 80 SMs in total,
while the more recent Turing chip in the Quadro RTX 5000 has just 48.


## GPU Memory

Just like a CPU, the GPU relies on a memory hierarchy—from RAM, through cache levels—to
ensure that its processing engines are kept supplied with the data they need to do
useful work. And just like the cores in a CPU, the streaming multiprocessors (SMs)
in a GPU ultimately require the data to be in registers to be available for computations.
This topic looks at the sizes and properties of the different elements of the GPU's
memory hierarchy and how they compare to those found in CPUs.





