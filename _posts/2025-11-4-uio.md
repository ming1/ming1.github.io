---
title: UIO
category: tech
tags: [userspace, driver, uio, vfio, linux kernel]
---

title: UIO

* TOC
{:toc}



# VFIO Complete Walkthrough

Generated by AI

This document provides a complete walkthrough of VFIO (Virtual Function I/O) with practical examples.

## Table of Contents
1. [What is VFIO?](#what-is-vfio)
2. [Prerequisites](#prerequisites)
3. [Architecture Overview](#architecture-overview)
4. [VFIO DMA Mapping and IOMMU Implementation](#vfio-dma-mapping-and-iommu-implementation)
5. [Step-by-Step Setup](#step-by-step-setup)
6. [Real-World Use Cases](#real-world-use-cases)
7. [Troubleshooting](#troubleshooting)

## What is VFIO?

**VFIO (Virtual Function I/O)** is a kernel framework that enables **safe, direct device access from userspace** using the IOMMU for memory protection.

### Key Benefits

- **Security**: IOMMU prevents malicious DMA attacks
- **Performance**: Direct hardware access, no kernel overhead
- **Flexibility**: Write drivers in userspace
- **Isolation**: Safe device sharing between VMs

### Primary Use Cases

1. **GPU Passthrough**: Gaming VMs with native graphics performance
2. **Network I/O**: DPDK for high-performance packet processing
3. **VM Device Assignment**: PCI device passthrough to KVM guests
4. **Userspace Drivers**: Custom hardware without kernel modules

## Prerequisites

### 1. Hardware Requirements

#### IOMMU Support
```bash
# Intel VT-d
grep -e DMAR -e IOMMU /var/log/dmesg

# AMD-Vi
grep -i iommu /var/log/dmesg

# ARM SMMU
dmesg | grep -i smmu
```

### 2. BIOS/UEFI Settings

Enable:
- **Intel VT-d** (Intel) or **AMD-Vi** (AMD)
- **Virtualization Technology**
- **IOMMU**

### 3. Kernel Boot Parameters

Add to kernel command line (`/etc/default/grub`):

```bash
# Intel
GRUB_CMDLINE_LINUX="intel_iommu=on iommu=pt"

# AMD
GRUB_CMDLINE_LINUX="amd_iommu=on iommu=pt"

# Then update grub
sudo grub2-mkconfig -o /boot/grub2/grub.cfg
sudo reboot
```

**Parameters explained:**
- `intel_iommu=on` / `amd_iommu=on`: Enable IOMMU
- `iommu=pt`: Passthrough mode (better performance for host devices)

### 4. Verify IOMMU is Active

```bash
# Check if IOMMU is enabled
dmesg | grep -i iommu

# Should see lines like:
# DMAR: Intel(R) Virtualization Technology for Directed I/O
# DMAR: IOMMU enabled
```

### 5. Required Kernel Modules

```bash
# Load VFIO modules
sudo modprobe vfio
sudo modprobe vfio-pci
sudo modprobe vfio_iommu_type1

# Verify
lsmod | grep vfio
```

## Architecture Overview

### The VFIO Stack

```
┌─────────────────────────────────────────────────┐
│                 Userspace                       │
│                                                 │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐     │
│  │   QEMU   │  │   DPDK   │  │  Custom  │     │
│  │  /KVM    │  │          │  │   App    │     │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘     │
│       │             │             │            │
│       └─────────────┴─────────────┘            │
│                     │                          │
│              /dev/vfio/vfio                    │
│              /dev/vfio/<group>                 │
└─────────────────────┼──────────────────────────┘
                      │
        ══════════════════════════
               Kernel Space
        ══════════════════════════
                      │
┌─────────────────────┼──────────────────────────┐
│              VFIO Framework                    │
│  ┌──────────────────────────────────────┐     │
│  │  Container (IOMMU context)           │     │
│  │    - Manages address spaces          │     │
│  │    - DMA mapping/unmapping           │     │
│  └──────────────────────────────────────┘     │
│  ┌──────────────────────────────────────┐     │
│  │  Group (isolation boundary)          │     │
│  │    - Device enumeration              │     │
│  │    - IRQ management                  │     │
│  │    - MMIO region access              │     │
│  └──────────────────────────────────────┘     │
│  ┌──────────────────────────────────────┐     │
│  │  Bus Driver (vfio-pci, etc.)         │     │
│  │    - Device binding                  │     │
│  │    - Config space access             │     │
│  └──────────────────────────────────────┘     │
└─────────────────────┼──────────────────────────┘
                      │
┌─────────────────────┼──────────────────────────┐
│                  IOMMU                         │
│  ┌──────────────────────────────────────┐     │
│  │  Address Translation                 │     │
│  │    GPA (Guest) → HPA (Host)          │     │
│  │    or IOVA → Physical Address        │     │
│  │                                       │     │
│  │  DMA Isolation                       │     │
│  │    Enforces memory access controls   │     │
│  └──────────────────────────────────────┘     │
└─────────────────────┼──────────────────────────┘
                      │
              ┌───────┴────────┐
              │                │
         ┌────▼─────┐    ┌────▼─────┐
         │   NIC    │    │   GPU    │
         └──────────┘    └──────────┘
           Hardware         Hardware
```

### IOMMU Groups

**Critical Concept**: Devices in the same IOMMU group can DMA to each other, so they must be assigned together.

```bash
# View all IOMMU groups
for g in /sys/kernel/iommu_groups/*/devices/*; do
    echo "Group $(basename $(dirname $(dirname $g))): $(basename $g)"
done | sort -V

# Example output:
# Group 1: 0000:00:00.0  (Host bridge)
# Group 2: 0000:00:01.0  (PCI bridge)
# Group 3: 0000:01:00.0  (GPU)
# Group 4: 0000:02:00.0  (NIC)
# Group 4: 0000:02:00.1  (NIC) ← Same group!
```

**Important**: All devices in Group 4 must be assigned together!

## VFIO DMA Mapping and IOMMU Implementation

This section explains in detail how VFIO DMA mapping works and how the IOMMU provides memory protection and address translation.

### The Problem VFIO Solves

Consider a device performing DMA (Direct Memory Access):

```
Without IOMMU - DANGEROUS:
┌──────────────┐
│   Device     │
│   (DMA)      │
└──────┬───────┘
       │ Device can DMA to ANY physical address!
       │ e.g., DMA to 0x1000 → Physical address 0x1000
       ↓
┌─────────────────────────────────────────┐
│      Physical Memory                    │
│  0x0000: Kernel code                    │
│  0x1000: Kernel data     ← Can corrupt! │
│  0x2000: Process A                      │
│  0x3000: Process B                      │
└─────────────────────────────────────────┘

Userspace can program device to:
- Read kernel memory (security breach!)
- Overwrite kernel data (system crash!)
- Access other VMs' memory (VM escape!)
```

With IOMMU - SAFE:
```
┌──────────────┐
│   Device     │
│   (DMA)      │
└──────┬───────┘
       │ Device uses IOVA (I/O Virtual Address)
       │ e.g., DMA to 0x10000000
       ↓
┌──────────────────────────┐
│       IOMMU              │
│   (Hardware MMU)         │
│                          │
│  Translate & Check:      │
│  0x10000000 → 0x50000    │ ← Only if allowed!
│                          │
│  If not in page table:   │
│  → IOMMU fault!         │
└──────┬───────────────────┘
       │ Only allowed physical addresses
       ↓
┌─────────────────────────────────────────┐
│      Physical Memory                    │
│  0x0000: Kernel code                    │
│  0x1000: Kernel data                    │
│  0x2000: Process A                      │
│  0x3000: Process B                      │
│  0x50000: DMA buffer    ← Only here!    │
└─────────────────────────────────────────┘
```

### IOMMU Hardware Architecture

The IOMMU sits between devices and memory, similar to how the CPU's MMU sits between CPU and memory:

```
CPU Side:                    Device Side:
┌─────────┐                 ┌─────────┐
│   CPU   │                 │ Device  │
└────┬────┘                 └────┬────┘
     │ Virtual Address          │ IOVA (I/O Virtual Addr)
     ↓                          ↓
┌────────────┐            ┌──────────────┐
│  CPU MMU   │            │    IOMMU     │
│            │            │              │
│ Page       │            │ I/O Page     │
│ Tables     │            │ Tables       │
└────┬───────┘            └──────┬───────┘
     │ Physical                  │ Physical
     │ Address                   │ Address
     └───────────────┬───────────┘
                     ↓
          ┌──────────────────┐
          │ Physical Memory  │
          └──────────────────┘
```

### Intel VT-d IOMMU Implementation

Intel's VT-d (Virtualization Technology for Directed I/O) uses multi-level page tables:

```
Device BDF → Root Table → Context Table → Page Tables → Physical Memory
(Bus:Dev:Fn)

Example: Device 01:00.0 performing DMA

Step 1: Root Table Lookup
┌─────────────────────────────┐
│      Root Table Entry       │ ← Indexed by Bus# (01)
│  Points to Context Table    │
└────────────┬────────────────┘
             ↓
Step 2: Context Table Lookup
┌─────────────────────────────┐
│    Context Entry            │ ← Indexed by DevFn (00.0)
│  - Address Space Root (ASR) │ ← Points to page tables
│  - Domain ID                │
│  - Translation Type         │
└────────────┬────────────────┘
             ↓
Step 3: Multi-Level Page Table Walk
┌─────────────────────────────┐
│   Level 4 Page Table        │
└────────────┬────────────────┘
             ↓
┌─────────────────────────────┐
│   Level 3 Page Table        │
└────────────┬────────────────┘
             ↓
┌─────────────────────────────┐
│   Level 2 Page Table        │
└────────────┬────────────────┘
             ↓
┌─────────────────────────────┐
│   Level 1 Page Table (PTE)  │
│  - Physical Address         │
│  - Read/Write permissions   │
│  - Present bit              │
└────────────┬────────────────┘
             ↓
      Physical Memory
```

### VFIO Type1 IOMMU Driver

VFIO's Type1 IOMMU driver (`vfio_iommu_type1`) manages IOMMU page tables:

#### Key Data Structures

```c
/* Container - represents an IOMMU context */
struct vfio_iommu {
    struct list_head        domain_list;    /* List of IOMMU domains */
    struct mutex            lock;           /* Protects domain_list */
    struct rb_root          dma_list;       /* RB tree of DMA mappings */
    bool                    v2;             /* Type1 v2 extensions */
    bool                    nesting;        /* Nested translation */
};

/* IOMMU Domain - hardware page table context */
struct vfio_domain {
    struct iommu_domain     *domain;        /* Actual IOMMU domain */
    struct list_head        next;           /* Next in container */
    struct list_head        group_list;     /* Groups in this domain */
    bool                    fgsp;           /* Fine-grained super pages */
};

/* DMA Mapping - tracks each mapped region */
struct vfio_dma {
    struct rb_node          node;           /* RB tree node */
    dma_addr_t              iova;           /* I/O virtual address */
    unsigned long           vaddr;          /* Process virtual address */
    size_t                  size;           /* Mapping size */
    int                     prot;           /* IOMMU_READ | IOMMU_WRITE */
    struct task_struct      *task;          /* Task owning the memory */
    struct vfio_pfn_list    pfn_list;       /* Pinned pages */
};
```

### DMA Mapping Workflow

When userspace calls `VFIO_IOMMU_MAP_DMA`:

```c
/* Userspace API call */
struct vfio_iommu_type1_dma_map {
    __u32   argsz;
    __u32   flags;              /* VFIO_DMA_MAP_FLAG_READ | WRITE */
    __u64   vaddr;              /* Process virtual address */
    __u64   iova;               /* I/O virtual address (device sees this) */
    __u64   size;               /* Size of mapping */
};

ioctl(container_fd, VFIO_IOMMU_MAP_DMA, &dma_map);
```

#### Kernel Processing Steps:

```
1. Userspace Preparation
   ┌──────────────────────────┐
   │ Allocate buffer:         │
   │   void *buf = malloc()   │
   │   or mmap()              │
   └──────────┬───────────────┘
              ↓
2. Request DMA Mapping
   ┌──────────────────────────┐
   │ ioctl(VFIO_IOMMU_MAP_DMA)│
   │   vaddr = 0x7f0000000    │ ← Userspace VA
   │   iova  = 0x10000000     │ ← Device address
   │   size  = 4096           │
   └──────────┬───────────────┘
              ↓
3. Kernel: vfio_iommu_type1_ioctl()
   ┌──────────────────────────┐
   │ Validate parameters      │
   │ - Check alignment        │
   │ - Check overlaps         │
   │ - Verify user owns vaddr │
   └──────────┬───────────────┘
              ↓
4. Pin User Pages
   ┌──────────────────────────┐
   │ pin_user_pages_remote()  │
   │ - Walk page tables       │
   │ - Pin physical pages     │
   │ - Increment page refcount│
   │ - Prevent page from      │
   │   being swapped out      │
   └──────────┬───────────────┘
              ↓
5. Get Physical Addresses
   ┌──────────────────────────┐
   │ For each pinned page:    │
   │   pfn = page_to_pfn(page)│
   │   phys = pfn << PAGE_SHIFT│
   └──────────┬───────────────┘
              ↓
6. Program IOMMU Page Tables
   ┌──────────────────────────┐
   │ iommu_map()              │
   │   domain = vfio_domain   │
   │   iova = 0x10000000      │
   │   phys = 0x50000         │ ← Physical addr
   │   size = 4096            │
   │   prot = READ|WRITE      │
   │                          │
   │ This creates PTE:        │
   │   IOVA[0x10000000] →     │
   │     PA[0x50000]          │
   │     Permissions: RW      │
   │     Present: 1           │
   └──────────┬───────────────┘
              ↓
7. Flush IOMMU TLB
   ┌──────────────────────────┐
   │ iommu_flush_iotlb()      │
   │ - Clear cached entries   │
   │ - Ensure new mapping     │
   │   is visible to device   │
   └──────────┬───────────────┘
              ↓
8. Track Mapping
   ┌──────────────────────────┐
   │ vfio_dma_do_map()        │
   │ - Allocate vfio_dma      │
   │ - Insert into RB tree    │
   │ - Store pinned pages     │
   └──────────────────────────┘
```

### Complete Memory Flow Example

Let's trace a DMA operation from start to finish:

```
Scenario: Userspace wants device to DMA 4KB to a buffer

┌─────────────────────────────────────────────────────────────┐
│ 1. Userspace Allocates Buffer                              │
└─────────────────────────────────────────────────────────────┘

void *buffer = mmap(NULL, 4096, PROT_READ | PROT_WRITE,
                    MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);

// buffer = 0x7f1234000 (virtual address in process)

Process Address Space:
┌──────────────────────┐
│  ...                 │
│  0x7f1234000  ─┐     │
│  [4KB buffer]  │     │
│  0x7f1235000  ─┘     │
│  ...                 │
└──────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│ 2. Map for DMA                                              │
└─────────────────────────────────────────────────────────────┘

struct vfio_iommu_type1_dma_map dma_map = {
    .argsz = sizeof(dma_map),
    .vaddr = 0x7f1234000,     // Process virtual address
    .iova  = 0x20000000,      // Device virtual address
    .size  = 4096,
    .flags = VFIO_DMA_MAP_FLAG_READ | VFIO_DMA_MAP_FLAG_WRITE,
};

ioctl(container_fd, VFIO_IOMMU_MAP_DMA, &dma_map);

Kernel Action:
- Walks process page tables for 0x7f1234000
- Finds physical page at 0x50000
- Pins the page (prevents swapping)
- Programs IOMMU:

  IOMMU Page Table Entry:
  ┌────────────────────────────┐
  │ IOVA: 0x20000000           │
  │   ↓                        │
  │ Physical: 0x50000          │
  │ Permissions: RW            │
  │ Present: Yes               │
  └────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│ 3. Program Device                                           │
└─────────────────────────────────────────────────────────────┘

// Write to device registers via MMIO
volatile uint32_t *device_regs = mmap_device_bar0();

device_regs[DMA_ADDR_REG] = 0x20000000;  // IOVA!
device_regs[DMA_SIZE_REG] = 4096;
device_regs[DMA_CTRL_REG] = DMA_START | DMA_WRITE;

┌─────────────────────────────────────────────────────────────┐
│ 4. Device Performs DMA                                      │
└─────────────────────────────────────────────────────────────┘

Device generates DMA transaction:
  Address: 0x20000000 (IOVA)
  Size: 4096 bytes
  Data: [device data]

     │
     ↓
┌────────────────────────────────┐
│         IOMMU                  │
│                                │
│ 1. Receive DMA transaction     │
│    IOVA = 0x20000000           │
│                                │
│ 2. Look up in page tables:     │
│    Root table [Bus]            │
│      → Context [Dev:Fn]        │
│        → Page tables           │
│          → PTE                 │
│                                │
│ 3. Find mapping:               │
│    0x20000000 → 0x50000        │
│    Permissions: RW ✓           │
│                                │
│ 4. Check permissions:          │
│    Write requested? Yes        │
│    Write allowed? Yes ✓        │
│                                │
│ 5. Translate address:          │
│    Physical = 0x50000          │
│                                │
│ 6. Forward to memory:          │
│    Write 4KB to 0x50000        │
└────────┬───────────────────────┘
         │
         ↓
┌────────────────────────────────┐
│    Physical Memory             │
│                                │
│  0x50000: [DMA data written]   │ ← Success!
│                                │
└────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│ 5. Userspace Reads Result                                  │
└─────────────────────────────────────────────────────────────┘

// Wait for DMA complete interrupt
read(eventfd, &val, sizeof(val));

// Read data from buffer
memcpy(local_buf, buffer, 4096);

CPU Action:
- CPU accesses virtual address 0x7f1234000
- CPU MMU translates to physical 0x50000
- CPU reads data that device wrote

The data flows:
  Device → IOMMU(translate) → Physical Mem → CPU MMU → Process
```

### IOMMU Page Table Structure

For x86-64 with 4-level paging (similar to CPU page tables):

```
IOVA Breakdown (48-bit address):
┌──────┬──────┬──────┬──────┬────────────┐
│ L4   │ L3   │ L2   │ L1   │   Offset   │
│ 9bit │ 9bit │ 9bit │ 9bit │   12bit    │
└──────┴──────┴──────┴──────┴────────────┘
 47-39  38-30  29-21  20-12    11-0

Example IOVA = 0x20001234:
  0x00 0000 2000 1234

  Bits 47-39 (L4): 0x000 = entry 0
  Bits 38-30 (L3): 0x000 = entry 0
  Bits 29-21 (L2): 0x001 = entry 1
  Bits 20-12 (L1): 0x000 = entry 0
  Bits 11-0 (Off): 0x234 = byte offset

Page Table Walk:
┌───────────────────────────────┐
│ Context Entry (from dev)      │
│   ASR → Level 4 Table         │
└──────────┬────────────────────┘
           │
           ↓ Index [0]
┌───────────────────────────────┐
│ Level 4 Page Table            │
│ Entry[0] → Level 3 Table      │
└──────────┬────────────────────┘
           │
           ↓ Index [0]
┌───────────────────────────────┐
│ Level 3 Page Table            │
│ Entry[0] → Level 2 Table      │
└──────────┬────────────────────┘
           │
           ↓ Index [1]
┌───────────────────────────────┐
│ Level 2 Page Table            │
│ Entry[1] → Level 1 Table      │
└──────────┬────────────────────┘
           │
           ↓ Index [0]
┌───────────────────────────────┐
│ Level 1 Page Table (PTE)      │
│ Entry[0]:                     │
│   Physical Address: 0x50000   │
│   Present:     1               │
│   Read:        1               │
│   Write:       1               │
│   Execute:     0               │
│   Snoop:       1 (cache coh.)  │
└──────────┬────────────────────┘
           │
           ↓ Add offset 0x234
┌───────────────────────────────┐
│ Physical Address: 0x50234     │
└───────────────────────────────┘
```

### IOMMU Fault Handling

If a device tries to access an unmapped or unauthorized address:

```
Device attempts DMA to 0x99999000 (not mapped)

     ↓
┌────────────────────────────────┐
│         IOMMU                  │
│                                │
│ 1. Look up 0x99999000          │
│    → Page table walk           │
│    → No valid PTE found        │
│                                │
│ 2. Generate IOMMU Fault        │
│    - Fault address: 0x99999000 │
│    - Fault reason: NOT_PRESENT │
│    - Device: 01:00.0           │
│                                │
│ 3. Block the DMA transaction   │
│    ✗ Do NOT access memory      │
│                                │
│ 4. Log fault:                  │
│    dmesg: DMAR fault           │
│                                │
│ 5. Notify software (optional)  │
│    - Fault interrupt           │
│    - Fault event to VFIO       │
└────────────────────────────────┘
         │
         ↓
  DMA aborted
  Device may receive error response
```

### Huge Page Support

VFIO supports huge pages (2MB, 1GB) for better performance:

```
Without Huge Pages:
┌────────────────────────────────┐
│ 512 × 4KB pages = 2MB          │
│ Requires: 512 PTEs             │
│ IOMMU TLB: 512 entries needed  │
│ TLB misses: frequent           │
└────────────────────────────────┘

With 2MB Huge Page:
┌────────────────────────────────┐
│ 1 × 2MB page = 2MB             │
│ Requires: 1 PTE                │
│ IOMMU TLB: 1 entry             │
│ TLB misses: rare               │
│ Performance: Much better       │
└────────────────────────────────┘

Code example:
// Allocate huge page
void *buf = mmap(NULL, 2*1024*1024,
                 PROT_READ | PROT_WRITE,
                 MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB,
                 -1, 0);

// VFIO will automatically use huge pages in IOMMU if possible
struct vfio_iommu_type1_dma_map dma_map = {
    .vaddr = (uint64_t)buf,
    .iova  = 0x20000000,
    .size  = 2*1024*1024,  // 2MB
    .flags = VFIO_DMA_MAP_FLAG_READ | VFIO_DMA_MAP_FLAG_WRITE,
};
```

### Kernel Code References

Key functions in the VFIO Type1 IOMMU driver:

```c
/* drivers/vfio/vfio_iommu_type1.c */

// Main ioctl handler
static long vfio_iommu_type1_ioctl(void *iommu_data,
                                    unsigned int cmd, unsigned long arg)

// Map DMA region
static int vfio_dma_do_map(struct vfio_iommu *iommu,
                           struct vfio_iommu_type1_dma_map *map)
{
    // 1. Validate IOVA range
    // 2. Pin user pages
    // 3. Create IOMMU mappings
    // 4. Track in vfio_dma structure
}

// Unmap DMA region
static int vfio_dma_do_unmap(struct vfio_iommu *iommu,
                             struct vfio_iommu_type1_dma_unmap *unmap)
{
    // 1. Find vfio_dma entries
    // 2. Remove IOMMU mappings
    // 3. Unpin pages
    // 4. Free vfio_dma structures
}

// Pin user memory pages
static long vfio_pin_pages_remote(struct vfio_dma *dma,
                                   unsigned long vaddr,
                                   long npage,
                                   unsigned long *pfn_base)
{
    // Use pin_user_pages_remote() to:
    // - Walk page tables
    // - Pin physical pages
    // - Return PFNs
}

// Actual IOMMU mapping
static int vfio_iommu_map(struct vfio_iommu *iommu,
                          dma_addr_t iova,
                          unsigned long pfn,
                          long npage, int prot)
{
    list_for_each_entry(d, &iommu->domain_list, next) {
        ret = iommu_map(d->domain, iova, (phys_addr_t)pfn << PAGE_SHIFT,
                        npage << PAGE_SHIFT, prot | d->prot);
    }
}
```

### DMA Coherency and Cache Management

IOMMU ensures DMA coherency with CPU caches:

```
Without Snoop Control:
┌─────────┐                 ┌─────────┐
│   CPU   │                 │ Device  │
│  Cache  │                 │  DMA    │
└────┬────┘                 └────┬────┘
     │                           │
     │ Cached data              │ Writes directly
     │ (stale!)                 │ to memory
     ↓                           ↓
┌──────────────────────────────────┐
│         Memory                   │
│  CPU sees old cached value       │
│  Device wrote new value          │
│  → INCONSISTENCY!                │
└──────────────────────────────────┘

With IOMMU Snoop:
┌─────────┐                 ┌─────────┐
│   CPU   │                 │ Device  │
│  Cache  │←──────Snoop─────│  DMA    │
└────┬────┘     coherency   └────┬────┘
     │             ↑             │
     │             │             │
     │        ┌────┴────┐        │
     └────────│  IOMMU  │────────┘
              │ Enforces│
              │ Snoop   │
              └────┬────┘
                   ↓
           ┌──────────────┐
           │    Memory    │
           │  Always      │
           │  Consistent  │
           └──────────────┘
```

### Performance Considerations

```
1. TLB Misses
   - IOMMU has a TLB (IOTLB) like CPU
   - Misses cause page table walks (expensive)
   - Solution: Use huge pages

2. Map/Unmap Overhead
   - Each map requires page table updates
   - TLB flushes are expensive
   - Solution: Map once, reuse mapping

3. Page Pinning
   - Pinned pages can't be swapped
   - Limits available memory
   - Solution: Unpin when not in use

4. Interrupt Remapping
   - IOMMU also remaps MSI interrupts
   - Adds latency to interrupt delivery
   - Solution: Use posted interrupts (VT-d PI)

Benchmark example:
┌──────────────────────────┬───────────┬────────────┐
│ Operation                │ Latency   │ Throughput │
├──────────────────────────┼───────────┼────────────┤
│ DMA Map (4KB)            │ ~10 µs    │ 100K ops/s │
│ DMA Map (2MB huge)       │ ~10 µs    │ 100K ops/s │
│ DMA Unmap (4KB)          │ ~8 µs     │ 125K ops/s │
│ DMA transaction (mapped) │ +50 ns    │ Same as    │
│   vs. no IOMMU          │           │ no IOMMU   │
│ IOTLB miss penalty       │ ~200 ns   │ N/A        │
└──────────────────────────┴───────────┴────────────┘
```

### Summary

VFIO DMA mapping provides:

1. **Security**: IOMMU prevents unauthorized memory access
2. **Address Translation**: Maps process virtual → device virtual (IOVA) → physical
3. **Memory Pinning**: Keeps pages in physical memory (no swapping)
4. **Cache Coherency**: Ensures CPU and device see consistent data
5. **Fault Isolation**: Faults are contained, don't crash system

The key insight is that the IOMMU provides a **per-device MMU**, giving each device its own isolated address space - just like processes have isolated virtual memory thanks to the CPU's MMU.

## Step-by-Step Setup

### Step 1: Identify Your Device

```bash
# List all PCI devices
lspci

# Find your target device (example: NIC)
lspci | grep -i ethernet

# Get detailed info
lspci -vvv -s 0000:02:00.0
```

### Step 2: Check IOMMU Group

```bash
# Using our helper script
./vfio_bind_device.sh 0000:02:00.0 status

# Manual check
ls -l /sys/bus/pci/devices/0000:02:00.0/iommu_group
```

### Step 3: Bind Device to vfio-pci

```bash
# Option 1: Use helper script
sudo ./vfio_bind_device.sh 0000:02:00.0 bind

# Option 2: Manual binding
# a) Find vendor:device ID
VENDOR=$(cat /sys/bus/pci/devices/0000:02:00.0/vendor)
DEVICE=$(cat /sys/bus/pci/devices/0000:02:00.0/device)

# b) Unbind from current driver (if bound)
echo "0000:02:00.0" | sudo tee /sys/bus/pci/devices/0000:02:00.0/driver/unbind

# c) Load vfio-pci
sudo modprobe vfio-pci

# d) Bind to vfio-pci
echo "$VENDOR $DEVICE" | sudo tee /sys/bus/pci/drivers/vfio-pci/new_id
```

### Step 4: Verify VFIO Device

```bash
# Check IOMMU group
ls -l /sys/bus/pci/devices/0000:02:00.0/iommu_group
# Should show: ../../kernel/iommu_groups/X

# Check vfio device node exists
ls -l /dev/vfio/X  # X is the group number

# Verify driver binding
cat /sys/bus/pci/devices/0000:02:00.0/driver
# Should show: ../../../bus/pci/drivers/vfio-pci
```

### Step 5: Use the Device

```bash
# Compile the example
gcc -o vfio_example vfio_simple_example.c

# Run it
sudo ./vfio_example 0000:02:00.0
```

## Real-World Use Cases

### Use Case 1: GPU Passthrough to VM

**Scenario**: Pass a dedicated GPU to a Windows VM for gaming.

```xml
<!-- QEMU/KVM libvirt configuration snippet -->
<hostdev mode='subsystem' type='pci' managed='yes'>
  <source>
    <address domain='0x0000' bus='0x01' slot='0x00' function='0x0'/>
  </source>
  <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
</hostdev>
```

**How it works**:
1. Host binds GPU to vfio-pci
2. QEMU opens VFIO device
3. QEMU creates VM with IOMMU mapping
4. Guest sees real GPU hardware
5. Guest loads native GPU drivers
6. Near-native performance!

**Benefits**:
- ~95% native performance
- Full driver support in guest
- Multiple VMs can have different GPUs

### Use Case 2: DPDK High-Performance Networking

**Scenario**: Userspace network packet processing at 10+ Gbps.

```c
/* DPDK pseudo-code */
int main() {
    /* DPDK internally uses VFIO */
    rte_eal_init(argc, argv);

    /* Get NIC port (bound to vfio-pci) */
    uint16_t port = 0;

    /* Configure with zero-copy DMA */
    rte_eth_dev_configure(port, ...);

    /* Receive packets directly via VFIO DMA */
    while (1) {
        struct rte_mbuf *pkts[32];
        nb_rx = rte_eth_rx_burst(port, 0, pkts, 32);

        /* Process packets in userspace */
        for (i = 0; i < nb_rx; i++)
            process_packet(pkts[i]);
    }
}
```

**Benefits**:
- Kernel bypass: no system calls
- Zero-copy: DMA directly to userspace
- Better latency: microseconds vs milliseconds
- Higher throughput: 10-100 Gbps possible

### Use Case 3: Custom Hardware Device

**Scenario**: Access custom FPGA or hardware accelerator from userspace.

```c
/* Simple MMIO access */
int main() {
    int device_fd = setup_vfio("0000:04:00.0");

    /* Map BAR0 (device MMIO registers) */
    void *bar0 = mmap(NULL, BAR0_SIZE,
                      PROT_READ | PROT_WRITE,
                      MAP_SHARED, device_fd, BAR0_OFFSET);

    /* Write to device register */
    volatile uint32_t *reg = (uint32_t *)bar0;
    reg[0] = 0x12345678;  /* Write command */

    /* Read status */
    uint32_t status = reg[1];

    /* Setup DMA buffer */
    void *dma_buf = alloc_dma_buffer(container_fd, 1024*1024);

    /* Tell device to DMA */
    reg[2] = get_iova(dma_buf);  /* DMA address */
    reg[3] = 1024*1024;          /* Size */
    reg[0] = CMD_START_DMA;      /* Start */

    /* Wait for completion */
    while (!(reg[1] & STATUS_DMA_DONE));

    /* Process data in dma_buf */
}
```

**Benefits**:
- Rapid prototyping
- Easy debugging (userspace tools)
- No kernel module needed
- Safe DMA via IOMMU

## Troubleshooting

### Problem: "IOMMU not found"

```bash
# Check IOMMU in BIOS - must be enabled!

# Verify kernel parameters
cat /proc/cmdline | grep iommu

# Check dmesg
dmesg | grep -i iommu
# Should see: "IOMMU enabled" or similar
```

**Solution**: Enable VT-d/AMD-Vi in BIOS, add kernel parameters, reboot.

### Problem: "Group not viable"

```bash
# Check group devices
ls -l /sys/kernel/iommu_groups/X/devices/

# Find which device is not bound to vfio
for dev in /sys/kernel/iommu_groups/X/devices/*; do
    echo "$(basename $dev): $(readlink $dev/driver 2>/dev/null || echo 'no driver')"
done
```

**Solution**: Bind ALL devices in the group to vfio-pci or unbind them.

### Problem: "Operation not permitted"

**Causes**:
1. Not running as root
2. SELinux blocking
3. Secure boot enabled
4. Device in use by another driver

```bash
# Run as root
sudo ./vfio_example ...

# Check SELinux
getenforce
sudo setenforce 0  # Temporary

# Check device usage
lsof | grep vfio
```

### Problem: Poor VM Performance

**Possible issues**:
1. CPU pinning not configured
2. Huge pages not enabled
3. IOMMU in strict mode (not passthrough)

```bash
# Enable huge pages
echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages

# Use iommu=pt for better performance
# Edit /etc/default/grub, add: iommu=pt
```

### Problem: Device Reset Failed

Some devices don't support FLR (Function Level Reset).

```bash
# Check if reset is supported
cat /sys/bus/pci/devices/0000:XX:XX.X/reset

# Workaround: Use vendor-specific reset or reboot
```

## Advanced Topics

### Multiple Containers for Device Isolation

```c
/* Create separate containers for isolation */
int container1 = open("/dev/vfio/vfio", O_RDWR);
int container2 = open("/dev/vfio/vfio", O_RDWR);

/* Attach different groups */
attach_group(container1, group_id_1);
attach_group(container2, group_id_2);

/* Now devices can't DMA to each other */
```

### IRQ Handling with eventfd

```c
/* Setup MSI-X interrupt */
int eventfd = eventfd(0, 0);

struct vfio_irq_set irq_set = {
    .argsz = sizeof(irq_set) + sizeof(int),
    .flags = VFIO_IRQ_SET_DATA_EVENTFD | VFIO_IRQ_SET_ACTION_TRIGGER,
    .index = VFIO_PCI_MSIX_IRQ_INDEX,
    .start = 0,
    .count = 1,
};

memcpy(&irq_set.data, &eventfd, sizeof(int));
ioctl(device_fd, VFIO_DEVICE_SET_IRQS, &irq_set);

/* Wait for interrupt */
uint64_t count;
read(eventfd, &count, sizeof(count));
printf("Interrupt received!\n");
```

### Zero-Copy DMA Example

```c
/* Allocate huge page for better performance */
void *buffer = mmap(NULL, 2*1024*1024,
                    PROT_READ | PROT_WRITE,
                    MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB,
                    -1, 0);

/* Map for device DMA */
struct vfio_iommu_type1_dma_map dma_map = {
    .argsz = sizeof(dma_map),
    .vaddr = (uint64_t)buffer,
    .iova = 0x20000000,  /* Device address */
    .size = 2*1024*1024,
    .flags = VFIO_DMA_MAP_FLAG_READ | VFIO_DMA_MAP_FLAG_WRITE,
};

ioctl(container_fd, VFIO_IOMMU_MAP_DMA, &dma_map);

/* Device DMAs directly to userspace buffer! */
/* No copying, kernel just sets up IOMMU page tables */
```

## References

### Kernel Documentation
- `Documentation/driver-api/vfio.rst`
- `Documentation/virt/kvm/devices/vfio.rst`

### Headers
- `include/uapi/linux/vfio.h` - VFIO API definitions
- `include/linux/iommu.h` - IOMMU interface

### Related Projects
- **QEMU**: Uses VFIO for device passthrough
- **DPDK**: High-performance packet processing
- **SPDK**: Storage Performance Development Kit
- **libvirt**: VM management with VFIO devices

### Useful Commands

```bash
# List all IOMMU groups with devices
for d in /sys/kernel/iommu_groups/*/devices/*; do
    n=${d#*/iommu_groups/*}; n=${n%%/*}
    printf 'IOMMU Group %s ' "$n"
    lspci -nns "${d##*/}"
done

# Find devices suitable for passthrough (in singleton groups)
for g in /sys/kernel/iommu_groups/*; do
    if [ $(ls "$g/devices" | wc -l) -eq 1 ]; then
        echo "Singleton group $(basename $g): $(ls $g/devices)"
    fi
done

# Check device capabilities
lspci -vvv -s 0000:XX:XX.X | grep -i "capabilities\|reset"
```

## Summary

VFIO enables safe, high-performance device access from userspace by:

1. **Using IOMMU** for DMA protection and address translation
2. **Organizing devices** into isolation groups
3. **Providing APIs** for device control, DMA mapping, and interrupts
4. **Enabling use cases** like VM device passthrough, DPDK, and userspace drivers

The key insight is that the IOMMU acts as a "memory firewall" - it allows devices to perform DMA while preventing them from accessing unauthorized memory, making it safe to give userspace programs direct device access.


# DMA-BUF

## WALKTHROUGH

```
  1. The Cast of Characters

  In any DMA-BUF transaction, there are three distinct roles:

   * The Exporter: The driver that allocates the memory and "owns" the backing storage. (e.g., A GPU driver allocating a texture, or
     a camera driver allocating a frame).
   * The Importer: The driver that wants to access that memory. (e.g., A Display driver wanting to show the texture, or in your
     patchset's case, the NVMe driver wanting to write it to disk).
   * Userspace: The coordinator. It doesn't touch the data; it just passes the "ticket" (file descriptor) from the Exporter to the
     Importer.

  2. The Lifecycle of a DMA-BUF

  Step 1: Exporting (Allocation)
  Everything starts in the Exporter driver.
   1. Userspace requests a buffer (e.g., via a GPU IOCTL like DRM_IOCTL_MODE_CREATE_DUMB).
   2. The driver allocates physical memory pages.
   3. The driver calls dma_buf_export(). This creates a new struct dma_buf kernel object.
       * This object contains ops (operations) like map_dma_buf, unmap_dma_buf, release, etc.
   4. The driver gives a File Descriptor (FD) representing this buffer back to userspace.

  Step 2: Handover
  Userspace holds the FD. It passes this FD to the Importer driver (e.g., via the io_uring registration in your patchset).

  Step 3: Attachment (dma_buf_attach)
  Before the Importer can use the memory, it must "attach" to the buffer.
   1. The Importer calls dma_buf_attach(dmabuf, device).
   2. The DMA-BUF subsystem creates a struct dma_buf_attachment.
   3. Negotiation: This step ensures the two devices are compatible. For example, the Exporter might check if the Importer is a peer
      device connected via PCIe P2P, or if the memory needs to be moved to a specific region for the Importer to reach it.

  Step 4: Mapping (dma_buf_map_attachment)
  This is the critical step for performance.
   1. When the Importer is ready to do I/O, it calls dma_buf_map_attachment().
   2. The Exporter's callback is triggered. Its job is to generate a Scatter-Gather Table (`sg_table`).
       * Why Scatter-Gather? In physical memory, the buffer might be fragmented (scattered). The sg_table is a list of physical
         address ranges and lengths.
   3. The Translation: The Exporter maps these physical pages into the Importer's DMA address space.
       * If an IOMMU is present, this creates IO-Virtual Addresses (IOVA) mapped to the physical pages.
   4. The Importer receives the sg_table containing valid DMA addresses it can program directly into its hardware.

  Step 5: Synchronization (Fences)
  Sharing memory is dangerous. What if the GPU is still writing to the buffer while the NVMe drive tries to read it?
   * DMA-BUF uses dma_fence objects associated with the dma_buf.
   * Implicit Synchronization: The kernel tracks fences automatically. If the Exporter attaches a "write fence," the Importer knows
     to wait until that fence is signaled before starting its DMA transfer.
   * Explicit Synchronization: Userspace manages the fences (sync objects) and tells the kernel exactly when to wait.

  Step 6: Detach and Release
   1. When the I/O is done, the Importer calls dma_buf_unmap_attachment() (unmapping the DMA addresses) and dma_buf_detach().
   2. When userspace closes the FD, dma_buf_put() is called. When the reference count hits zero, the Exporter frees the physical
      memory.

  ---

  3. Key Structures to Recognize

  If you look at the code (specifically include/linux/dma-buf.h), these are the structs you will see:

   * `struct dma_buf`: The central object representing the shared buffer. It holds the size, file pointer, and ops.
   * `struct dma_buf_ops`: The vtable implemented by the Exporter.
       * .attach: "Can this device access me?"
       * .map_dma_buf: "Give me the scatter-gather list for this device."
   * `struct dma_buf_attachment`: Represents the link between one Importer and the DMA-BUF. A single buffer can have multiple
     attachments (e.g., one buffer shared by GPU, Video Encoder, and Display).

  4. How the Patchset Uses This

  Relating this back to the io_uring patchset:

   1. Registration: When you register the dmabuf with io_uring, the kernel acts as the Importer. It calls dma_buf_attach.
   2. Submission: When you submit a write request, io_uring (via the block layer) calls dma_buf_map_attachment.
   3. Zero-Copy: The block layer gets the sg_table (DMA addresses) from the dmabuf and passes those addresses directly to the NVMe
      driver.
   4. No CPU Copy: The NVMe drive reads directly from the physical addresses provided by the Exporter (e.g., GPU memory), completely
      bypassing the CPU linear mapping.

```


# Related features

## Patchset "vfio/pci: Allow MMIO regions to be exported through dma-buf"

[[PATCH v6 00/11] vfio/pci: Allow MMIO regions to be exported through dma-buf](https://lore.kernel.org/linux-block/20251102-dmabuf-vfio-v6-0-d773cff0db9f@nvidia.com/)

## [RFC v2 00/11] Add dmabuf read/write via io_uring

[[RFC v2 00/11] Add dmabuf read/write via io_uring](https://lore.kernel.org/linux-block/cover.1763725387.git.asml.silence@gmail.com/)

### Overview from AI

```
This patchset introduces support for performing read and write operations on 
dmabuf file descriptors through io_uring. This allows for efficient, zero-copy 
data transfers between devices by enabling asynchronous I/O on shared dmabuf buffers.

The key changes in this patchset are: 
 - dmabuf Integration with io_uring:
   It adds the necessary infrastructure to io_uring to handle dmabuf buffers, 
   including the management of DMA addresses and dynamic buffer attachments. 

 - Block Layer Modifications: 
   The core logic is integrated into the blk-mq subsystem to support asynchronous 
   DMA mapping and request cancellation.

 - User-space API: The existing io_uring user-space API remains unchanged. A dmabuf 
   can be registered and used like a normal I/O buffer.

Overall, this patchset aims to improve performance and reduce latency for 
applications that rely on dmabuf for data sharing between devices by leveraging the 
asynchronous capabilities of io_uring. The patchset is currently in the RFC (Request for 
Comments) stage, with the author planning future simplifications to reduce complexity.

```

### Here’s a breakdown of how it works:

```
1. The Core Problem: The CPU as a Data Mover

Traditionally, when you want to move data from one device (like a GPU) to another (like an NVMe SSD), the data often has to take a
detour through the main system memory (RAM) and be copied by the CPU.

A typical, inefficient workflow looks like this:

 1. A GPU finishes rendering a frame into a memory buffer.
 2. Your application wants to save this frame to a fast NVMe drive.
 3. The application issues a write() system call.
 4. The kernel instructs the CPU to execute a memcpy-like operation, copying the data from the GPU's buffer into the kernel's page
    cache (a temporary buffer in RAM).
 5. Finally, the kernel instructs the NVMe drive to fetch the data from the page cache and write it to storage.

The bottleneck here is Step 4. The CPU spends valuable cycles just copying data from one place in memory to another. This is slow,
consumes CPU resources that could be used for other tasks, and adds latency.

2. The Solution: A Direct, CPU-less Path

This patchset creates a "fast path" that bypasses the CPU-driven copy. It achieves this in two key ways:

a) dmabuf for Zero-Copy Transfers

A dmabuf (DMA buffer) is a kernel mechanism for sharing a buffer of memory between multiple devices without involving the CPU.
When a device (like a GPU) creates a dmabuf, it's not just a block of data; it's a handle that describes where the data lives in
physical memory.

Other drivers (like a storage driver) can "import" this dmabuf and get the physical memory addresses directly. This allows them to
set up Direct Memory Access (DMA) transfers, where the hardware moves data directly from the source to the destination without the
CPU's intervention. This is the principle of "zero-copy."

b) io_uring for Asynchronous, Low-Overhead I/O

io_uring is a high-performance interface for asynchronous I/O. Its key advantages are:
 * Reduced System Calls: Instead of one system call per I/O operation (like read() or write()), you can submit hundreds of
   operations with a single system call. This dramatically reduces the overhead of switching between user-space and kernel-space.
 * True Asynchronicity: Your application can submit a batch of I/O requests and immediately go back to doing other work. The
   kernel processes them in the background and notifies the application only when they are complete.

How the Patchset Combines Them for Maximum Performance

This patchset teaches io_uring and the block layer how to speak the language of dmabuf.

The new, highly efficient workflow is:

 1. A GPU renders a frame into a dmabuf.
 2. The application registers this dmabuf with io_uring.
 3. The application submits a write operation to io_uring, telling it to write the contents of the dmabuf to the NVMe drive.
 4. Here's the magic:
     * io_uring passes the request to the block layer.
     * The NVMe driver, using the new logic from the patchset, imports the dmabuf.
     * The driver gets the list of physical memory pages for the buffer directly.
     * The driver programs the NVMe controller's DMA engine to pull the data straight from the GPU's memory buffer and write it to
       the SSD.

Summary of Performance Wins:

 * Improved Performance/Throughput: By eliminating the memcpy step, the CPU is freed up. It can be used for application logic
   (e.g., preparing the next frame) while the DMA hardware handles the data transfer in parallel. This allows for a much higher
   rate of data movement.
 * Reduced Latency: The time for a single write operation is significantly lower because you remove the entire data-copying step
   from the critical path. The request is submitted with minimal overhead via io_uring, and the hardware handles the rest.

In essence, this patchset creates a direct pipeline between device memory and storage, orchestrated by the highly efficient
io_uring interface, cutting the slow CPU out of the data path.

```

