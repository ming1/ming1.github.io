---
title: RocksDB learning
category: tech
tags: [storage, database]
---

title:  RocksDB learning

* TOC
{:toc}


# Overview

[How RocksDB works](https://artem.krylysov.com/blog/2023/04/19/how-rocksdb-works/)


## RocksDB Overview

### High Level Architecture

RocksDB is a storage engine library of key-value store interface where keys and
values are arbitrary byte streams. RocksDB organizes all data in sorted order and
the common operations are Get(key), NewIterator(), Put(key, val), Delete(key), and
SingleDelete(key).

The three basic constructs of RocksDB are memtable, sstfile and logfile. The
memtable is an in-memory data structure, new writes are inserted into the memtable
and are optionally written to the logfile (aka. Write Ahead Log(WAL)). The logfile
is a sequentially-written file on storage. When the memtable fills up, it is flushed
to a sstfile on storage and the corresponding logfile can be safely deleted. The
data in an sstfile is sorted to facilitate easy lookup of keys.

- Metadata storage

A manifest log file is used to record all the database state changes. The compaction
process adds new files and deletes existing files from the database, and it makes
these operations persistent by recording them in the MANIFEST.


### Features

- Column Families

- 


## Resources

[RocksDB wiki](https://github.com/facebook/rocksdb/wiki)

[Journal](https://github.com/facebook/rocksdb/wiki/Journal)

[Atomic Flush](https://github.com/facebook/rocksdb/wiki/Atomic-flush)

[Compression](https://github.com/facebook/rocksdb/wiki/Compression)


# Compression (From gemini-cli)

## block based compression

```
  How Compression Works: Block-Based


  The most important thing to understand is that RocksDB performs compression at the block level, not at the individual value
  level.


  An SSTable file is not a monolithic compressed blob. Instead, it's composed of many smaller "data blocks." Each data block
  contains a group of key-value pairs that are sorted and stored contiguously.


  The compression process is:
   1. When RocksDB flushes a memtable or performs a compaction, it takes a chunk of sorted key-value pairs.
   2. This chunk becomes a "data block" (the size is configurable, typically 4KB to 32KB).
   3. RocksDB applies the chosen compression algorithm (like Snappy, ZSTD, etc.) to the entire block.
   4. This compressed block is then written to the SSTable file.


  This approach is very efficient because it amortizes the overhead of compression over many key-value pairs. Compressing each
  value individually would be very inefficient, especially for small values.

```

```
  We will use a concrete example:
   * `options.block_size = 4 KB` (4096 bytes)
   * `options.compression = kZSTD`


  The Big Picture: Where Things Live

  First, a quick reminder of the state of data in each location:


   * Memtable: Uncompressed. Data is live in a skiplist for fast reads/writes.
   * WAL: Uncompressed (by default). It's a log for durability.
   * SSTable: Compressed. This is where data lives permanently on disk.

  ---

  Step 1: Writing and Compressing (The Flush)


  This process happens when a memtable becomes full and is flushed to a new Level-0 SSTable.


  1.1. Reading from the Memtable
  The flush process creates an iterator over the entire memtable. It starts reading the key-value pairs sequentially, in
  sorted order.


  1.2. Forming a Data Block
  The flush process reads just enough consecutive key-value pairs from the memtable to fill up a buffer that is approximately
  4 KB in size (your configured block_size).


  Let's say your average key-value pair is 100 bytes. The process would read about 40-41 key-value pairs into an in-memory
  buffer. This buffer is the source data for compression.


   * Input to Compressor: A 4 KB chunk of uncompressed, sorted key-value pairs.


   1     [key1, value1, key2, value2, key3, value3, ... key40, value40]



  1.3. Compressing the Block
  This 4 KB buffer is then passed to the ZSTD compression function.


   * ZSTD_compress(output_buffer, ..., input_buffer, 4096, ...)


  Let's assume ZSTD achieves a 3:1 compression ratio.


   * Output of Compressor: A ~1.3 KB chunk of compressed binary data.


  1.4. Writing to the SSTable File
  This compressed ~1.3 KB block is now written to the SSTable file on disk. RocksDB also writes some metadata, like the
  block's offset and size, to the file's index.


  This process repeats:
   * Read the next ~40 KV pairs from the memtable into the 4 KB buffer.
   * Compress the buffer down to ~1.3 KB.
   * Write the compressed block to the SSTable file.
   * ...and so on, until the entire memtable has been written out as a series of compressed blocks.

  ---


  Step 2: Reading and Decompressing (The Get operation)

  This process happens when you call db->Get("my_key", &value).


  2.1. Finding the Right SSTable
  First, RocksDB checks the memtables. If the key isn't there, it uses its metadata and Bloom filters to quickly identify the
  specific SSTable file on disk that might contain "my_key".


  2.2. Finding the Right Block (Using the Index)
  RocksDB does not read the whole SSTable file. At the end of every SSTable is an index block. This index maps key ranges to
  block locations.


  RocksDB reads the index and quickly determines that "my_key" (if it exists) must be located in, for example, "Block #73" at
  offset 54321 in the file.


  2.3. Reading the Compressed Block from Disk
  Now, RocksDB performs the actual I/O. It seeks to offset 54321 in the SSTable file and reads the entire compressed block
  into memory.


   * How much data is read from the SSTable? The size of the compressed block. In our example, this is ~1.3 KB. This data is
     placed into the Block Cache, so subsequent reads for keys in the same block will be served from RAM and require no disk
     I/O.


  2.4. Decompressing the Block
  The ~1.3 KB of compressed data is passed to the ZSTD decompression function.


   * ZSTD_decompress(output_buffer, ..., compressed_block_buffer, 1300, ...)


  This inflates the data back to the original 4 KB uncompressed block in memory.


   * Output of Decompressor: The 4 KB chunk of uncompressed key-value pairs.


   1     [key1, value1, key2, value2, ... "my_key", "its_value", ... key40, value40]



  2.5. Finding the Key in the Decompressed Block
  Finally, RocksDB performs a binary search within the 4 KB uncompressed block in memory to find the exact key "my_key" and
  retrieve its corresponding value. This value is then returned to your application.

  Summary of Data Flow

```

# Random Ideas

## Fixed-length key optimization

```
  The Single Most Important Optimization: prefix_extractor


  This is the key to unlocking performance for fixed-length keys. A prefix_extractor allows RocksDB to build a separate index
  on a prefix of your keys.


  For fixed-length keys, the concept of a "prefix" is a perfect match. You can treat the entire key as the prefix.


  How to implement it:
  You use the NewFixedPrefixTransform(N) function, where N is the exact length of your keys.

```

## Safe to set WriteOptions::disableWAL if we don't care data-loss


## any other optimizations for using RocksDB for storage

### basic ideas

- use 64bit LBA as key

- data is byte stream, which length is fixed 'logical block size' of the
  block device

- enable compression

Compression is done in block level [key 0, value 0, key 1, value 1, ..., key N - 1, value N - 1]

