---
title: Understanding LLM and Transformer
category: tech
tags: [AI, LLM, Transformer]
---

title: Understanding LLM and Transformer

* TOC
{:toc}



# Introduction to LLM

[**Visualize transformer**](https://poloclub.github.io/transformer-explainer/)

[Â§ßËØ≠Ë®ÄÊ®°ÂûãËÆ≠ÁªÉÊï∞ÊçÆ](https://arxiv.org/html/2411.07715v1)

[Large language model](https://en.wikipedia.org/wiki/Large_language_model)


## What is LLM?

```
A large language model (LLM) is a language model trained with self-supervised  
machine learning on a vast amount of text, designed for natural language processing  
tasks, especially language generation.[1][2] The largest and most capable LLMs  
are generative pre-trained transformers (GPTs) and provide the core capabilities  
of chatbots such as ChatGPT, Gemini and Claude. LLMs can be fine-tuned for specific  
tasks or guided by prompt engineering.[3] These models acquire predictive power  
regarding syntax, semantics, and ontologies[4] inherent in human language corpora,  
but they also inherit inaccuracies and biases present in the data they are trained on.[5]
```

```
They consist of billions to trillions of parameters and operate as general-purpose  
sequence models, generating, summarizing, translating, and reasoning over text. LLMs  
represent a significant new technology in their ability to generalize across tasks  
with minimal task-specific supervision, enabling capabilities like conversational  
agents, code generation, knowledge retrieval, and automated reasoning that previously  
required bespoke systems.[6]
```

## What is Transformer?

[**Visualize transformer**](https://poloclub.github.io/transformer-explainer/)

Transformer is a neural network architecture that has fundamentally changed the approach  
to Artificial Intelligence. Transformer was first introduced in the seminal paper  
"Attention is All You Need" in 2017 and has since become the go-to architecture for deep  
learning models, powering text-generative models like OpenAI's GPT, Meta's Llama, and  
Google's Gemini. Beyond text, Transformer is also applied in audio generation, image  
recognition, protein structure prediction, and even game playing, demonstrating its  
versatility across numerous domains.

Fundamentally, **text-generative Transformer models operate on the principle of next-token  
prediction: given a text prompt from the user, what is the most probable next token (a  
word or part of a word) that will follow this input?** The core innovation and power of  
Transformers lie in their use of self-attention mechanism, which allows them to process  
entire sequences and capture long-range dependencies more effectively than previous  
architectures.

### Transformer Architecture

#### Embedding

- Tokenization

The full vocabulary of tokens is decided before training the model: GPT-2's vocabulary has  
50,257 unique tokens. 

- Token Embedding

GPT-2 (small) represents each token in the vocabulary as a 768-dimensional vector; the  
dimension of the vector depends on the model.

These embedding vectors are stored in a matrix of shape (50,257, 768), containing approximately  
39 million parameters!

This extensive matrix allows the model to assign semantic meaning to each token, in the  
sense that **tokens with similar usage or meaning in language are placed close together**  
in this high-dimensional space, while dissimilar tokens are farther apart.  

- Positional Encoding

The Embedding layer also encodes information about each token's position in the input prompt.  
Different models use various methods for positional encoding. GPT-2 trains its own positional  
encoding matrix from scratch, integrating it directly into the training process.

- Final Embedding

Finally, we sum the token and positional encodings to get the final embedding representation. This  
combined representation captures both the semantic meaning of the tokens and their position in  
the input sequence.

#### Transformer Block

The core of the Transformer's processing lies in the Transformer block, which comprises multi-head  
self-attention and a Multi-Layer Perceptron layer.

**Most models consist of multiple such blocks that are stacked sequentially one after the other**.

The token representations evolve through layers, from the first block to the last one, allowing  
the model to build up an intricate understanding of each token. This layered approach leads to  
higher-order representations of the input.

The GPT-2 (small) model we are examining consists of **12 such blocks**.


##### Multi-Head Self-Attention

The self-attention mechanism enables the model to capture relationships among tokens in a  
sequence, so that each token‚Äôs representation is influenced by the others.

Multiple attention heads allow the model to consider these relationships from different  
perspectives; for example, one head may capture short-range syntactic links while another  
tracks broader semantic context. 

###### Step 1: Query, Key, and Value Matrices

Each token's embedding vector is transformed into three vectors: Query (Q), Key (K), and  
Value (V). These vectors are derived by multiplying the input embedding matrix with learned  
weight matrices for Q, K, and V. Here's a web search analogy to help us build some intuition  
behind these matrices:

![Transformer Query, Key and Value](/assets/images/transformer-QKV.png)


$$\mathbf{Q}_{i,j} = \left( \sum_{d=1}^{768} \mathbf{E}_{i,d} \cdot \mathbf{W}_{d,j} \right) + \mathbf{B}_{j}$$  


- Query (Q) is the search text you type in the search engine bar.

This is the token you want to "find more information about".

- Key (K) is the title of each web page in the search result window.

It represents the possible tokens the query can attend to.

- Value (V) is the actual content of web pages shown.

Once we matched the appropriate search term (Query) with the relevant results (Key),  
we want to get the content (Value) of the most relevant pages.

By using these QKV values, the model can calculate attention scores, which determine how  
much focus each token should receive when generating predictions.

From google search:

```
In the Transformer architecture, Query (Q), Key (K), and Value (V) matrices are fundamental
components of the self-attention mechanism, which enables the model to weigh the importance
of different parts of the input sequence when processing each element. 

Query (Q): The Query matrix represents "what you are looking for" for each word or token in
the sequence. Each row of the Query matrix corresponds to a word's query vector, which is
used to compare against the Key vectors of other words to determine their relevance.

Key (K): The Key matrix represents "how to identify information" for each word or token.
Each row of the Key matrix corresponds to a word's key vector, encoding information about
that word in a searchable format. The dot product between a Query vector and Key vectors
yields attention scores, indicating how well a particular word's query matches another
word's key.

Value (V): The Value matrix represents "what information is provided" by each word or
token. Each row of the Value matrix corresponds to a word's value vector, containing the
actual content or meaning that the word contributes to the overall representation. These
value vectors are weighted by the attention scores to create a contextualized
representation for each word.

How they are formed:

These matrices are typically derived from the input embeddings of the sequence. For each
word embedding, three distinct linear transformations (learned weight matrices) are applied
to generate its corresponding Query, Key, and Value vectors. These vectors are then stacked
to form the Q, K, and V matrices for the entire sequence.
In essence:

The Q, K, and V matrices facilitate a "search-and-retrieve" mechanism within the self-attention
layer. Queries seek relevant information, Keys provide searchable attributes, and Values
offer the actual content to be integrated, allowing the Transformer to build rich, context-aware
representations of the input.

```

###### Step 2: Multi-Head Splitting

Query, key, and Value vectors are split into multiple heads‚Äîin GPT-2 (small)'s case, into
12 heads.

**Each head processes a segment of the embeddings independently, capturing different  
syntactic and semantic relationships.**

**This design facilitates parallel learning of diverse linguistic features, enhancing  
the model's representational power.**


###### Step 3: Masked Self-Attention

In each head, we perform masked self-attention calculations. This mechanism allows the  
model to generate sequences by focusing on relevant parts of the input while preventing  
access to future tokens.

- Dot Product:

The dot product of Query and Key matrices determines the attention score, producing a  
square matrix that reflects the relationship between all input tokens.

- Scaling ¬∑ Mask:

The attention scores are scaled and a mask is applied to the upper triangle of the  
attention matrix to prevent the model from accessing future tokens, setting these values  
to negative infinity. The model needs to learn how to predict the next token without  
‚Äúpeeking‚Äù into the future.

- Softmax ¬∑ Dropout:

After masking and scaling, the attention scores are converted into probabilities by  
the softmax operation, then optionally regularized with dropout. Each row of the matrix  
sums to one and indicates the relevance of every other token to the left of it.

###### Step 4: Output and Concatenation

The model uses the masked self-attention scores and multiplies them with the Value matrix  
to get the final output of the self-attention mechanism. GPT-2 has 12 self-attention heads,  
each capturing different relationships between tokens. The outputs of these heads are  
concatenated and passed through a linear projection.


##### MLP: Multi-Layer Perceptron

The MLP block consists of two linear transformations with a GELU activation function in between.

The first linear transformation expands the dimensionality of the input four-fold from 768 to 3072.  
This expansion step allows the model to project the token representations into a higher-dimensional  
space, where it can capture richer and more complex patterns that may not be visible in the  
original dimension.


The second linear transformation then reduces the dimensionality back to the original size of 768.  
This compression step brings the representations back to a manageable size while retaining the  
useful nonlinear transformations introduced in the expansion step.


##### Output Probabilities

After the input has been processed through all Transformer blocks, the output is passed through  
the final linear layer to prepare it for token prediction. This layer projects the final  
representations into a 50,257 dimensional space, where every token in the vocabulary has a  
corresponding value called logit. Any token can be the next word, so this process allows us  
to simply rank these tokens by their likelihood of being that next word. We then apply the  
softmax function to convert the logits into a probability distribution that sums to one.  
This will allow us to sample the next token based on its likelihood.

The final step is to generate the next token by sampling from this distribution

- temperature hyperparameter

The temperature hyperparameter plays a critical role in this process. Mathematically speaking,  
it is a very simple operation: model output logits are simply divided by the temperature:

temperature = 1: Dividing logits by one has no effect on the softmax outputs.

temperature < 1: Lower temperature makes the model more confident and deterministic by  
sharpening the probability distribution, leading to more predictable outputs.

temperature > 1: Higher temperature creates a softer probability distribution, allowing  
for more randomness in the generated text ‚Äì what some refer to as model ‚Äúcreativity‚Äù.

In addition, the sampling process can be further refined using top-k and top-p parameters:

top-k sampling:

Limits the candidate tokens to the top k tokens with the highest probabilities, filtering out
less likely options.

top-p sampling:

Considers the smallest set of tokens whose cumulative probability exceeds a threshold p,  
ensuring that only the most likely tokens contribute while still allowing for diversity.

By tuning temperature, top-k, and top-p, you can balance between deterministic and diverse  
outputs, tailoring the model's behavior to your specific needs.


### Advanced Architectural Features

Layer Normalization, Dropout, and Residual Connections are crucial components in Transformer  
models, particularly during the training phase.

Layer Normalization stabilizes training and helps the model converge faster. Dropout prevents  
overfitting by randomly deactivating neurons. Residual Connections allows gradients to flow  
directly through the network and helps to prevent the vanishing gradient problem.

#### Layer Normalization

Layer Normalization helps to stabilize the training process and improves convergence. It  
works by normalizing the inputs across the features, ensuring that the mean and variance of  
the activations are consistent. This normalization helps mitigate issues related to internal  
covariate shift, allowing the model to learn more effectively and reducing the sensitivity  
to the initial weights. Layer Normalization is applied twice in each Transformer block, once  
before the self-attention mechanism and once before the MLP layer.


#### Dropout

Dropout is a regularization technique used to prevent overfitting in neural networks by  
randomly setting a fraction of model weights to zero during training. This encourages the  
model to learn more robust features and reduces dependency on specific neurons, helping  
the network generalize better to new, unseen data. During model inference, dropout is  
deactivated. This essentially means that we are using an ensemble of the trained  
subnetworks, which leads to a better model performance.


#### Residual Connections

Residual connections were first introduced in the ResNet model in 2015. This architectural  
innovation revolutionized deep learning by enabling the training of very deep neural  
networks. Essentially, residual connections are shortcuts that bypass one or more layers,  
adding the input of a layer to its output. This helps mitigate the vanishing gradient  
problem, making it easier to train deep networks with multiple Transformer blocks stacked  
on top of each other. In GPT-2, residual connections are used twice within each Transformer  
block: once before the MLP and once after, ensuring that gradients flow more easily, and  
earlier layers receive sufficient updates during back propagation.


## What is vLLM?

[What is vLLM?](https://www.redhat.com/en/topics/ai/what-is-vllm)

```
vLLM, which stands for virtual large language model, is a library of open source code  
maintained by the vLLM community. It helps large language models (LLMs) perform  
calculations more efficiently and at scale.

Specifically, vLLM is an inference server that speeds up the output of generative  
AI applications by making better use of the GPU memory. 
```

## Related papers

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

[A Comprehensive Overview of Large Language Models](https://arxiv.org/html/2307.06435v9)

[https://arxiv.org/html/2510.02209v1](https://arxiv.org/html/2510.02209v1)


# Hugging Face llm-course

[Hugging Face llm-course](https://huggingface.co/learn/llm-course/chapter1/1)

## Transformer models

### Introduction

#### Understanding NLP and LLMs

- NLP (Natural Language Processing)

NLP (Natural Language Processing) is the broader field focused on enabling computers to  
understand, interpret, and generate human language. NLP encompasses many techniques and  
tasks such as sentiment analysis, named entity recognition, and machine translation.

- LLMs (Large Language Models)

LLMs (Large Language Models) are a powerful subset of NLP models characterized by their  
massive size, extensive training data, and ability to perform a wide range of language  
tasks with minimal task-specific training. Models like the Llama, GPT, or Claude series  
are examples of LLMs that have revolutionized what‚Äôs possible in NLP.

### Natural Language Processing and Large Language Models

#### The following is a list of common NLP tasks, with some examples of each:

- Classifying whole sentences

Getting the sentiment of a review, detecting if an email is spam, determining if a sentence  
is grammatically correct or whether two sentences are logically related or not

- Classifying each word in a sentence

Identifying the grammatical components of a sentence (noun, verb, adjective), or the named  
entities (person, location, organization)

- Generating text content

Completing a prompt with auto-generated text, filling in the blanks in a text with masked words

- Extracting an answer from a text

Given a question and a context, extracting the answer to the question based on the information  
provided in the context

- Generating a new sentence from an input text

Translating a text into another language, summarizing a text

NLP isn‚Äôt limited to written text though. It also tackles complex challenges in speech recognition  
and computer vision, such as generating a transcript of an audio sample or a description of an image.

#### The Rise of Large Language Models (LLMs)

In recent years, the field of NLP has been revolutionized by Large Language Models (LLMs). These models,  
which include architectures like GPT (Generative Pre-trained Transformer) and Llama, have transformed  
what‚Äôs possible in language processing.

A large language model (LLM) is an AI model trained on massive amounts of text data that can

    - understand and generate human-like text

    - recognize patterns in language,

    - and perform a wide variety of language tasks without task-specific training.

They represent a significant advancement in the field of natural language processing (NLP).

- LLMs are characterized by:

Scale: They contain millions, billions, or even hundreds of billions of parameters

General capabilities: They can perform multiple tasks without task-specific training

In-context learning: They can learn from examples provided in the prompt

Emergent abilities: As these models grow in size, they demonstrate capabilities that weren‚Äôt
explicitly programmed or anticipated


- However, LLMs also have important limitations:

Hallucinations: They can generate incorrect information confidently

Lack of true understanding: They lack true understanding of the world and operate purely on statistical patterns

Bias: They may reproduce biases present in their training data or inputs.

Context windows: They have limited context windows (though this is improving)

Computational resources: They require significant computational resources

#### Why is language processing challenging?

Computers don‚Äôt process information in the same way as humans. For example, when we read the sentence  
‚ÄúI am hungry,‚Äù we can easily understand its meaning. Similarly, given two sentences such as  
‚ÄúI am hungry‚Äù and ‚ÄúI am sad,‚Äù we‚Äôre able to easily determine how similar they are. For machine  
learning (ML) models, such tasks are more difficult. The text needs to be processed in a way that  
enables the model to learn from it. And because language is complex, we need to think carefully  
about how this processing must be done. There has been a lot of research done on how to represent  
text, and we will look at some methods in the next chapter.

Even with the advances in LLMs, many fundamental challenges remain. These include understanding  
ambiguity, cultural context, sarcasm, and humor. LLMs address these challenges through massive  
training on diverse datasets, but still often fall short of human-level understanding in many  
complex scenarios.

### Transformers, what can they do?

Transformer models are used to solve all kinds of tasks across different modalities, including  
natural language processing (NLP), computer vision, audio processing, and more.

### How do Transformers work?

[Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- Transformers are language models

All the Transformer models mentioned above (GPT, BERT, T5, etc.) have been trained as language models.
This means they have been trained on large amounts of raw text in a self-supervised fashion.

Self-supervised learning is a type of training in which the objective is automatically computed from  
the inputs of the model. That means that humans are not needed to label the data!

This type of model develops a statistical understanding of the language it has been trained on, but  
it‚Äôs less useful for specific practical tasks. Because of this, the general pretrained model then  
goes through a process called transfer learning or fine-tuning. During this process, the model is  
fine-tuned in a supervised way ‚Äî that is, using human-annotated labels ‚Äî on a given task.

- Transformers are big models

Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by  
increasing the models‚Äô sizes as well as the amount of data they are pretrained on.

Unfortunately, training a model, especially a large one, requires a large amount of data. This becomes  
very costly in terms of time and compute resources. It even translates to environmental impact, as can  
be seen in the following graph.

This is why sharing language models is paramount: sharing the trained weights and building on top of  
already trained weights reduces the overall compute cost and carbon footprint of the community.

By the way, you can evaluate the carbon footprint of your models‚Äô training through several tools.  
For example ML CO2 Impact or Code Carbon which is integrated in ü§ó Transformers. To learn more about  
this, you can read this blog post which will show you how to generate an emissions.csv file with an  
estimate of the footprint of your training, as well as the documentation of ü§ó Transformers addressing  
this topic.

- Transfer Learning

Pretraining is the act of training a model from scratch: the weights are randomly initialized, and the  
training starts without any prior knowledge.

Fine-tuning, on the other hand, is the training done after a model has been pretrained. To perform  
fine-tuning, you first acquire a pretrained language model, then perform additional training with a  
dataset specific to your task. Wait ‚Äî why not simply train the model for your final use case from the  
start (scratch)? There are a couple of reasons:


    - The pretrained model was already trained on a dataset that has some similarities with the  
    fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired
    by the initial model during pretraining (for instance, with NLP problems, the pretrained model
    will have some kind of statistical understanding of the language you are using for your task).

    - Since the pretrained model was already trained on lots of data, the fine-tuning requires way
    less data to get decent results.

    - For the same reason, the amount of time and resources needed to get good results are much lower.

Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also  
quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining  
than a full pretraining.

#### General Transformer architecture

The model is primarily composed of two blocks:

Encoder (left): The encoder receives an input and builds a representation of it (its features). This  
means that the model is optimized to acquire understanding from the input.

Decoder (right): The decoder uses the encoder‚Äôs representation (features) along with other inputs to  
generate a target sequence. This means that the model is optimized for generating outputs.

Each of these parts can be used independently, depending on the task:

- Encoder-only models: Good for tasks that require understanding of the input, such as sentence
classification and named entity recognition.

- Decoder-only models: Good for generative tasks such as text generation.

- Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an
input, such as translation or summarization.


##### Attention layers

this layer will tell the model to pay specific attention to certain words in the sentence you passed  
it (and more or less ignore the others) when dealing with the representation of each word.

##### The original architecture

The Transformer architecture was originally designed for translation. During training, the encoder  
receives inputs (sentences) in a certain language, while the decoder receives the same sentences in  
the desired target language. In the encoder, the attention layers can use all the words in a sentence  
(since, as we just saw, the translation of a given word can be dependent on what is after as well as  
before it in the sentence). The decoder, however, works sequentially and can only pay attention to  
the words in the sentence that it has already translated (so, only the words before the word currently  
being generated). For example, when we have predicted the first three words of the translated target,  
we give them to the decoder which then uses all the inputs of the encoder to try to predict the  
fourth word.

To speed things up during training (when the model has access to target sentences), the decoder is fed  
the whole target, but it is not allowed to use future words (if it had access to the word at position 2  
when trying to predict the word at position 2, the problem would not be very hard!). For instance,  
when trying to predict the fourth word, the attention layer will only have access to the words in  
positions 1 to 3.

### How ü§ó Transformers solve tasks

Language models work by being trained to predict the probability of a word given the context of  
surrounding words. This gives them a foundational understanding of language that can generalize to  
other tasks.

There are two main approaches for training a transformer model:

Masked language modeling (MLM): Used by encoder models like BERT, this approach randomly masks  
some tokens in the input and trains the model to predict the original tokens based on the surrounding  
context. This allows the model to learn bidirectional context (looking at words both before and after  
the masked word).

Causal language modeling (CLM): Used by decoder models like GPT, this approach predicts the next token  
based on all previous tokens in the sequence. The model can only use context from the left (previous  
tokens) to predict the next token.


Understanding which part of the Transformer architecture (encoder, decoder, or both) is best suited  
for a particular NLP task is key to choosing the right model. Generally, tasks requiring bidirectional  
context use encoders, tasks generating text use decoders, and tasks converting one sequence to another  
use encoder-decoders.

#### Text generation

Text generation involves creating coherent and contextually relevant text based on a prompt or input.

GPT-2 is a decoder-only model pretrained on a large amount of text. It can generate convincing (though  
not always true!) text given a prompt and complete other NLP tasks like question answering despite not  
being explicitly trained to.

GPT-2 uses byte pair encoding (BPE) to tokenize words and generate a token embedding. Positional encodings  
are added to the token embeddings to indicate the position of each token in the sequence. The input  
embeddings are passed through multiple decoder blocks to output some final hidden state. Within each decoder  
block, GPT-2 uses a masked self-attention layer which means GPT-2 can‚Äôt attend to future tokens. It is  
only allowed to attend to tokens on the left. This is different from BERT‚Äôs [mask] token because, in masked  
self-attention, an attention mask is used to set the score to 0 for future tokens.

The output from the decoder is passed to a language modeling head, which performs a linear transformation  
to convert the hidden states into logits. The label is the next token in the sequence, which are created by  
shifting the logits to the right by one. The cross-entropy loss is calculated between the shifted logits  
and the labels to output the next most likely token.

GPT-2‚Äôs pretraining objective is based entirely on causal language modeling, predicting the next word in a  
sequence. This makes GPT-2 especially good at tasks that involve generating text.

```
Multi-Head Attention is the core innovation of the Transformer architecture, designed to enhance the model's  
ability to capture diverse relationships within the input data. It is an extension of the single Self-Attention  
mechanism.

Think of it as looking at the same sentence through multiple pairs of specialized lenses (the "heads")  
simultaneously.

Instead of performing a single attention calculation, Multi-Head Attention performs h (the number of heads)  
attention calculations in parallel.

```

### Transformer Architectures

- Attention mechanisms

Most transformer models use full attention in the sense that the attention matrix is square. It can be a  
big computational bottleneck when you have long texts. Longformer and reformer are models that try to be  
more efficient and use a sparse version of the attention matrix to speed up training.

Standard attention mechanisms have a computational complexity of O(n¬≤), where n is the sequence length.  
This becomes problematic for very long sequences. The specialized attention mechanisms below help address  
this limitation.

- LSH attention

Reformer uses LSH attention. In the softmax(QK^t), only the biggest elements (in the softmax dimension)  
of the matrix QK^t are going to give useful contributions. So for each query q in Q, we can consider only  
the keys k in K that are close to q. A hash function is used to determine if q and k are close. The  
attention mask is modified to mask the current token (except at the first position), because it will give  
a query and a key equal (so very similar to each other). Since the hash can be a bit random, several hash  
functions are used in practice (determined by a n_rounds parameter) and then are averaged together.

### Deep dive into Text Generation Inference with LLMs

Inference is the process of using a trained LLM to generate human-like text from a given input prompt.  
Language models use their knowledge from training to formulate responses one word at a time. The model  
leverages learned probabilities from billions of parameters to predict and generate the next token in a  
sequence.


#### The Role of Attention

The attention mechanism is what gives LLMs their ability to understand context and generate coherent  
responses. When predicting the next word, not every word in a sentence carries equal weight - for  
example, in the sentence ‚ÄúThe capital of France is ‚Ä¶‚Äù, the words ‚ÄúFrance‚Äù and ‚Äúcapital‚Äù are crucial  
for determining that ‚ÄúParis‚Äù should come next. This ability to focus on relevant information is what  
we call attention.

This process of identifying the most relevant words to predict the next token has proven to be  
incredibly effective. Although the basic principle of training LLMs‚Äîpredicting the next token‚Äîhas  
remained generally consistent since BERT and GPT-2, there have been significant advancements in  
scaling neural networks and making the attention mechanism work for longer and longer sequences, at  
lower and lower costs.

**In short, the attention mechanism is the key to LLMs being able to generate text that is both coherent  
and context-aware. It sets modern LLMs apart from previous generations of language models.**


#### Context Length and Attention Span

The context length refers to the maximum number of tokens (words or parts of words) that the LLM can  
process at once. Think of it as the size of the model‚Äôs working memory.

These capabilities are limited by several practical factors:

- The model‚Äôs architecture and size

- Available computational resources

- The complexity of the input and desired output

In an ideal world, we could feed unlimited context to the model, but hardware constraints and  
computational costs make this impractical. This is why different models are designed with different  
context lengths to balance capability with efficiency.

The context length is the maximum number of tokens the model can consider at once when generating a response.

#### The Art of Prompting

When we pass information to LLMs, we structure our input in a way that guides the generation of  
the LLM toward the desired output. This is called prompting.

Understanding how LLMs process information helps us craft better prompts. Since the model‚Äôs  
primary task is to predict the next token by analyzing the importance of each input token, the  
wording of your input sequence becomes crucial.

#### The Two-Phase Inference Process

Now that we understand the basic components, let‚Äôs dive into how LLMs actually generate text.  
The process can be broken down into two main phases: prefill and decode. These phases work  
together like an assembly line, each playing a crucial role in producing coherent text.

##### The Prefill Phase

- Tokenization: Converting the input text into tokens (think of these as the basic building blocks the model understands)

- Embedding Conversion: Transforming these tokens into numerical representations that capture their meaning

- Initial Processing: Running these embeddings through the model‚Äôs neural networks to create a rich understanding of the context

This phase is computationally intensive because it needs to process all input tokens at once.  
Think of it as reading and understanding an entire paragraph before starting to write a response.


##### The Decode Phase

The decode phase involves several key steps that happen for each new token:

- Attention Computation: Looking back at all previous tokens to understand context

- Probability Calculation: Determining the likelihood of each possible next token

- Token Selection: Choosing the next token based on these probabilities

- Continuation Check: Deciding whether to continue or stop generation

This phase is memory-intensive because the model needs to keep track of all previously generated  
tokens and their relationships.


#### Sampling Strategies

##### Understanding Token Selection: From Probabilities to Token Choices


##### Managing Repetition: Keeping Output Fresh


##### Controlling Generation Length: Setting Boundaries 

We can control generation length in several ways:

Token Limits: Setting minimum and maximum token counts

Stop Sequences: Defining specific patterns that signal the end of generation

End-of-Sequence Detection: Letting the model naturally conclude its response


##### Beam Search: Looking Ahead for Better Coherence

While the strategies we‚Äôve discussed so far make decisions one token at a time, beam search  
takes a more holistic approach. Instead of committing to a single choice at each step, it  
explores multiple possible paths simultaneously - like a chess player thinking several moves ahead.


Here‚Äôs how it works:

At each step, maintain multiple candidate sequences (typically 5-10)

For each candidate, compute probabilities for the next token

Keep only the most promising combinations of sequences and next tokens

Continue this process until reaching the desired length or stop condition
Select the sequence with the highest overall probability


#### Practical Challenges and Optimization

##### Key Performance Metrics

When working with LLMs, four critical metrics will shape your implementation decisions:

Time to First Token (TTFT):

How quickly can you get the first response? This is crucial for user experience and is primarily
affected by the prefill phase.

Time Per Output Token (TPOT):

How fast can you generate subsequent tokens? This determines the overall generation speed.

Throughput:

How many requests can you handle simultaneously? This affects scaling and cost efficiency.

VRAM Usage:

How much GPU memory do you need? This often becomes the primary constraint in real-world applications.

##### The Context Length Challenge

One of the most significant challenges in LLM inference is managing context length effectively. Longer  
contexts provide more information but come with substantial costs:

Memory Usage: Grows quadratically with context length

Processing Speed: Decreases linearly with longer contexts

Resource Allocation: Requires careful balancing of VRAM usage

##### The KV Cache Optimization

To address these challenges, one of the most powerful optimizations is KV (Key-Value) caching. This  
technique significantly improves inference speed by storing and reusing intermediate calculations.  
This optimization:

Reduces repeated calculations

Improves generation speed

Makes long-context generation practical

The trade-off is additional memory usage, but the performance benefits usually far outweigh this cost.


# Checkpoints in LLM

[Machine Learning Checkpointing](https://www.giskard.ai/glossary/machine-learning-checkpointing)

[DDN Importance of Checkpoints with LLM](https://indico.cern.ch/event/1289243/contributions/5624317/attachments/2735220/4756168/Vic%20Presentation%202023%20-%2010%20Darft.pdf)

[A Checkpoint on Checkpoints in LLMs](https://www.vastdata.com/blog/a-checkpoint-on-checkpoints-in-llms)



# Questions

## Why training error can't be avoided?

During LLM training, snapshot often need to backup, why is the training
error can't be avoided?






