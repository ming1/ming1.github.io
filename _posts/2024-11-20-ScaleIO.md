---
title: ScacleIO note
category: operation
tags: [cloud, storage, block layer]
---

* TOC
{:toc}

ScaleIO note

# what is ScaleIO

[Understanding the difference between EMC ECS and EMC ScaleIO](https://stackoverflow.com/questions/41850427/understanding-the-difference-between-emc-ecs-and-emc-scaleio?newreg=747b63c22bb64c3cbff829c85b0d597d)

> ScaleIO is block-level storage. Every host in a cluster can function as both
> a compute and storage node. The nodes have direct attached storage. ScaleIO
> is installed on each host and pools the storage resources. It's a distributed
> and scale-out storage architecture. This type of storage is needed when running
> high performant applications that need high availability such as general
> applications and databases.  

  
> ECS (Elastic Cloud Storage) is object based storage. It's analogous to running
> Amazon S3 on premise. So if you're application uses object storage to get media
> such as JPG/MOV/MPEG/DOC/XLS type of files, then this is what you want to use.

# ScaleIO framework

From [EMC SCALEIO BASIC ARCHITECTURE DOCUMENTATION][1], ScaleIO systems contain a
number of elements including the SDC, SDS and MDM.

![Framework](/assets/images/sdwan1.png)

![SDC and SDS in same host](/assets/images/scaleio-framework.png)


## SDC

> The SDC is a lightweight **block device driver** that exposes ScaleIO shared block
> volumes to applications. The SDC runs on the same server as the application. This
> enables the application to issue an IO request and the SDC fulfills it regardless
> of where the particular blocks physically reside. The SDC communicates with other
> nodes (beyond its own local server) over TCP/IP-based protocol, so it is fully
> routable.  

  
> Users may modify the default ScaleIO configuration parameter to allow two SDCs to
> access the same data. This feature provides supportability of applications like
> Oracle RAC.

SDC block device driver: proprietary 3rd party module `scini`

## SDS

> The SDS owns local storage that contributes to the ScaleIO Storage Pools. An
> instance of the SDS runs on every server that contributes some or all of its
> local storage space (HDDs, SSDs, PCIe, NVMe and flash cards) to the aggregated
> pool of storage within the ScaleIO virtual SAN. Local storage may be disks,
> disk partitions, even files. The role of the SDS is to actually perform the
> **Back-End IO operations** as requested by an SDC.


## MDM

> The Meta Data Manager manages the ScaleIO system. The MDM contains all the
> metadata required for system operation; such as configuration changes. The MDM
> also allows monitoring capabilities to assist users with most system management
> tasks.  

 
> The MDM manages the meta data, SDC, SDS, devices mapping, volumes, snapshots, system
> capacity including device allocations and/or release of capacity, RAID protection,
> errors and failures, and system rebuild tasks including rebalancing. In addition, all
> user interaction with the system is handled by the MDM. In a **normal IO flow**, the MDM
> is not part of the data path and user data does not pass through the MDM. Therefore,
> the MDM is never a performance bottleneck for IO operations.  

   
> Currently, an MDM can manage up to 1024 servers. When several MDMs are present, an
> SDC may be managed by several MDMs, whereas, **an SDS can only belong to one MDM**.
> ScaleIO version 2.0 and later supports five MDMs (with a minimum of three) where we
> define a Master, Slave and Tie-breaker MDM.  

   
> The MDM is extremely lightweight and has an asynchronous (or lazy) interaction with
> the SDCs and SDSs. The MDM daemon produces a **heartbeat** where updates are performed
> every few seconds. If the MDM does not detect the heartbeat from an SDS it will
> initiate a rebuild.  

  
> All ScaleIO commands are asynchronous with one exception. For consistency reasons,
> the **unmap command is synchronous** where the user must wait for the completion before
> continuing.  

  
> Each SDC holds mapping information that is light-weight and efficient so it can be
> stored in real memory. For every 8 PB of storage, the SDC requires roughly 2 MB RAM.
> Mapping information may change without the client being notified, this is the nature
> of a lazy or loosely-coupled approach.


# IO Flow

> IOs from the application are serviced by the SDC that runs on the same server
> as the application. The SDC fulfills the IO request regardless of where any
> particular block physically resides.  

  
> When the IO is a Write, the SDC sends the IO to the SDS where the Primary copy is
> located. The Primary SDS will send the IO to its local drive and in parallel, the IO
> is sent to the secondary mirror located on a secondary SDS. **Only after an
> acknowledgment is received from the secondary SDS, the primary SDS will acknowledge
> the IO to the SDC.**  

   
> A Read IO from the application will trigger the SDC to issue the IO to the SDS with
> the Primary chunk.  

  
> In terms of resources consumed, one host **Write** IO will generate **two IOs** over
> both the network and back-end drives. A **read** will generate **one** network IO and
> **one** back-end IO to the drives. For example, if the application is issuing an 8 KB
> Write, the network and drives will get 2x8 KB IOs. For an 8 KB Read, there will be
> only one 8 KB IO on the network and drives.  

   
> Note: The IO flow does not require any MDM or any other central management point. For
> this reason, ScaleIO is able to scale linearly in terms of performance. *Every SDC knows
> how to direct an IO operation to the destination SDS.* There is no flooding or
> broadcasting. This is extremely efficient parallelism that eliminates single points
> of failure. Since there is no central point of routing, all of this happens in a
> distributed manner.  

  
> The SDC has all the intelligence needed to route every request, preventing unnecessary
> network traffic and redundant SDS resource usage

# Read Cache

## introduction

> Cache is a critical aspect of storage performance. ScaleIO uses server DRAM for
> **Read RAM Cache (RMcache)** as well as **SSD/Flash devices (RFcache)** for caching
> reads. ScaleIO cache uses recently-accessed (LRU) data readily available to manage
> caching. IOs read from cache have a lower response time than IOs serviced by the
> drives.  

  
> Another benefit of caching IO is that it reduces the data drive workload which
> in many cases is a performance bottleneck in the system.  

  
> **Cache in ScaleIO is managed by the SDS**. It is a simple and clean implementation
> that does not require cache coherency management, which would have been required
> if cache were managed by the SDC.  

## Read RAM Cache

> Read RAM Cache, or RMcache, is an SDS feature that uses server DRAM memory
> for read caching.

> Read cache characteristics:
> - No caching of rebuild and rebalance IOs (because there is no chance these
>   IOs will be reused)
> - Writes are buffered for Read after Write IOs
>     - Default = ON
> - Unaligned writes are not buffered
> - The cache page size is 4 KB
>     -- If IO is smaller than 4 KB or not aligned to 4 KB, the back-end IO
>     will be aligned to 4 KB. This is a type of pre-fetch mechanism.
> - Cache size (per SDS)
>     -- Default = 128 MB
>     -- Cache can be resized dynamically and disabled on the fly.
>     -- Max Cache Size = 300 GB
> - Cache can be defined for each Storage Pool
> - Cache can be defined per Volume
> - Max IO size cached: 128 KB

> Cache is managed by two main data structures:
> - User Data (UD): This is a copy of the disk data residing in Cache
>     -- pre-allocated in one continuous buffer
>     -- divided into 128 KB blocks
>     -- managed using an efficient Least Recently Used (LRU) algorithm
> - Meta Data (MD): Contains pointers to addresses in the UD
>     -- MD uses Hash with two inputs (keys): physical LBA, Device number


## Read Flash Cache


## Write buffering

> Write buffering has many advantages. One benefit is a reduction in Write
> Response Time which is much lower when the IO is acknowledged from a **write
> buffering DRAM/Flash device**, rather than a HDD drive. An added benefit of
> buffering writes is that it utilizes the “elevator reordering”. This is
> sometimes referred to as Command Tag Queuing(CTQ). Elevator reordering
> minimizes the seek on the drive and increases the max IOPs of an HDD drive,
> and even reduces the drive’s load because of rewrites to the same address
> locations.

Write buffering can be achieved in various ways:

- RMCache and RFCache
- Raid controllers (e.g. LSI/Broadcom, PMC etc.) that have write buffering
- using SSD/Flash drives configured in the systems
- NVDIMMs




[1]: https://cdn2.hubspot.net/hubfs/2907348/Whitepapers/EMC/h14344-emc-scaleio-basic-architecture.pdf

