---
title: io-uring notes
category: tech
tags: [linux kernel, io_uring, IO]
---

* TOC
{:toc}


io-uring notes

# **io-uring uses**

## IORING_SETUP_DEFER_TASKRUN

### motivation

- [initial patchset](https://lore.kernel.org/io-uring/20220815130911.988014-1-dylany@fb.com/)

- defer async work until user space calls io_uring_enter with the GETEVENTS flag

- [avoid to overflow CQ queue](https://lore.kernel.org/io-uring/ab2d2f5c-0e76-44a2-8a7e-6f9edcfa5a92@gmail.com/)

- improve network workload perf

### usage

- require IORING_SETUP_SINGLE_ISSUER

For DEFER_TASKRUN we require the completion task to be the same as the

submission task. This implies that there is only one submitter, so enforce that.


## IOSQE_IO_LINK vs. IOSQE_IO_HARDLINK

```
liburing man page

IOSQE_IO_LINK
       When this flag is specified, the SQE forms a link with the next SQE in  the  submission  ring.  That
       next  SQE  will not be started before the previous request completes. This, in effect, forms a chain
       of SQEs, which can be arbitrarily long. The tail of the chain is denoted by the first SQE that  does
       not  have this flag set. Chains are not supported across submission boundaries. Even if the last SQE
       in a submission has this flag set, it will still terminate the current chain. This flag has  no  ef‐
       fect  on  previous SQE submissions, nor does it impact SQEs that are outside of the chain tail. This
       means that multiple chains can be executing in parallel, or chains and individual SQEs. Only members
       inside the chain are serialized. A chain of SQEs will be broken, if any request in that  chain  ends
       in  error. io_uring considers any unexpected result an error. This means that, eg, a short read will
       also terminate the remainder of the chain.  If a chain of SQE links is  broken,  the  remaining  un‐
       started part of the chain will be terminated and completed with -ECANCELED as the error code. Avail‐
       able since 5.3.

IOSQE_IO_HARDLINK
       Like  IOSQE_IO_LINK,  but  it doesn't sever regardless of the completion result.  Note that the link
       will still sever if we fail submitting the parent request, hard links  are  only  resilient  in  the
       presence  of  completion  results  for requests that did submit correctly. IOSQE_IO_HARDLINK implies
       IOSQE_IO_LINK.  Available since 5.5.
```

**The main difference is on how to advance the chain in case of parent request failure.**



# **io-uring internal**

## io_kiocb double completion & link advancing in case of io-wq

1) request may be complete via task_work: 1st completetion

>
> ```
> static void io_req_complete_post(struct io_kiocb *req, unsigned issue_flags)
>     ...
> 	/*
> 	 * Handle special CQ sync cases via task_work. DEFER_TASKRUN requires
> 	 * the submitter task context, IOPOLL protects with uring_lock.
> 	 */
> 	if (ctx->task_complete || (ctx->flags & IORING_SETUP_IOPOLL)) {
> 		req->io_task_work.func = io_req_task_complete;
> 		io_req_task_work_add(req);
> 		return;
> 	}
>     ...
> ```

2) request completed via io_free_req(): final completion

> ```
> struct io_wq_work *io_wq_free_work(struct io_wq_work *work)
> {
> 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
> 	struct io_kiocb *nxt = NULL;
> 
> 	if (req_ref_put_and_test(req)) {
> 		if (req->flags & IO_REQ_LINK_FLAGS)
> 			nxt = io_req_find_next(req);
> 		io_free_req(req);
> 	}
> 	return nxt ? &nxt->work : NULL;
> }
> ```

Then the result is that the same request appears two times in `__io_submit_flush_completions`

BTW, for io-wq, link advancing is done in `io_wq_free_work()` or `io_queue_next()` from
`io_free_batch_list`, and it depends on where the refcount drops to zero in the
two code paths.

## understand rsrc_node(v6.13)

- cover registered file & fixed buffer

resource lifetime management(buffer register/unregister vs buffer use)


## understand io_uring buffers

### fixed buffer

- imported in ->prep()

- may be delayed to ->issue()

- managed by rsrc_node

- why can't be imported more than once in `io_uring/rw.c`?

### plain userspace buffer

- call io_import_iovec() directly in ->prep()

- why does vectored buffer have to be imported in the submission context?
-EFAULT is observed when importing uiov in io-wq context. But not observed
in case of io_uring_prep_read().

    EFAULT is returned from copy_iovec_from_user() <- iovec_from_user <-
    __import_iovec, and **copy_from_user()** can be only done in the submission
    conext

### selected buffer

- io_import_iovec() directly, called in ->issue() only

- how to cover sync between providing buffer & buffer use, buffer group

- rsrc_node isn't used


### io_read() & io_write buffer use


- IORING_RECVSEND_FIXED_BUF

```
io_send_zc_prep
    if (req->opcode != IORING_OP_SEND_ZC) {
        if (unlikely(zc->flags & IORING_RECVSEND_FIXED_BUF))
            return -EINVAL;
    }

io_send_zc_import
    if (sr->flags & IORING_RECVSEND_FIXED_BUF) {
        sr->notif->buf_index = req->buf_index;
        ret = io_import_reg_buf(sr->notif, &kmsg->msg.msg_iter,
                                (u64)(uintptr_t)sr->buf, sr->len,
                                ITER_SOURCE, issue_flags);
        if (unlikely(ret))
            return ret;
        kmsg->msg.sg_from_iter = io_sg_from_iter;
    }
```

- is it possible to use IORING_RECVSEND_FIXED_BUF for recv?



### net io buffer use




## Add support for vectored registered buffers

[https://lore.kernel.org/all/cover.1741014186.git.asml.silence@gmail.com/#r](https://lore.kernel.org/all/cover.1741014186.git.asml.silence@gmail.com/#r)


### userspace buffer vs. kernel bvec buffer


- userspace buffer

each bvec size is uniform, with fixed size of `1 << imu->folio_shift`

[io_sqe_buffer_register](https://web.git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/io_uring/rsrc.c?h=v6.14-rc7#n715)


- kernel bvec buffer

each bvec size is not uniform

### how to support vectored fixed kernel buffer

- io_estimate_bvec_size()

- io_vec_fill_bvec()

use for_each_mp_bvec() to implement

### interfaces

#### io_prep_reg_iovec


#### io_import_reg_vec

- userspace vec is stored at the last half of iou_vec->vec 

#### io_vec_fill_bvec


# **how to support kernel buffer**

## motivation

- device zero copy

buffer is from kernel, and need to lease to io_uring for reading data to
the buffer, or writing data in the buffer.

It is like traditional copy_file_range() or splice() which can't support
to read data to the pages in pipe, also buffer ownership model isn't same.

[io_uring: support leased group buffer with REQ_F_GROUP_KBUF](https://lore.kernel.org/linux-block/20241025122247.3709133-6-ming.lei@redhat.com/)

> ```
> This way looks a bit similar with kernel's pipe/splice, but there are some
> important differences:
> 
> - splice is for transferring data between two FDs via pipe, and fd_out can
> only read data from pipe, but data can't be written to; this feature can
> lease buffer from group leader(driver subsystem) to members(io_uring subsystem),
> so member request can write data to this buffer if the buffer direction is
> allowed to write to.
> 
> - splice implements data transfer by moving pages between subsystem and
> pipe, that means page ownership is transferred, and this way is one of the
> most complicated thing of splice; this patch supports scenarios in which
> the buffer can't be transferred, and buffer is only borrowed to member
> requests for consumption, and is returned back after member requests
> consume the leased buffer, so buffer lifetime is aligned with group leader
> lifetime, and buffer lifetime is simplified a lot. Especially the buffer
> is guaranteed to be returned back.
> 
> - splice can't run in async way basically
> ```

Also it has to be careful for dealing with reading data to the kernel buffer.
If the kernel buffer may be copied to userspace, the application has to be
trusted, that said the application has to read data to the buffer, otherwise
kernel data may be leaked to userspace.

## design

### does it need to cover userspace buffer?

- such as, the buffer is originated from userspace, and it is provided
from one bpf OP(group leader)

- technically it is pretty easy to cover it

- at least it can be used for test purpose

### how to make SQE group work with fixed kernel buffer?

- when to remove the registered buffer node?

  when the group leader is completed, one request flag of unregister
  buffer is used, buffer index is required

- is it possible to not introduce buffer index concept?

- local buffer

mark this buffer as **local buffer**, and local buffer is un-registered
automatically when its reference drops to zero

local buffer lifetime is same with sqe group's lifetime, or the buffer
table is local? auto-unregistered.

group leader owns the local buffer, only group member can use this local
buffer.

- buffer table

Any buffer is registered to the table, it becomes global visible for
everyone, it looks one big trouble now.


## questions

### is faulted page for READ initialized as 0 before reading file data?

# issues

## 'kernel NULL pointer dereference' on percpu_ref_get_many+0x23/0x30

```
[  103.067047] RIP: 0010:percpu_ref_get_many+0x23/0x30
...
[  103.073111]  io_fallback_req_func+0x3d/0xaf
[  103.073422]  process_one_work+0x188/0x340
```

- exact exception IP

```
(gdb) l *(percpu_ref_get_many+0x23)
0xffffffff81a96d23 is in percpu_ref_get_many (./arch/x86/include/asm/atomic64_64.h:25).
20		__WRITE_ONCE(v->counter, i);
21	}
```

Looks this percpu data is freed.

### test description

- 'make test T=generic/002'

- can be triggered with get_data or without get_data


### use-after-free on 'io_uring_cmd' issue

- one usual problem

- posisoning io->cmd may figure out the reason


# **idea: buffer abstraction & generic buffer**

## use cases 
- one typical use case is for supporting io_uring bpf

io_uring exports kfunc for bpf prog, and buffer consumer is done as plugin,
which need to decouple OP implementation from buffer use

## buffer operations

- setup

- teardown

- import

- recycle(?)

