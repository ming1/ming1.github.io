---
title: io-uring notes
category: tech
tags: [linux kernel, io_uring, IO]
---

* TOC
{:toc}


io-uring notes

# **io-uring uses**


## IOPOLL

- man page

```
IORING_SETUP_IOPOLL
       Perform  busy-waiting  for  an  I/O  completion, as opposed to getting notifications via an asynchronous IRQ (Interrupt Re‐
       quest). The file system (if any) and block device must support polling in order for this  to  work.  Busy-waiting  provides
       lower  latency,  but  may consume more CPU resources than interrupt driven I/O. Currently, this feature is usable only on a
       file descriptor opened using the O_DIRECT flag. When a read or write is submitted to a polled context, the application must
       poll for completions on the CQ ring by calling io_uring_enter(2).  It is illegal to mix and match polled and non-polled I/O
       on an io_uring instance.

       This is only applicable for storage devices for now, and the storage device must be configured for polling. How to do  that
       depends on the device type in question. For NVMe devices, the nvme driver must be loaded with the poll_queues parameter set
       to the desired number of polling queues. The polling queues will be shared appropriately between the CPUs in the system, if
       the number is less than the number of online CPU threads.

IORING_SETUP_HYBRID_IOPOLL
       This  flag  must  be  used  with  IORING_SETUP_IOPOLL flag. Hybrid io polling is a feature based on iopoll, it differs from
       strict polling in that it will delay a bit before doing completion side polling, to avoid wasting too much  CPU  resources.
       Like IOPOLL , it requires that devices support polling.

IORING_SETUP_SQPOLL
       When  this  flag is specified, a kernel thread is created to perform submission queue polling. An io_uring instance config‐
       ured in this way enables an application to issue I/O without ever context switching into the kernel. By using  the  submis‐
       sion  queue  to  fill in new submission queue entries and watching for completions on the completion queue, the application
       can submit and reap I/Os without doing a single system call.

       If the kernel thread is idle for more than sq_thread_idle milliseconds, it will set the IORING_SQ_NEED_WAKEUP  bit  in  the
       flags  field  of  the struct io_sq_ring.  When this happens, the application must call io_uring_enter(2) to wake the kernel
       thread. If I/O is kept busy, the kernel thread will never sleep. An application making use of this  feature  will  need  to
       guard the io_uring_enter(2) call with the following code sequence:

           /*
            * Ensure that the wakeup flag is read after the tail pointer
            * has been written. It's important to use memory load acquire
            * semantics for the flags read, as otherwise the application
            * and the kernel might not agree on the consistency of the
            * wakeup flag.
            */
           unsigned flags = atomic_load_relaxed(sq_ring->flags);
           if (flags & IORING_SQ_NEED_WAKEUP)
               io_uring_enter(fd, 0, 0, IORING_ENTER_SQ_WAKEUP);

       where sq_ring is a submission queue ring setup using the struct io_sqring_offsets described below.
```

```
man io_uring_enter()

       If  the  io_uring  instance  was  configured for polling, by specifying IORING_SETUP_IOPOLL in the call to io_uring_setup(2), then
       min_complete has a slightly different meaning.  Passing a value of 0 instructs the kernel to return any events which  are  already
       complete,  without  blocking.   If  min_complete  is  a non-zero value, the kernel will still return immediately if any completion
       events are available.  If no event completions are available, then the call will poll either until one or more completions  become
       available, or until the process has exceeded its scheduler time slice.

       Note that, for interrupt driven I/O (where IORING_SETUP_IOPOLL was not specified in the call to io_uring_setup(2)), an application
       may check the completion queue for event completions without entering the kernel at all.

```


## IORING_REGISTER_PBUF_RING

- man page

```
Registers a shared buffer ring to be used with provided buffers. This  is  a  newer  alternative
to  using  IORING_OP_PROVIDE_BUFFERS which is more efficient, to be used with request types
that support the IOSQE_BUFFER_SELECT flag.

The arg argument must be filled in with the appropriate information. It looks as follows:

     struct io_uring_buf_reg {
         __u64 ring_addr;
         __u32 ring_entries;
         __u16 bgid;
         __u16 pad;
         __u64 resv[3];
     };

The  ring_addr  field  must contain the address to the memory allocated to fit this ring.  The memory must be page aligned
and hence allocated appropriately using eg posix_memalign(3) or similar. The size of the ring is the product  of  ring_en‐
tries  and  the  size  of  struct io_uring_buf.  ring_entries is the desired size of the ring, and must be a power-of-2 in
size. The maximum size allowed is 2^15 (32768).  bgid is the buffer group ID associated with this ring. SQEs that select a
buffer have a buffer group associated with them in  their  buf_group  field,  and  the  associated  CQEs  will  have  IOR‐
ING_CQE_F_BUFFER  set  in  their flags member, which will also contain the specific ID of the buffer selected. The rest of
the fields are reserved and must be cleared to zero.

nr_args must be set to 1.

Also see io_uring_register_buf_ring(3) for more details. Available since 5.19.
```

```
/*
 * Flags for IORING_REGISTER_PBUF_RING.
 *
 * IOU_PBUF_RING_MMAP:  If set, kernel will allocate the memory for the ring.
 *                      The application must not set a ring_addr in struct
 *                      io_uring_buf_reg, instead it must subsequently call
 *                      mmap(2) with the offset set as:
 *                      IORING_OFF_PBUF_RING | (bgid << IORING_OFF_PBUF_SHIFT)
 *                      to get a virtual mapping for the ring.
 * IOU_PBUF_RING_INC:   If set, buffers consumed from this buffer ring can be
 *                      consumed incrementally. Normally one (or more) buffers
 *                      are fully consumed. With incremental consumptions, it's
 *                      feasible to register big ranges of buffers, and each
 *                      use of it will consume only as much as it needs. This
 *                      requires that both the kernel and application keep
 *                      track of where the current read/recv index is at.
 */
```

- tokio discussion

[Use of IORING_REGISTER_PBUF_RING, introduced in kernel 5.19](https://github.com/tokio-rs/tokio-uring/issues/112)


## timeout multishot

```
       void io_uring_prep_timeout(struct io_uring_sqe *sqe,
                                  struct __kernel_timespec *ts,
                                  unsigned count,
                                  unsigned flags);

       IORING_TIMEOUT_MULTISHOT
              The  request  will return multiple timeout completions. The completion flag
              IORING_CQE_F_MORE is set if more timeouts are expected. The value specified
              in count is the number of repeats. A value of 0 means the timeout is indefinite
              and can only be stopped by a removal request. Available since the 6.4 kernel.

```


## IORING_SETUP_DEFER_TASKRUN

### motivation

- [initial patchset](https://lore.kernel.org/io-uring/20220815130911.988014-1-dylany@fb.com/)

- defer async work until user space calls io_uring_enter with the GETEVENTS flag

- [avoid to overflow CQ queue](https://lore.kernel.org/io-uring/ab2d2f5c-0e76-44a2-8a7e-6f9edcfa5a92@gmail.com/)

- improve network workload perf

### usage

- require IORING_SETUP_SINGLE_ISSUER

For DEFER_TASKRUN we require the completion task to be the same as the

submission task. This implies that there is only one submitter, so enforce that.


## IORING_SETUP_COOP_TASKRUN

```
IORING_SETUP_COOP_TASKRUN
       By default, io_uring will interrupt a task running in userspace when a completion event comes in. This  is  to  ensure
       that  completions  run  in a timely manner. For a lot of use cases, this is overkill and can cause reduced performance
       from both the inter-processor interrupt used to do this, the kernel/user transition, the needless interruption of  the
       tasks  userspace activities, and reduced batching if completions come in at a rapid rate. Most applications don't need
       the forceful interruption, as the events are processed at any kernel/user transition. The exception are  setups  where
       the  application  uses multiple threads operating on the same ring, where the application waiting on completions isn't
       the one that submitted them. For most other use cases, setting this flag will  improve  performance.  Available  since
       5.19.

IORING_SETUP_TASKRUN_FLAG
       Used  in  conjunction  with IORING_SETUP_COOP_TASKRUN, this provides a flag, IORING_SQ_TASKRUN, which is set in the SQ
       ring flags whenever completions are pending that should be processed. liburing will check for this flag even when  do‐
       ing  io_uring_peek_cqe(3)  and  enter  the  kernel  to process them, and applications can do the same. This makes IOR‐
       ING_SETUP_TASKRUN_FLAG safe to use even when applications rely on a peek style operation on the CQ ring to see if any‐
       thing might be pending to reap. Available since 5.19.
```

## Registered eventfd


## Read multishot

The  io_uring_prep_read_multishot(3) helper prepares an IO read multishot request. The submission queue
entry sqe is setup to use the file descriptor fd to start reading into a  buffer  from  the  provided
buffer group with ID buf_group at the specified offset.

nbytes must be set to zero, as the size read will be given by the size of the buffers in the indicated
buffer group IO.

On files that are not capable of seeking, the offset must be 0 or -1.

If nbytes exceeds the size of the buffers in the specified buffer group, or if nbytes is 0 , then the
size of the buffer in that group will be used for the transfer.

A multishot read request will repeatedly trigger a completion event whenever data is available to read
from the file. Because of that, this type of request can only be used with a file type that is pollable.
Examples of that include pipes, tun devices, etc. If used with a regular file, or a wrong file type in
general, the request will fail with -EBADFD in the CQE res field.

Since multishot requests repeatedly trigger completion events as data arrives, it must be used  with
provided  buffers.  With  provided buffers, the application provides buffers to io_uring upfront, and
then the kernel picks a buffer from the specified group in buf_group when the request is ready to
transfer data.

A multishot request will persist as long as no errors are encountered doing handling of  the  request.
For each CQE posted on behalf of this request, the CQE flags will have IORING_CQE_F_MORE set if the
application should expect more completions from this request.  If this flag isn't set, then that
signifies termination of the multishot read request.

## io_uring_buf_ring_add

The  io_uring_buf_ring_add(3)  adds a new buffer to the shared buffer ring br.  The buffer address is
indicated by addr and is of len bytes of length.  bid is the buffer ID, which will  be  returned  in  the
CQE. mask  is  the size mask of the ring, available from io_uring_buf_ring_mask(3).  buf_offset is the
offset to insert at from the current tail. If just one buffer is provided before the  ring  tail  is
committed  with io_uring_buf_ring_advance(3)  or  io_uring_buf_ring_cq_advance(3),  then buf_offset should
be 0. If buffers are provided in a loop before being committed, the buf_offset must be incremented by one
for  each  buffer added.

```
IOURINGINLINE void io_uring_buf_ring_add(struct io_uring_buf_ring *br,
                                         void *addr, unsigned int len,
                                         unsigned short bid, int mask,
                                         int buf_offset)
{
        struct io_uring_buf *buf = &br->bufs[(br->tail + buf_offset) & mask];

        buf->addr = (unsigned long) (uintptr_t) addr;
        buf->len = len;
        buf->bid = bid;
}

/*
 * Make 'count' new buffers visible to the kernel. Called after
 * io_uring_buf_ring_add() has been called 'count' times to fill in new
 * buffers.
 */
IOURINGINLINE void io_uring_buf_ring_advance(struct io_uring_buf_ring *br,
                                             int count)
{
        unsigned short new_tail = br->tail + count;

        io_uring_smp_store_release(&br->tail, new_tail);
}
```




## IOSQE_IO_LINK vs. IOSQE_IO_HARDLINK

```
liburing man page

IOSQE_IO_LINK
       When this flag is specified, the SQE forms a link with the next SQE in  the  submission  ring.  That
       next  SQE  will not be started before the previous request completes. This, in effect, forms a chain
       of SQEs, which can be arbitrarily long. The tail of the chain is denoted by the first SQE that  does
       not  have this flag set. Chains are not supported across submission boundaries. Even if the last SQE
       in a submission has this flag set, it will still terminate the current chain. This flag has  no  ef‐
       fect  on  previous SQE submissions, nor does it impact SQEs that are outside of the chain tail. This
       means that multiple chains can be executing in parallel, or chains and individual SQEs. Only members
       inside the chain are serialized. A chain of SQEs will be broken, if any request in that  chain  ends
       in  error. io_uring considers any unexpected result an error. This means that, eg, a short read will
       also terminate the remainder of the chain.  If a chain of SQE links is  broken,  the  remaining  un‐
       started part of the chain will be terminated and completed with -ECANCELED as the error code. Avail‐
       able since 5.3.

IOSQE_IO_HARDLINK
       Like  IOSQE_IO_LINK,  but  it doesn't sever regardless of the completion result.  Note that the link
       will still sever if we fail submitting the parent request, hard links  are  only  resilient  in  the
       presence  of  completion  results  for requests that did submit correctly. IOSQE_IO_HARDLINK implies
       IOSQE_IO_LINK.  Available since 5.5.
```

**The main difference is on how to advance the chain in case of parent request failure.**


## Io_uring resize

### background

[[PATCHSET RFC 0/3] Add support for ring resizing](https://lore.kernel.org/io-uring/20241022021159.820925-1-axboe@kernel.dk/)

[Resizing io_uring SQ/CQ?](https://lore.kernel.org/io-uring/20230309134808.GA374376@fedora/)


## Registered io_uring

```
/*
 * Register the ring fd in itself for use with
 * IORING_REGISTER_USE_REGISTERED_RING; return a registered fd index rather
 * than an fd.
 */
#define IORING_SETUP_REGISTERED_FD_ONLY (1U << 15)
```



# **Io-uring internal**

## Cancel code path


### data structure


#### io_ring_ctx

```
struct io_ring_ctx {
	...
	/* ctx exit and cancelation */
    struct llist_head               fallback_llist;
    struct delayed_work             fallback_work;
    struct work_struct              exit_work;
    struct list_head                tctx_list;
    struct completion               ref_comp;
	...
}
```

#### struct io_uring_task

```
struct io_uring_task {
	/* submission side */
	int				cached_refs;
	const struct io_ring_ctx 	*last;
	struct io_wq			*io_wq;
	struct file			*registered_rings[IO_RINGFD_REG_MAX];

	struct xarray			xa;					//save each ctx
	struct wait_queue_head		wait;
	atomic_t			in_cancel;
	atomic_t			inflight_tracked;
	struct percpu_counter		inflight;

	struct { /* task_work */
		struct llist_head	task_list;
		struct callback_head	task_work;
	} ____cacheline_aligned_in_smp;
};
```

note:

1) one task may include multiple io_uring ctx, one same io_uring ctx
could be used in multiple tasks(pthreads)

2) per-task IO tracking

- any `io_ring_ctx` is used in this task context, it will be tracked in this
`io_uring_task`

3) task_work:  ???

4) current->io_uring points to this structure

5) abstraction in task_struct side for io_uring, good point

	
#### struct io_tctx_node

```
struct io_tctx_node {
	struct list_head	ctx_node;
	struct task_struct	*task;
	struct io_ring_ctx	*ctx;
};
```

note:

1) each io_tctx_node is linked to ctx->tctx_list

2) for !SQD, each io_tctx_node is stored in tctx->xa too, see
io_uring_cancel_generic()

3) per-(task, io_uring ctx) pair

4) added to ctx->tctx_list in __io_uring_add_tctx_node()


#### io_kiocb

```
struct io_kiocb {
	...
	struct io_ring_ctx		*ctx;
	struct task_struct		*task;
	...
}
```

note:

```
1) req->ctx

req->ctx = ctx
	io_preinit_req
		__io_alloc_req_refill
			io_alloc_req
				io_submit_sqes
				io_alloc_notif

2) req->task
req->task = current
	io_init_req
		io_submit_sqe
			io_submit_sqes
```

### interfaces


#### io_ring_exit_work

```
note:

1) caller:

INIT_WORK(&ctx->exit_work, io_ring_exit_work);
queue_work(system_unbound_wq, &ctx->exit_work);
	io_ring_ctx_wait_and_kill
		io_uring_release
			.release        = io_uring_release
				io_uring_fops
		io_uring_create		//failure path

2)
```


#### io_uring_alloc_task_context

```
__cold int io_uring_alloc_task_context(struct task_struct *task,
				       struct io_ring_ctx *ctx)
note:
1) caller:
__io_uring_add_tctx_node
	io_uring_install_fd
		io_uring_create
	__io_uring_add_tctx_node_from_submit
	io_ringfd_register
io_sq_offload_create
	io_uring_create

2) allocate tctx
	tctx = kzalloc(sizeof(*tctx), GFP_KERNEL);
	if (unlikely(!tctx))
		return -ENOMEM;

3) init ->inflight
	ret = percpu_counter_init(&tctx->inflight, 0, GFP_KERNEL);
	if (unlikely(ret)) {
		kfree(tctx);
		return ret;
	}

4) init & alloc io wq
	tctx->io_wq = io_init_wq_offload(ctx, task);
	if (IS_ERR(tctx->io_wq)) {
		ret = PTR_ERR(tctx->io_wq);
		percpu_counter_destroy(&tctx->inflight);
		kfree(tctx);
		return ret;
	}

5) post initialization
	xa_init(&tctx->xa);
	init_waitqueue_head(&tctx->wait);
	atomic_set(&tctx->in_cancel, 0);
	atomic_set(&tctx->inflight_tracked, 0);
	task->io_uring = tctx;
	init_llist_head(&tctx->task_list);
	init_task_work(&tctx->task_work, tctx_task_work);
```


#### __io_uring_add_tctx_node

```
// called when this ctx is used in current task
int __io_uring_add_tctx_node(struct io_ring_ctx *ctx)
note:
0) callers
	io_uring_create
	__io_uring_add_tctx_node_from_submit
		io_uring_add_tctx_node
			io_uring_enter
	io_ringfd_register

	/*
	 * Register a ring fd to avoid fdget/fdput for each io_uring_enter()
	 * invocation. User passes in an array of struct io_uring_rsrc_update
	 * with ->data set to the ring_fd, and ->offset given for the desired
	 * index. If no index is desired, application may set ->offset == -1U
	 * and we'll find an available index. Returns number of entries
	 * successfully processed, or < 0 on error if none were processed.
	 */
	int io_ringfd_register(struct io_ring_ctx *ctx, void __user *__arg,
			       unsigned nr_args)

1) allocate tctx for current->io_uring if it isn't allocated yet
	struct io_uring_task *tctx = current->io_uring;
	struct io_tctx_node *node;
	int ret;

	if (unlikely(!tctx)) {
		ret = io_uring_alloc_task_context(current, ctx);
		if (unlikely(ret))
			return ret;

		tctx = current->io_uring;
		if (ctx->iowq_limits_set) {
			unsigned int limits[2] = { ctx->iowq_limits[0],
						   ctx->iowq_limits[1], };

			ret = io_wq_max_workers(tctx->io_wq, limits);
			if (ret)
				return ret;
		}
	}

2) install ctx to tctx->xa & alloc/add ctx_node into ctx->tctx_list
	if (!xa_load(&tctx->xa, (unsigned long)ctx)) {
		node = kmalloc(sizeof(*node), GFP_KERNEL);
		if (!node)
			return -ENOMEM;
		node->ctx = ctx;
		node->task = current;

		ret = xa_err(xa_store(&tctx->xa, (unsigned long)ctx,
					node, GFP_KERNEL));
		if (ret) {
			kfree(node);
			return ret;
		}

		mutex_lock(&ctx->uring_lock);
		list_add(&node->ctx_node, &ctx->tctx_list);
		mutex_unlock(&ctx->uring_lock);
	}
	return 0;

- one ctx could be run from multiple task too, so io_tctx_node should
be per-(ctx, task) pair
```

##### __io_uring_add_tctx_node_from_submit

```
note:
1) caller:
__io_uring_add_tctx_node_from_submit
	io_uring_add_tctx_node
		io_uring_enter 

2) called when new ctx is used in current task

io_uring_add_tctx_node
static inline int io_uring_add_tctx_node(struct io_ring_ctx *ctx)
{
	struct io_uring_task *tctx = current->io_uring;

	if (likely(tctx && tctx->last == ctx))
		return 0;

	return __io_uring_add_tctx_node_from_submit(ctx);
}

3) 
__io_uring_add_tctx_node_from_submit
	if (ctx->flags & IORING_SETUP_SINGLE_ISSUER
	    && ctx->submitter_task != current)
		return -EEXIST;

	ret = __io_uring_add_tctx_node(ctx);
	if (ret)
		return ret;

	current->io_uring->last = ctx;
```

#### __io_uring_free

```
static inline void io_uring_free(struct task_struct *tsk)
{
	if (tsk->io_uring)
		__io_uring_free(tsk);
}
void __io_uring_free(struct task_struct *tsk)
note:
1) caller
io_uring_free
	__put_task_struct
		put_task_struct
		put_task_struct_many
io_uring_cancel_generic
	io_sq_thread
	__io_uring_cancel
		io_uring_files_cancel
			do_exit
		io_uring_task_cancel
			begin_new_exec
```

#### io_uring_files_cancel

```
static inline void io_uring_files_cancel(void)
{
	if (current->io_uring) {
		io_uring_unreg_ringfd();
		__io_uring_cancel(false);
	}
}
```

#### io_uring_task_cancel 

```
static inline void io_uring_task_cancel(void)
{
	if (current->io_uring)
		__io_uring_cancel(true);
}
```

#### __io_uring_cancel

```
void __io_uring_cancel(bool cancel_all)
{
	io_uring_cancel_generic(cancel_all, NULL);
}
note:
1) caller:
io_uring_files_cancel
	io_uring_files_cancel
		do_exit
	io_uring_task_cancel
		begin_new_exec
```

#### io_uring_cancel_generic

```
/*
 * Find any io_uring ctx that this task has registered or done IO on, and cancel
 * requests. @sqd should be not-null IFF it's an SQPOLL thread cancellation.
 */
__cold void io_uring_cancel_generic(bool cancel_all, struct io_sq_data *sqd)
note:
1) background:

0f2122045b94 io_uring: don't rely on weak ->files references

// make the difference
9936c7c2bc76 io_uring: deduplicate core cancellations sequence

2) 
		/* read completions before cancelations */
		inflight = tctx_inflight(tctx, !cancel_all);
		if (!inflight)
			break;
static s64 tctx_inflight(struct io_uring_task *tctx, bool tracked)
{
	if (tracked)
		return atomic_read(&tctx->inflight_tracked);
	return percpu_counter_sum(&tctx->inflight);
}

io_req_track_inflight
	io_file_get_normal
		io_req_prep_async
			io_queue_sqe_fallback>>
				io_submit_fail_init
				io_submit_sqe
			io_submit_sqe>>
			io_resubmit_prep>>
				kiocb_done
		io_assign_file
			io_issue_sqe
			io_wq_submit_work
```

#### io_cancel_ctx_cb
note:

```
1) caller
io_wq_cancel_cb(tsk->io_uring->io_wq, io_cancel_ctx_cb, ctx, true);
	io_ring_exit_work

cret = io_wq_cancel_cb(tctx->io_wq, io_cancel_ctx_cb, ctx, true);
	io_uring_try_cancel_iowq
		io_uring_try_cancel_requests
			io_ring_exit_work
			io_uring_cancel_generic
```

#### io_fallback_req_func

```
static __cold void io_fallback_req_func(struct work_struct *work)
{
	struct io_ring_ctx *ctx = container_of(work, struct io_ring_ctx,
						fallback_work.work);
	struct llist_node *node = llist_del_all(&ctx->fallback_llist);
	struct io_kiocb *req, *tmp;
	bool locked = true;

	mutex_lock(&ctx->uring_lock);
	llist_for_each_entry_safe(req, tmp, node, io_task_work.node)
		req->io_task_work.func(req, &locked);
	if (WARN_ON_ONCE(!locked))
		return;
	io_submit_flush_completions(ctx);
	mutex_unlock(&ctx->uring_lock);
}
```

#### io_fallback_tw

```
static __cold void io_fallback_tw(struct io_uring_task *tctx)
{
	struct llist_node *node = llist_del_all(&tctx->task_list);
	struct io_kiocb *req;

	while (node) {
		req = container_of(node, struct io_kiocb, io_task_work.node);
		node = node->next;
		if (llist_add(&req->io_task_work.node,
			      &req->ctx->fallback_llist))
			schedule_delayed_work(&req->ctx->fallback_work, 1);
	}
}
```

#### io_uring_drop_tctx_refs

```
note:
1) caller:
tctx_task_work>>
io_uring_cancel_generic>>

tctx_task_work>>
	/* relaxed read is enough as only the task itself sets ->in_cancel */
	if (unlikely(atomic_read(&tctx->in_cancel)))
		io_uring_drop_tctx_refs(current);

2) 
static __cold void io_uring_drop_tctx_refs(struct task_struct *task)
{
	struct io_uring_task *tctx = task->io_uring;
	unsigned int refs = tctx->cached_refs;

	if (refs) {
		tctx->cached_refs = 0;
		percpu_counter_sub(&tctx->inflight, refs);
		put_task_struct_many(task, refs);
	}
}
```

#### io_uring_try_cancel_requests

```
static __cold bool io_uring_try_cancel_requests(struct io_ring_ctx *ctx,
						struct task_struct *task,
						bool cancel_all)
note:
1) caller:
io_ring_exit_work
io_uring_cancel_generic
```

#### io_wq_submit_work

```
note:
1) is REQ_F_NOWAIT set for 't/io_uring /dev/nullb0'?
```

#### tctx_inflight

```
note:
1) caller:
io_uring_cancel_generic

- see story in 0f2122045b94 io_uring: don't rely on weak ->files references
- for tracking current->files reference

2) 
static s64 tctx_inflight(struct io_uring_task *tctx, bool tracked)
{
	if (tracked)
		return atomic_read(&tctx->inflight_tracked);
	return percpu_counter_sum(&tctx->inflight);
}

- why is there the difference between canceling task and files?

- inflight_tracked doesn't track fixed files
- ->inflight tracks all requests
```

### context

#### io_uring->in_cancel

```
atomic_inc(&tctx->in_cancel)
	io_uring_cancel_generic

atomic_dec(&tctx->in_cancel);
	io_uring_cancel_generic

atomic_read(&tctx->in_cancel)
	io_put_task_remote
		io_put_task
		__io_req_complete_post
		io_free_req
	tctx_task_work
	io_req_local_work_add
	io_tctx_exit_cb
```

#### who add/remove io to ctx->iopoll_list

```
io_iopoll_req_issued
	io_issue_sqe

wq_list_cut
	io_do_iopoll
```

#### unregistering fixed file

```
__io_sqe_files_unregister
	io_ring_ctx_free>>
		io_ring_exit_work
	io_sqe_files_unregister>>
	io_sqe_files_register>>
```

#### fput io_uring FD

```
do_exit()
	exit_files(tsk);
		put_files_struct(files);
			close_files(files);
				filp_close(file, files);
					fput(filp);

actual trace:

  b'io_uring_release'
  b'__fput'
  b'task_work_run'
  b'do_exit'
  b'__pfx___ia32_sys_exit_group'
  b'__pfx___wake_up_parent'
  b'do_syscall_64'
  b'entry_SYSCALL_64_after_hwframe'
    4
```

#### HIPRI implies NOWAIT


### comment

#### ublk requirement

1) support request submitted from multiple tasks which share one io_uring
context


#### callback prototype

```
1) io_uring_task_exit(unsigned long ctx_id, const struct task_struct *task)

2) API for checking uring command

io_uring_cmd_match(struct uring_cmd *cmd, unsigned long ctx_id, const struct task_struct *task)
```

#### delayed __fput()

```
1) io timeout is caused

2) one solution is to schedule exit work from do_exit()/__io_uring_cancel()

3) long delay is caused from:

io_uring_cancel_generic
	io_uring_clean_tctx
		io_wq_put_and_exit(wq);
			io_wq_exit_workers(wq);

4) how to solve it

- released resource in current task by io poll can be consumed by other
  pthreads immediately, so till may cause hang in current context

- why fallback work doesn't work?

- where is the timedout request submitted?

- why can't move exit_wq into exit_work()?
```

#### t/fio io_uring setttings

```
io_uring_setup():

          p->flags |= IORING_SETUP_CQSIZE;
          p->cq_entries = entries;
  
          p->flags |= IORING_SETUP_COOP_TASKRUN;
          p->flags |= IORING_SETUP_SINGLE_ISSUER;
          p->flags |= IORING_SETUP_DEFER_TASKRUN;
```

#### 2bc057692599 block: don't make REQ_POLLED imply REQ_NOWAIT

Then io_wq won't get -EAGAIN, but still may block because polling hold
requests without progress.


#### cancel_all

```
1) cancel_all is false

- called in do_exit(), each pthread or process? per-pthread, since the
  do_exit() is from kernel.

- when task->files are live yet

- pending task works

- there might be other ctx associated with tctx


2) cancel_all is true

- called in exec()


3) related commits


- https://lore.kernel.org/io-uring/7bfc5409a78f8e2d6b27dec3293ec2d248677348.1621201931.git.asml.silence@gmail.com/#r

3dd0c97a9e01 io_uring: get rid of files in exit cancel

- https://lore.kernel.org/io-uring/1a5986a97df4dc1378f3fe0ca1eb483dbcf42112.1618101759.git.asml.silence@gmail.com/#r
3f48cf18f886 io_uring: unify files and task cancel


- 0f2122045b94 io_uring: don't rely on weak ->files references

Start to cancel files


- fcb323cc53e2 io_uring: io_uring: add support for async work inheriting files


start of the story, add ->files to async io_work

- 3dd0c97a9e01 io_uring: get rid of files in exit cancel  (v5.14)

- 3bfe6106693b io-wq: fork worker threads from original task


- original cancel function: __io_uring_cancel / io_uring_cancel_files
```

#### defer_list


## io_kiocb double completion & link advancing in case of io-wq

1) request may be complete via task_work: 1st completetion

>
> ```
> static void io_req_complete_post(struct io_kiocb *req, unsigned issue_flags)
>     ...
> 	/*
> 	 * Handle special CQ sync cases via task_work. DEFER_TASKRUN requires
> 	 * the submitter task context, IOPOLL protects with uring_lock.
> 	 */
> 	if (ctx->task_complete || (ctx->flags & IORING_SETUP_IOPOLL)) {
> 		req->io_task_work.func = io_req_task_complete;
> 		io_req_task_work_add(req);
> 		return;
> 	}
>     ...
> ```

2) request completed via io_free_req(): final completion

> ```
> struct io_wq_work *io_wq_free_work(struct io_wq_work *work)
> {
> 	struct io_kiocb *req = container_of(work, struct io_kiocb, work);
> 	struct io_kiocb *nxt = NULL;
> 
> 	if (req_ref_put_and_test(req)) {
> 		if (req->flags & IO_REQ_LINK_FLAGS)
> 			nxt = io_req_find_next(req);
> 		io_free_req(req);
> 	}
> 	return nxt ? &nxt->work : NULL;
> }
> ```

Then the result is that the same request appears two times in `__io_submit_flush_completions`

BTW, for io-wq, link advancing is done in `io_wq_free_work()` or `io_queue_next()` from
`io_free_batch_list`, and it depends on where the refcount drops to zero in the
two code paths.

## Understand rsrc_node(v6.13)

- cover registered file & fixed buffer

resource lifetime management(buffer register/unregister vs buffer use)


## Understand io_uring buffers

### buffer register/unregister

```
io_sqe_buffers_unregister
   io_ring_ctx_free
    __io_uring_register 
    
``` 


### fixed buffer

- imported in ->prep()

- may be delayed to ->issue()

- managed by rsrc_node

- why can't be imported more than once in `io_uring/rw.c`?

### plain userspace buffer

- call io_import_iovec() directly in ->prep()

- why does vectored buffer have to be imported in the submission context?
-EFAULT is observed when importing uiov in io-wq context. But not observed
in case of io_uring_prep_read().

    EFAULT is returned from copy_iovec_from_user() <- iovec_from_user <-
    __import_iovec, and **copy_from_user()** can be only done in the submission
    conext

### selected buffer

#### overview


#### interface

##### io_kbuf_recycle

static inline bool io_kbuf_recycle(struct io_kiocb *req, unsigned issue_flags)

note:

1) usually for handling -EAGAIN

2) reset the buffer, so that it can be reused next time


##### io_put_kbuf

static inline unsigned int io_put_kbuf(struct io_kiocb *req, int len,
                                         unsigned issue_flags)

note:
1) called after data is read to this buffer 



#### context


##### select buffer

- io_iov_buffer_select_prep()

- what if buffer is used up?

```
    io_buffer_select+9
    __io_import_rw_buffer+158
    __io_read+909
    io_read_mshot+84
    io_poll_issue+97
    io_poll_task_func+306
    __io_run_local_work_loop+29
    __io_run_local_work+291
    io_cqring_wait+143
    __do_sys_io_uring_enter+1280
    do_syscall_64+130
    entry_SYSCALL_64_after_hwframe+118
```

##### how to import one provided buffer

- import_ubuf()

nothing is special


#### comment

- io_import_iovec() directly, called in ->issue() only

- how to cover sync between providing buffer & buffer use, buffer group

- rsrc_node isn't used


### REQ_F_BUFFER_RING / IORING_REGISTER_PBUF_RING

#### io_register_pbuf_ring


### io_read() & io_write buffer use


- IORING_RECVSEND_FIXED_BUF

```
io_send_zc_prep
    if (req->opcode != IORING_OP_SEND_ZC) {
        if (unlikely(zc->flags & IORING_RECVSEND_FIXED_BUF))
            return -EINVAL;
    }

io_send_zc_import
    if (sr->flags & IORING_RECVSEND_FIXED_BUF) {
        sr->notif->buf_index = req->buf_index;
        ret = io_import_reg_buf(sr->notif, &kmsg->msg.msg_iter,
                                (u64)(uintptr_t)sr->buf, sr->len,
                                ITER_SOURCE, issue_flags);
        if (unlikely(ret))
            return ret;
        kmsg->msg.sg_from_iter = io_sg_from_iter;
    }
```

- is it possible to use IORING_RECVSEND_FIXED_BUF for recv?



### net io buffer use




## Add support for vectored registered buffers

[https://lore.kernel.org/all/cover.1741014186.git.asml.silence@gmail.com/#r](https://lore.kernel.org/all/cover.1741014186.git.asml.silence@gmail.com/#r)


### userspace buffer vs. kernel bvec buffer


- userspace buffer

each bvec size is uniform, with fixed size of `1 << imu->folio_shift`

[io_sqe_buffer_register](https://web.git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/io_uring/rsrc.c?h=v6.14-rc7#n715)


- kernel bvec buffer

each bvec size is not uniform

### how to support vectored fixed kernel buffer

- io_estimate_bvec_size()

- io_vec_fill_bvec()

use for_each_mp_bvec() to implement

### interfaces

#### io_prep_reg_iovec


#### io_import_reg_vec

- userspace vec is stored at the last half of iou_vec->vec 

#### io_vec_fill_bvec


## Read multishot support

### background

[[PATCHSET v2 0/3] Add support for multishot reads](https://lore.kernel.org/io-uring/20230912172458.1646720-1-axboe@kernel.dk/)


### core point

- IO_URING_F_MULTISHOT

```
/* the request is executed from poll, it should not be freed */
```

```
io_poll_issue
    io_poll_check_events

__io_read
```


- REQ_F_APOLL_MULTISHOT

```
REQ_F_MULTISHOT|REQ_F_APOLL_MULTISHOT
    io_wq_submit_work
    io_poll_check_events
    io_arm_poll_handler

IO_URING_F_MULTISHOT
    io_poll_issue
        io_poll_check_events
            io_poll_task_func
                __io_poll_execute
                    io_poll_task_func
                        io_handle_tw_list
                        __io_run_local_work_loop
                        __io_poll_execute
                    io_poll_wake
                        init_waitqueue_func_entry(&poll->wait, io_poll_wake)
                            io_init_poll_iocb
                                __io_queue_proc
                                __io_arm_poll_handler
                                    io_poll_queue_proc
                                        ipt.pt._qproc = io_poll_queue_proc;
                                            io_poll_add
                                                .issue                  = io_poll_add
                                                    IORING_OP_POLL_ADD
                    __io_arm_poll_handler
                        io_arm_poll_handler
                            io_wq_submit_work
                        io_poll_add
                            io_poll_remove
                                .issue                  = io_poll_remove
                                    IORING_OP_POLL_REMOVE
                    io_poll_remove_all
                        io_uring_try_cancel_requests
                            io_ring_exit_work
```

- O_NONBLOCK or FMODE_NOWAIT

- -EAGAIN

- file_can_poll()

- provided buffer


- **multishot can't be run from io-wq**


**comment in io_wq_submit_work()**

```
/*
 * If DEFER_TASKRUN is set, it's only allowed to post CQEs from the
 * submitter task context. Final request completions are handed to the
 * right context, however this is not the case of auxiliary CQEs,
 * which is the main mean of operation for multishot requests.
 * Don't allow any multishot execution from io-wq. It's more restrictive
 * than necessary and also cleaner.
 */
```


Reasons for the Restriction

1. DEFER_TASKRUN compatibility: When DEFER_TASKRUN is set, CQEs can only be posted from the submitter task context. Multishot
operations generate multiple auxiliary CQEs, which would violate this constraint if run from io-wq.
2. Cleaner design: The comment notes it's "more restrictive than necessary and also cleaner" - it simplifies the execution model.

What Happens Instead

When a multishot operation would be sent to io-wq, the kernel:
1. Checks if the file can be polled (io_file_can_poll())
2. If the file supports non-blocking operations (O_NONBLOCK or FMODE_NOWAIT), it arms a poll handler instead (io_arm_poll_handler())
3. The operation then runs via the poll mechanism rather than through io-wq

This ensures multishot operations always run in the appropriate task context where they can safely post multiple CQEs.



### data structures

#### io_poll_table

```
struct io_poll_table {
	struct poll_table_struct pt;
	struct io_kiocb *req;
	int nr_entries;
	int error;
	bool owning;
	/* output value, set only if arm poll returns >0 */
	__poll_t result_mask;
};
```

```
/*
 * structures and helpers for f_op->poll implementations
 */
typedef void (*poll_queue_proc)(struct file *, wait_queue_head_t *, struct poll_table_struct *);

/*
 * Do not touch the structure directly, use the access function
 * poll_requested_events() instead.
 */
typedef struct poll_table_struct {
	poll_queue_proc _qproc;
	__poll_t _key;
} poll_table;
```

- how is the `key` setup?


#### io_buffer_list borrowing to request

```
req->buf_list = bl
    io_ring_buffer_select
        io_buffer_select
            io_recvmsg
            io_recv_buf_select
            __io_import_rw_buffer
                io_import_rw_buffer
                    io_rw_do_import
                        io_prep_rw
                    __io_read
    io_ring_buffers_peek
        io_buffers_peek
            io_recv_buf_select
                io_recv
            io_send_select_buffer
        io_buffers_select
            io_send_select_buffer
                io_send
    
req->buf_list = NULL
    io_sendmsg_prep
    io_recvmsg_prep

```

```
io_kbuf_recycle_legacy
    io_kbuf_recycle
        io_queue_async
        io_recvmsg
        io_recv
        io_poll_task_func
        io_arm_apoll
        io_read_mshot
        io_uring_cmd
```

#### buffer unregister

```
io_unregister_pbuf_ring
    __io_uring_register(IORING_UNREGISTER_PBUF_RING)
```

- what is the auto-unregister code path?

```
io_destroy_bl
    io_register_pbuf_ring
io_destroy_buffers
    io_ring_ctx_free
```

#### buffer list flags

- REQ_F_BUFFER_RING

Purpose: Indicates that the request is using ring-provided buffers (not legacy buffers)

Usage:
- Set in io_ring_buffer_select() at kbuf.c:173 when selecting from a buffer ring
- Set in io_ring_buffers_peek() at kbuf.c:301 for peek operations
- Used in io_do_buffer_select() to determine if buffer selection is complete
- Cleared in io_kbuf_recycle_ring() during recycling

Key Behavior: Distinguishes between ring buffers (io_uring_buf_ring) vs legacy buffers (io_buffer list)


- REQ_F_BUFFERS_COMMIT

Purpose: Indicates that buffer ring head pointer needs to be incremented when the operation completes

Usage:
- Set alongside REQ_F_BUFFER_RING in io_ring_buffer_select() at kbuf.c:173
- Set in io_buffers_select() for peek operations at kbuf.c:323
- Checked in io_kbuf_commit() at kbuf.c:57 to determine if buffer advancement is needed
- Cleared after committing the buffer advancement in io_kbuf_commit() at kbuf.c:60

Key Behavior: Ensures buffer ring head advances after successful I/O completion to "consume" the buffer

```
io_kbuf_commit
    io_ring_buffer_select
    io_buffers_select
    __io_put_kbuf_ring
        __io_put_kbufs
            io_put_kbuf
                io_req_defer_failed
                io_send_finish
                io_recv_finish
                io_req_rw_complete
                kiocb_done
                io_read_mshot
                io_do_iopoll
                io_uring_mshot_cmd_post_cqe
            io_put_kbufs
                io_send_finish
                io_recv_finish
    io_net_kbuf_recyle
```

- REQ_F_BUFFER_SELECT

Purpose: Indicates that the request needs automatic buffer selection from provided buffers (user set via IOSQE_BUFFER_SELECT)

Usage:
- Set by user via IOSQE_BUFFER_SELECT SQE flag during prep
- Checked in various operations (net.c, rw.c, uring_cmd.c) to trigger buffer selection
- Used in io_do_buffer_select() to determine if buffer selection is needed
- Required for multishot uring_cmd operations

Key Behavior: Tells io_uring to automatically select a buffer before executing the operation


- REQ_F_BUFFER_SELECTED

Purpose: Indicates that a legacy buffer has been selected and assigned to req->kbuf

Usage:
- Set in io_provided_buffer_select() at kbuf.c:131 when selecting from legacy buffer list
- Used in io_do_buffer_select() to skip buffer selection if already done
- Checked in io_kbuf_recycle() to determine recycling path
- Used in cleanup (io_clean_op()) to trigger legacy buffer cleanup

Key Behavior: Marks that a legacy buffer is active and needs cleanup on completion

- REQ_F_BL_NO_RECYCLE

Purpose: Prevents buffer recycling/reclamation for the request (keeps buffers "consumed")

Usage:
- Set in io_buffers_select() for peek operations at kbuf.c:323
- Set in networking code (io_net_kbuf_recyle()) when buffers are already committed
- Set in read/write retry paths (rw.c:544, 607) to prevent buffer recycling on retry
- Checked in io_kbuf_recycle() at kbuf.h:138 to skip recycling

Key Behavior: Prevents buffers from being returned to the available pool, effectively "consuming" them


- Flag Combinations and Flow

Ring Buffer Selection Flow:
1. User sets IOSQE_BUFFER_SELECT → REQ_F_BUFFER_SELECT
2. io_ring_buffer_select() → REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT
3. On completion → io_kbuf_commit() advances ring head
4. On recycle → io_kbuf_recycle_ring() clears flags

Legacy Buffer Selection Flow:
1. User sets IOSQE_BUFFER_SELECT → REQ_F_BUFFER_SELECT
2. io_provided_buffer_select() → REQ_F_BUFFER_SELECTED
3. On completion → buffer returned to pool (unless REQ_F_BL_NO_RECYCLE)
4. On cleanup → io_kbuf_drop_legacy() frees buffer

Peek/Multi-buffer Operations:
1. io_buffers_select() → REQ_F_BUFFER_RING | REQ_F_BUFFERS_COMMIT | REQ_F_BL_NO_RECYCLE
2. Buffers are committed but not recycled (consumed permanently)

These flags work together to manage the complex buffer lifecycle in io_uring's
provided buffer system, handling both modern ring buffers and legacy buffer lists
efficiently.


### interfaces


#### io_req_post_cqe

```
/*
 * A helper for multishot requests posting additional CQEs.
 * Should only be used from a task_work including IO_URING_F_MULTISHOT.
 */
bool io_req_post_cqe(struct io_kiocb *req, s32 res, u32 cflags)

io_req_post_cqe
    io_read_mshot
    io_poll_check_events
    io_timeout_complete
        req->io_task_work.func = io_timeout_complete
            io_timeout_fn
    io_send_finish
    io_recv_finish
```


#### io_init_poll_iocb


#### io_arm_poll_handler
```
int io_arm_poll_handler(struct io_kiocb *req, unsigned issue_flags)
{
    ...
	req->flags |= REQ_F_POLLED;
	ipt.pt._qproc = io_async_queue_proc;

	io_kbuf_recycle(req, issue_flags);

	ret = __io_arm_poll_handler(req, &apoll->poll, &ipt, mask, issue_flags);
	if (ret)
		return ret > 0 ? IO_APOLL_READY : IO_APOLL_ABORTED;
	trace_io_uring_poll_arm(req, mask, apoll->poll.events);
	return IO_APOLL_OK;
}
```

#### io_async_queue_proc

```
static void io_async_queue_proc(struct file *file, struct wait_queue_head *head,
			       struct poll_table_struct *p)
{
	struct io_poll_table *pt = container_of(p, struct io_poll_table, pt);
	struct async_poll *apoll = pt->req->apoll;

	__io_queue_proc(&apoll->poll, pt, head, &apoll->double_poll);
}
```

- 


#### eventfd_poll(eventfd .poll()) 
```
static __poll_t eventfd_poll(struct file *file, poll_table *wait)
{
    struct eventfd_ctx *ctx = file->private_data;
    __poll_t events = 0;
    u64 count;
  
    poll_wait(file, &ctx->wqh, wait);
    ....
}
```

- poll_wait() calls wait->_qproc()

- eventfd isn't FMODE_STREAM, but it is FMODE_LSEEK


#### io_kbuf_recycle_ring

```
static inline bool io_kbuf_recycle_ring(struct io_kiocb *req)
{
        /*
         * We don't need to recycle for REQ_F_BUFFER_RING, we can just clear
         * the flag and hence ensure that bl->head doesn't get incremented.
         * If the tail has already been incremented, hang on to it.
         * The exception is partial io, that case we should increment bl->head
         * to monopolize the buffer.
         */
        if (req->buf_list) {
                req->flags &= ~(REQ_F_BUFFER_RING|REQ_F_BUFFERS_COMMIT);
                return true;
        }
        return false;
}
```

#### io_kbuf_recycle_legacy

```
bool io_kbuf_recycle_legacy(struct io_kiocb *req, unsigned issue_flags)
{
        struct io_ring_ctx *ctx = req->ctx;
        struct io_buffer_list *bl;
        struct io_buffer *buf;

        io_ring_submit_lock(ctx, issue_flags);

        buf = req->kbuf;
        bl = io_buffer_get_list(ctx, buf->bgid);
        list_add(&buf->list, &bl->buf_list);
        bl->nbufs++;
        req->flags &= ~REQ_F_BUFFER_SELECTED;

        io_ring_submit_unlock(ctx, issue_flags);
        return true;
}
```


### Contexts

#### io_arm_poll_handler() mechanism from AI

```
● The io_arm_poll_handler() mechanism is io_uring's way of handling asynchronous
I/O operations that can't complete immediately. Here's a detailed breakdown:

  Overview

  When an operation would block (like reading from a socket with no data), instead of
  sending it to io-wq, io_uring arms a poll handler that waits for the file descriptor
  to become ready.

  The Call Chain

  io_arm_poll_handler() → io_arm_apoll() → __io_arm_poll_handler()

  Step-by-Step Mechanism

  1. Initial Setup (io_arm_poll_handler() at poll.c:698)

  int io_arm_poll_handler(struct io_kiocb *req, unsigned issue_flags)
  {
      const struct io_issue_def *def = &io_issue_defs[req->opcode];
      __poll_t mask = POLLPRI | POLLERR;

      // Check if operation supports polling
      if (!def->pollin && !def->pollout)
          return IO_APOLL_ABORTED;
      if (!io_file_can_poll(req))
          return IO_APOLL_ABORTED;

  The function first determines what poll events to wait for based on the operation type:
  - Read operations: EPOLLIN | EPOLLRDNORM
  - Write operations: EPOLLOUT | EPOLLWRNORM
  - Always: POLLPRI | POLLERR for priority data and errors

  2. Async Poll Setup (io_arm_apoll() at poll.c:670)

  int io_arm_apoll(struct io_kiocb *req, unsigned issue_flags, __poll_t mask)
  {
      struct async_poll *apoll;
      struct io_poll_table ipt;

      mask |= EPOLLET;  // Always use edge-triggered
      if (!(req->flags & REQ_F_APOLL_MULTISHOT))
          mask |= EPOLLONESHOT;  // Single-shot unless multishot

      apoll = io_req_alloc_apoll(req, issue_flags);
      req->flags |= REQ_F_POLLED;
      ipt.pt._qproc = io_async_queue_proc;  // Callback for poll events

  Key aspects:
  - Edge-triggered polling: EPOLLET ensures we only get notified once per state change
  - Multishot support: For operations that can generate multiple events
  - Queue proc: io_async_queue_proc handles poll notifications

  3. Core Poll Handler (__io_arm_poll_handler() at poll.c:543)

  This is where the magic happens:

  static int __io_arm_poll_handler(struct io_kiocb *req,
                   struct io_poll *poll,
                   struct io_poll_table *ipt, __poll_t mask,
                   unsigned issue_flags)
  {
      // Set up ownership for concurrency control
      ipt->owning = issue_flags & IO_URING_F_UNLOCKED;
      atomic_set(&req->poll_refs, (int)ipt->owning);

      // Call into VFS layer to register poll
      mask = vfs_poll(req->file, &ipt->pt) & poll->events;

  Critical behavior:
  - vfs_poll(): Registers the request with the file's wait queue
  - Immediate ready check: If data is already available, returns immediately
  - Concurrency protection: Uses poll_refs to handle races between task_work and io-wq

  4. Poll Event Handling

  When the file becomes ready, the kernel's wait queue mechanism calls io_async_queue_proc():

  static void io_async_queue_proc(struct file *file, struct wait_queue_head *head,
                 struct poll_table_struct *p)
  {
      struct io_poll_table *pt = container_of(p, struct io_poll_table, pt);
      struct async_poll *apoll = pt->req->apoll;

      __io_queue_proc(&apoll->poll, pt, head, &apoll->double_poll);
  }

  This sets up the actual wait queue entry that will trigger when the file is ready.

  5. Task Work Execution

  When the poll triggers, io_poll_task_func() runs as task_work:

  void io_poll_task_func(struct io_kiocb *req, io_tw_token_t tw)
  {
      int ret = io_poll_check_events(req, tw);

      if (ret == IOU_POLL_NO_ACTION) {
          io_kbuf_recycle(req, 0);
          return;
      } else if (ret == IOU_POLL_REQUEUE) {
          // Re-arm for multishot
          __io_poll_execute(req, 0);
          return;
      }
      // Complete or reissue the original operation

  Key Design Principles

  Ownership Model

  - Task context: No ownership needed (serialized by running in same task)
  - io-wq context: Takes ownership to prevent races with task_work

  Multishot Support

  - For operations like recvmsg with MSG_WAITALL
  - Can generate multiple completion events from single submission
  - Uses REQ_F_APOLL_MULTISHOT flag

  Edge Cases

  - Spurious wakeups: Handled by re-checking conditions
  - Cancellation: Integrated with io_uring's cancellation system
  - Double polling: Some files may need multiple wait queues

  Why This Matters

  This mechanism allows io_uring to:
  1. Avoid blocking threads: No io-wq threads are consumed waiting
  2. Maintain task context: Completions run in the correct context for DEFER_TASKRUN
  3. Support complex operations: Like multishot receives and accepts
  4. Scale efficiently: Hundreds of thousands of polled operations with minimal overhead

  The poll handler is fundamental to io_uring's high-performance async I/O model, especially for network workloads where operations
  frequently need to wait for socket readiness.

```


### Questions

- how to cancel multishot request?

- does io_uring support multiple multishot requests? If yes, how to do it?

- what is the difference between REQ_F_MULTISHOT and REQ_F_APOLL_MULTISHOT?

### Trace

#### One read multishot on eventfd 

```
tracepoint:io_uring:io_uring_create mshot 1102401056: ring 0xffff8ae647079000, fd 4 sq size 1, cq size 1, flags 0x10008
tracepoint:io_uring:io_uring_register mshot 1102401162: ring 0xffff8ae647079000, opcode 22, nr_user_files 0, nr_user_bufs 0, ret 0 
tracepoint:io_uring:io_uring_submit_req mshot 1102401176: req 0xffff8ae70ab07000, user_data 0x0, opcode READ_MULTISHOT, flags 0x8400020, sq_thread 0
tracepoint:io_uring:io_uring_file_get mshot 1102401178: ring 0xffff8ae647079000, req 0xffff8ae70ab07000, user_data 0x0, fd 3
kretfunc:vmlinux:io_read_mshot mshot 1102401182: ret = fffffff5, 
        bpf_prog_4b0b12023458c732_kretfunc_vmlinux_io_read_mshot+170
        bpf_prog_4b0b12023458c732_kretfunc_vmlinux_io_read_mshot+170
        bpf_trampoline_6442564991+108
        io_read_mshot+9
        io_issue_sqe+84
        io_submit_sqes+498
        __do_sys_io_uring_enter+611
        do_syscall_64+132
        entry_SYSCALL_64_after_hwframe+118

kretfunc:vmlinux:io_async_queue_proc mshot 1102401201: ret = 0, 
        bpf_prog_63c5509802e8f0e0_kretfunc_vmlinux_io_async_queue_proc+159
        bpf_prog_63c5509802e8f0e0_kretfunc_vmlinux_io_async_queue_proc+159
        bpf_trampoline_6442508419+116
        io_async_queue_proc+9
        eventfd_poll+36
        bpf_trampoline_6442591614+57
        eventfd_poll+9
        __io_arm_poll_handler+187
        io_arm_poll_handler+456
        io_queue_async+51
        io_submit_sqes+1145
        __do_sys_io_uring_enter+611
        do_syscall_64+132
        entry_SYSCALL_64_after_hwframe+118

kretfunc:vmlinux:eventfd_poll mshot 1102401208: ret = 4, 
        bpf_prog_e0f1e6bc919a2c5f_kretfunc_vmlinux_eventfd_poll+255
        bpf_prog_e0f1e6bc919a2c5f_kretfunc_vmlinux_eventfd_poll+255
        bpf_trampoline_6442591614+108
        eventfd_poll+9
        __io_arm_poll_handler+187
        io_arm_poll_handler+456
        io_queue_async+51
        io_submit_sqes+1145
        __do_sys_io_uring_enter+611
        do_syscall_64+132
        entry_SYSCALL_64_after_hwframe+118

tracepoint:io_uring:io_uring_poll_arm mshot 1102401212: ring 0xffff8ae647079000, req 0xffff8ae70ab07000, user_data 0x0, opcode READ_MULTISHOT, mask 0x8000004b, events 0x8000207b
tracepoint:io_uring:io_uring_cqring_wait mshot 1102401217: ring 0xffff8ae647079000, min_events 1
tracepoint:io_uring:io_uring_task_add mshot 1102401340: ring 0xffff8ae647079000, req 0xffff8ae70ab07000, user_data 0x0, opcode READ_MULTISHOT, mask 1 
        __io_poll_execute+106
        __io_poll_execute+106
        io_poll_wake+229
        __wake_up_common+117
        eventfd_write+197
        vfs_write+204
        ksys_write+107
        do_syscall_64+132
        entry_SYSCALL_64_after_hwframe+118

kretfunc:vmlinux:eventfd_poll mshot 1102401405: ret = 5, 
        bpf_prog_e0f1e6bc919a2c5f_kretfunc_vmlinux_eventfd_poll+255
        bpf_prog_e0f1e6bc919a2c5f_kretfunc_vmlinux_eventfd_poll+255
        bpf_trampoline_6442591614+108
        eventfd_poll+9
        io_poll_task_func+254
        io_handle_tw_list+41
        tctx_task_work_run+81
        tctx_task_work+58
        task_work_run+89
        io_run_task_work+78
        io_cqring_wait+923
        __do_sys_io_uring_enter+1280
        do_syscall_64+132
        entry_SYSCALL_64_after_hwframe+118

tracepoint:io_uring:io_uring_complete mshot 1102401418: req (nil), user_data 0x0, result 8, cflags 0x10003 extra1 0 extra2 0
kretfunc:vmlinux:io_read_mshot mshot 1102401418: ret = fffffff5, 
        bpf_prog_4b0b12023458c732_kretfunc_vmlinux_io_read_mshot+170
        bpf_prog_4b0b12023458c732_kretfunc_vmlinux_io_read_mshot+170
        bpf_trampoline_6442564991+108
        io_read_mshot+9
        io_poll_issue+97
        io_poll_task_func+306
        io_handle_tw_list+41
        tctx_task_work_run+81
        tctx_task_work+58
        task_work_run+89
        io_run_task_work+78
        io_cqring_wait+923
        __do_sys_io_uring_enter+1280
        do_syscall_64+132
        entry_SYSCALL_64_after_hwframe+118

kretfunc:vmlinux:eventfd_poll mshot 1102401424: ret = 4, 
        bpf_prog_e0f1e6bc919a2c5f_kretfunc_vmlinux_eventfd_poll+255
        bpf_prog_e0f1e6bc919a2c5f_kretfunc_vmlinux_eventfd_poll+255
        bpf_trampoline_6442591614+108
        eventfd_poll+9
        io_poll_task_func+254
        io_handle_tw_list+41
        tctx_task_work_run+81
        tctx_task_work+58
        task_work_run+89
        io_run_task_work+78
        io_cqring_wait+923
        __do_sys_io_uring_enter+1280
        do_syscall_64+132
        entry_SYSCALL_64_after_hwframe+118

tracepoint:io_uring:io_uring_task_work_run mshot 1102401431: tctx 0xffff8ae74d612400, count 1
tracepoint:io_uring:io_uring_cqring_wait mshot 1102401517: ring 0xffff8ae647079000, min_events 1

```

### timeout multishot implementation

#### overview

[[PATCH v4] io_uring: add support for multishot timeouts](https://lore.kernel.org/io-uring/20230418225817.1905027-1-davidhwei@meta.com/)

[[PATCH] io_uring: add support for multishot timeouts](https://lore.kernel.org/io-uring/20230323231015.2170096-1-davidhwei@meta.com/)

Not use provided buffer.



# **io-uring patches**

## [RFC PATCH 0/4] rust: miscdevice: abstraction for uring-cmd

[[RFC PATCH 0/4] rust: miscdevice: abstraction for uring-cmd](https://lore.kernel.org/io-uring/20250719143358.22363-1-sidong.yang@furiosa.ai/)

[[RFC PATCH v2 0/4] rust: miscdevice: abstraction for uring-cmd](https://lore.kernel.org/io-uring/20250727150329.27433-1-sidong.yang@furiosa.ai/)



## Add uring_cmd multishot support

[[PATCH] io_uring: uring_cmd: add multishot support without poll](https://lore.kernel.org/io-uring/20250810025024.1659190-1-ming.lei@redhat.com/)


### background

#### Pavel's implementation

[tx timestamp io_uring commands](https://lore.kernel.org/io-uring/cover.1745843119.git.asml.silence@gmail.com/#r)

#### why doesn't read_mshot work?

- poll() on multiple queue

- read_multishot() on multiple queue

    - make each queue as pollable device?

- is it possible for poll() to poll on multiple queue?

    - no, looks not, ->poll() can't recognize the current polling queue

### requirement

#### device interrupt event delivery

- needn't poll()

- triggered by io_uring API() directly


#### from AI

Please support multishot uring_cmd on linux io_uring, follows its user interface:

1) passing one multishot flag from e->uring_cmd_flags

2) only support selected or provided buffer, such as, pass buffer group id
from sqe->sqe->buf_group, and set IOSQE_BUFFER_SELECT on sqe->flags.

The main use case is for delivering device event, such as interrupt event, to
userspace efficiently, and the event is stored to the selected buffer of
the multishot uring_cmd. Device can generate event automatically, so we
don't rely on device driver's poll() method, and REQ_F_APOLL_MULTISHOT
can't be set, so driver can just call io_uring uring_cmd APIs for getting
data. This way is similar with timeout multishot implementation.

For how to use provided buffer which is marked by IOSQE_BUFFER_SELECT,
please refer to io_read_mshot() and io_recvmsg().

io_uring uring_cmd.c needs to provide two APIs to driver: one is for
selecting the buffer, another is for handling event completion, such as
posting CQE to io_uring completion queue, and updating the provide buffer.


### design

#### support .poll() for ublk char device

It doesn't work, because we need per-queue event, instead of device wide
event

Also "poll()" isn't necessary, also not efficient.

Here the model is hardware interrupt, and the usage is similar with timeout
multishot, however provided buffer is really needed.

#### steal timeout multishot implementation

- REQ_F_APOLL_MULTISHOT isn't needed

- support provided buffer

#### internal per-queue ring buffer for holding incoming request tag


### implementation

#### buffer lifetime issue from Jens

```
Final comment on this part... uring_cmd is unique in the sense that it'd
be the first potentially pollable file type that supports buffer
selection AND can return -EIOCBQUEUED. For non-pollable, the buffer
would get committed upfront. For pollable, we'd either finish and put it
within this same execution context, or we'd drop it entirely when
returning -EAGAIN.

So what happens if we get -EIOCBQUEUED with a selected buffer from
provided buffer ring, and someome malicious unregisters and frees the
buffer ring before that request completes?
```

#### typical selected buffer & pollable file


- io_read_mshot

```
io_read_mshot()
    ...
    if (ret == -EAGAIN) {
            /*
             * Reset rw->len to 0 again to avoid clamping future mshot
             * reads, in case the buffer size varies.
             */
            if (io_kbuf_recycle(req, issue_flags))
                    rw->len = 0;
            return IOU_RETRY;
    } else if (ret <= 0) {
```

```
io_queue_sqe
    io_queue_async
        io_arm_poll_handler
        io_kbuf_recycle
```

`IORING_CQE_F_BUFFER` means that the provided buffer is valid:

```
ret = IORING_CQE_F_BUFFER | (req->buf_index << IORING_CQE_BUFFER_SHIFT);
    __io_put_kbufs

```


#### unregister ring buffer

```
__io_uring_register(IORING_UNREGISTER_PBUF_RING)
    io_unregister_pbuf_ring
```

Once the provided buffer is unregistered, the uring OP may return with -ENOBUFS


## **How to support kernel buffer**

### motivation

- device zero copy

buffer is from kernel, and need to lease to io_uring for reading data to
the buffer, or writing data in the buffer.

It is like traditional copy_file_range() or splice() which can't support
to read data to the pages in pipe, also buffer ownership model isn't same.

[io_uring: support leased group buffer with REQ_F_GROUP_KBUF](https://lore.kernel.org/linux-block/20241025122247.3709133-6-ming.lei@redhat.com/)

> ```
> This way looks a bit similar with kernel's pipe/splice, but there are some
> important differences:
> 
> - splice is for transferring data between two FDs via pipe, and fd_out can
> only read data from pipe, but data can't be written to; this feature can
> lease buffer from group leader(driver subsystem) to members(io_uring subsystem),
> so member request can write data to this buffer if the buffer direction is
> allowed to write to.
> 
> - splice implements data transfer by moving pages between subsystem and
> pipe, that means page ownership is transferred, and this way is one of the
> most complicated thing of splice; this patch supports scenarios in which
> the buffer can't be transferred, and buffer is only borrowed to member
> requests for consumption, and is returned back after member requests
> consume the leased buffer, so buffer lifetime is aligned with group leader
> lifetime, and buffer lifetime is simplified a lot. Especially the buffer
> is guaranteed to be returned back.
> 
> - splice can't run in async way basically
> ```

Also it has to be careful for dealing with reading data to the kernel buffer.
If the kernel buffer may be copied to userspace, the application has to be
trusted, that said the application has to read data to the buffer, otherwise
kernel data may be leaked to userspace.

### design

#### does it need to cover userspace buffer?

- such as, the buffer is originated from userspace, and it is provided
from one bpf OP(group leader)

- technically it is pretty easy to cover it

- at least it can be used for test purpose

#### how to make SQE group work with fixed kernel buffer?

- when to remove the registered buffer node?

  when the group leader is completed, one request flag of unregister
  buffer is used, buffer index is required

- is it possible to not introduce buffer index concept?

- local buffer

mark this buffer as **local buffer**, and local buffer is un-registered
automatically when its reference drops to zero

local buffer lifetime is same with sqe group's lifetime, or the buffer
table is local? auto-unregistered.

group leader owns the local buffer, only group member can use this local
buffer.

- buffer table

Any buffer is registered to the table, it becomes global visible for
everyone, it looks one big trouble now.


### questions

#### is faulted page for READ initialized as 0 before reading file data?


# Io-uring Issues

## 'kernel NULL pointer dereference' on percpu_ref_get_many+0x23/0x30

```
[  103.067047] RIP: 0010:percpu_ref_get_many+0x23/0x30
...
[  103.073111]  io_fallback_req_func+0x3d/0xaf
[  103.073422]  process_one_work+0x188/0x340
```

- exact exception IP

```
(gdb) l *(percpu_ref_get_many+0x23)
0xffffffff81a96d23 is in percpu_ref_get_many (./arch/x86/include/asm/atomic64_64.h:25).
20		__WRITE_ONCE(v->counter, i);
21	}
```

Looks this percpu data is freed.

### test description

- 'make test T=generic/002'

- can be triggered with get_data or without get_data


### use-after-free on 'io_uring_cmd' issue

- one usual problem

- posisoning io->cmd may figure out the reason


# **Idea: buffer abstraction & generic buffer**

## Use cases 
- one typical use case is for supporting io_uring bpf

io_uring exports kfunc for bpf prog, and buffer consumer is done as plugin,
which need to decouple OP implementation from buffer use

## Buffer operations

- setup

- teardown

- import

- recycle(?)

