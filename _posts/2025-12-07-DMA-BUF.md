---
title: DMA-BUF
category: tech
tags: [driver, DMA-BUF, zero copy, linux kernel]
---

title: DMA-BUF

* TOC
{:toc}


# DMA-BUF Subsystem Complete Walkthrough

Generated by AI.

This document provides a comprehensive guide to Linux's DMA-BUF (DMA Buffer Sharing) framework.

## Table of Contents
1. [What is DMA-BUF?](#what-is-dmabuf)
2. [The Problem DMA-BUF Solves](#the-problem-dmabuf-solves)
3. [Architecture Overview](#architecture-overview)
4. [Core Concepts](#core-concepts)
5. [DMA-BUF Operations Flow](#dmabuf-operations-flow)
6. [Kernel Implementation Details](#kernel-implementation-details)
7. [Synchronization and Fencing](#synchronization-and-fencing)
8. [Real-World Use Cases](#real-world-use-cases)
9. [Code Examples](#code-examples)
10. [Performance Considerations](#performance-considerations)

## What is DMA-BUF?

**DMA-BUF** is a Linux kernel framework for sharing buffers between different devices and subsystems without copying data.

### Key Features

- **Zero-Copy Sharing**: Multiple devices access the same physical memory
- **Device-Agnostic**: Works across GPU, Display, Camera, Video, etc.
- **IOMMU-Aware**: Handles address translation transparently
- **Synchronization**: Built-in fencing mechanism for access coordination
- **File Descriptor Based**: Buffers are represented as file descriptors

### Primary Use Cases

1. **GPU ↔ Display**: Render to GPU buffer, scan out from display controller
2. **Camera ↔ GPU**: Capture to camera buffer, process on GPU
3. **Video Decode ↔ Display**: Decode video, display directly
4. **Cross-Process Sharing**: Share buffers between processes via fd passing

## The Problem DMA-BUF Solves

### Before DMA-BUF: The Copy Problem

```
Traditional approach - WASTEFUL:

┌──────────────┐
│   Camera     │ Capture image
│   Driver     │
└──────┬───────┘
       │ DMA to camera buffer
       ↓
┌─────────────────────┐
│  Camera Buffer      │ (Physical memory region A)
│  [Image Data]       │
└──────┬──────────────┘
       │ memcpy() ← EXPENSIVE!
       ↓
┌─────────────────────┐
│  Application Buffer │ (Physical memory region B)
│  [Image Data COPY]  │
└──────┬──────────────┘
       │ memcpy() ← EXPENSIVE AGAIN!
       ↓
┌─────────────────────┐
│  GPU Buffer         │ (Physical memory region C)
│  [Image Data COPY]  │
└──────┬──────────────┘
       │ GPU processes
       ↓
┌─────────────────────┐
│  Display Buffer     │ (Physical memory region D)
│  [Image Data COPY]  │ ← memcpy() AGAIN!
└─────────────────────┘

Problems:
- 3 memory copies (camera → app → GPU → display)
- 4x memory usage
- CPU cycles wasted on memcpy
- Cache pollution
- Bandwidth consumption
- Increased latency
```

### With DMA-BUF: Zero-Copy Sharing

```
DMA-BUF approach - EFFICIENT:

┌──────────────┐
│   Camera     │ Allocates and exports DMA-BUF
│   Driver     │ fd = camera_export_dmabuf()
└──────┬───────┘
       │
       ↓
┌─────────────────────────────────────────┐
│         DMA-BUF (fd=50)                 │
│     [Physical Memory - ONE copy]        │
│                                         │
│  ┌────────────────────────────────┐    │
│  │  Actual buffer data             │    │
│  │  [Image: 1920x1080 RGB]         │    │
│  └────────────────────────────────┘    │
└─────────────────────────────────────────┘
       ↑         ↑         ↑         ↑
       │         │         │         │
    Camera     App      GPU     Display
   (owner)   (import) (import) (import)

Each device:
- Gets scatter-gather table for the SAME memory
- Maps into its own address space (CPU VA, GPU VA, etc.)
- NO data copying!

Benefits:
- 1x memory usage (shared)
- No CPU memcpy overhead
- Zero-copy pipeline
- Much lower latency
- Bandwidth saved for real work
```

## Architecture Overview

### High-Level Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                      Userspace                              │
│                                                             │
│  ┌──────────┐        ┌──────────┐        ┌──────────┐    │
│  │  Camera  │        │   GPU    │        │ Display  │    │
│  │  App     │        │  Process │        │  Manager │    │
│  └────┬─────┘        └────┬─────┘        └────┬─────┘    │
│       │ fd=50             │ fd=50             │ fd=50     │
│       │ (export)          │ (import)          │ (import)  │
└───────┼───────────────────┼───────────────────┼───────────┘
        │                   │                   │
   ═════════════════════════════════════════════════════
             Kernel DMA-BUF Framework
   ═════════════════════════════════════════════════════
        │                   │                   │
┌───────┼───────────────────┼───────────────────┼───────────┐
│       ↓                   ↓                   ↓           │
│  ┌─────────┐         ┌─────────┐        ┌─────────┐     │
│  │ Camera  │         │   GPU   │        │ Display │     │
│  │ Driver  │         │  Driver │        │ Driver  │     │
│  │(export) │         │(import) │        │(import) │     │
│  └────┬────┘         └────┬────┘        └────┬────┘     │
│       │                   │                   │           │
│       │    DMA-BUF Core (drivers/dma-buf/)    │           │
│       │         - Reference counting          │           │
│       │         - Attachment management        │           │
│       │         - Fence synchronization        │           │
│       └───────────────────┼───────────────────┘           │
│                           │                               │
│                  ┌────────┴────────┐                      │
│                  │  DMA-BUF Object │                      │
│                  │  struct dma_buf │                      │
│                  └────────┬────────┘                      │
│                           │                               │
│                  ┌────────┴────────┐                      │
│                  │ Physical Memory │                      │
│                  │  (pages/CMA)    │                      │
│                  └─────────────────┘                      │
└─────────────────────────────────────────────────────────────┘
```

### Key Components

```
1. struct dma_buf
   - The central object representing a shared buffer
   - Reference counted
   - Contains operations callbacks (dma_buf_ops)

2. struct dma_buf_ops
   - Exporter-provided operations
   - attach/detach: Device attachment
   - map_dma_buf/unmap_dma_buf: DMA mapping
   - mmap: CPU mapping
   - release: Cleanup

3. struct dma_buf_attachment
   - Represents a device's attachment to a DMA-BUF
   - One per importing device
   - Stores device-specific info

4. struct sg_table (scatter-gather table)
   - Describes buffer's physical memory layout
   - List of <physical address, length> pairs
   - Used for DMA operations

5. struct dma_fence
   - Synchronization primitive
   - Signals when operations complete
   - Used for producer-consumer coordination
```

## Core Concepts

### 1. Exporter vs Importer

```
Exporter (Producer):
┌──────────────────────────────┐
│  Camera Driver               │
│                              │
│  1. Allocates physical mem   │
│  2. Creates dma_buf          │
│  3. Provides dma_buf_ops     │
│  4. Returns fd to userspace  │
└──────────────────────────────┘

Importer (Consumer):
┌──────────────────────────────┐
│  GPU Driver                  │
│                              │
│  1. Receives fd from user    │
│  2. Gets dma_buf from fd     │
│  3. Attaches device          │
│  4. Maps for DMA access      │
│  5. Performs operations      │
│  6. Unmaps and detaches      │
└──────────────────────────────┘

One Exporter, Multiple Importers:
        ┌─────────────┐
        │  Exporter   │
        │  (Camera)   │
        └──────┬──────┘
               │ Owns buffer
               ↓
        ┌──────────────┐
        │   dma_buf    │
        └──┬────┬────┬─┘
           │    │    │
     ┌─────┘    │    └─────┐
     ↓          ↓          ↓
┌─────────┐ ┌─────────┐ ┌─────────┐
│ GPU     │ │ Display │ │ Video   │
│(import) │ │(import) │ │(import) │
└─────────┘ └─────────┘ └─────────┘
```

### 2. Scatter-Gather Tables

Physical memory isn't always contiguous:

```
Virtual Buffer View (contiguous):
┌─────────────────────────────────────┐
│ 0x0000  [  4KB data  ]              │
│ 0x1000  [  4KB data  ]              │
│ 0x2000  [  4KB data  ]              │
│ 0x3000  [  4KB data  ]              │
└─────────────────────────────────────┘

Physical Memory (scattered):
Physical Address    Length
─────────────────────────────
0x10000000      →   4KB    (page 0)
0x20000000      →   4KB    (page 1)  ← Not contiguous!
0x15000000      →   4KB    (page 2)  ← Scattered
0x18000000      →   4KB    (page 3)

Scatter-Gather Table:
struct scatterlist entries[] = {
    { .dma_address = 0x10000000, .length = 4096 },
    { .dma_address = 0x20000000, .length = 4096 },
    { .dma_address = 0x15000000, .length = 4096 },
    { .dma_address = 0x18000000, .length = 4096 },
};

Device reads via SG table:
- Device DMA engine reads SG table
- For each entry, DMAs from physical address
- Assembles contiguous stream for device
```

### 3. DMA Mapping and IOMMU

DMA-BUF handles IOMMU transparently:

```
Without IOMMU:
Device sees physical addresses directly

┌──────────┐
│  Device  │
└────┬─────┘
     │ DMA: read from 0x10000000
     ↓
┌──────────────────┐
│ Physical Memory  │
│ 0x10000000: data │
└──────────────────┘

With IOMMU:
Device uses I/O virtual addresses (IOVA)

┌──────────┐
│  Device  │
└────┬─────┘
     │ DMA: read from IOVA 0x5000
     ↓
┌────────────────┐
│     IOMMU      │
│  Translate:    │
│  0x5000 →      │
│  0x10000000    │
└────┬───────────┘
     │ Physical: 0x10000000
     ↓
┌──────────────────┐
│ Physical Memory  │
│ 0x10000000: data │
└──────────────────┘

DMA-BUF Handles This:
- dma_map_sgtable() automatically:
  * Checks if IOMMU present
  * If yes: creates IOMMU mappings
  * If no: uses physical addresses
- Importer doesn't need to care!
```

## DMA-BUF Operations Flow

### Export Flow

```
Step-by-step buffer export:

1. Driver Allocates Memory
   ┌──────────────────────────┐
   │ pages = alloc_pages()    │
   │ or CMA allocation        │
   │ or from reserved pool    │
   └────────┬─────────────────┘
            ↓

2. Create DMA-BUF Export Info
   ┌────────────────────────────────┐
   │ DEFINE_DMA_BUF_EXPORT_INFO()   │
   │   .ops = &my_dmabuf_ops        │
   │   .size = buffer_size          │
   │   .flags = O_RDWR              │
   │   .priv = driver_private_data  │
   └────────┬───────────────────────┘
            ↓

3. Export DMA-BUF
   ┌────────────────────────────────┐
   │ dmabuf = dma_buf_export(&exp)  │
   │                                │
   │ Kernel creates:                │
   │  - struct dma_buf              │
   │  - struct file                 │
   │  - Initializes refcount        │
   └────────┬───────────────────────┘
            ↓

4. Get File Descriptor
   ┌────────────────────────────────┐
   │ fd = dma_buf_fd(dmabuf, flags) │
   │                                │
   │ Installs fd in current         │
   │ process's file descriptor      │
   │ table                          │
   └────────┬───────────────────────┘
            ↓

5. Return fd to Userspace
   ┌────────────────────────────────┐
   │ return fd to application       │
   │                                │
   │ App can now:                   │
   │  - Share fd with other process │
   │  - Pass to other drivers       │
   │  - mmap() for CPU access       │
   └────────────────────────────────┘
```

### Import and Use Flow

```
Application shares fd=50 with GPU process

1. Get DMA-BUF from FD
   ┌────────────────────────────────┐
   │ GPU Process / Driver:          │
   │                                │
   │ dmabuf = dma_buf_get(fd)       │
   │                                │
   │ Kernel:                        │
   │  - Looks up fd in file table   │
   │  - Verifies it's a dma_buf     │
   │  - Increments refcount         │
   │  - Returns dma_buf pointer     │
   └────────┬───────────────────────┘
            ↓

2. Attach Device
   ┌────────────────────────────────┐
   │ attach = dma_buf_attach(       │
   │            dmabuf, gpu_dev)    │
   │                                │
   │ Kernel:                        │
   │  - Allocates dma_buf_attachment│
   │  - Calls exporter's attach()   │
   │  - Links attachment to dmabuf  │
   └────────┬───────────────────────┘
            ↓

3. Map for DMA
   ┌────────────────────────────────┐
   │ sgt = dma_buf_map_attachment(  │
   │         attach, DMA_TO_DEVICE) │
   │                                │
   │ Exporter's map_dma_buf():      │
   │  - Builds scatter-gather table │
   │  - Calls dma_map_sgtable()     │
   │  - IOMMU mapping if present    │
   │  - Returns sg_table            │
   └────────┬───────────────────────┘
            ↓

4. Use Buffer (DMA)
   ┌────────────────────────────────┐
   │ GPU Driver:                    │
   │                                │
   │ for_each_sgtable_dma_sg(sgt) { │
   │   dma_addr = sg_dma_address(sg)│
   │   dma_len  = sg_dma_len(sg)    │
   │                                │
   │   /* Program GPU */            │
   │   gpu_set_src_addr(dma_addr)   │
   │   gpu_process(dma_len)         │
   │ }                              │
   └────────┬───────────────────────┘
            ↓

5. Unmap
   ┌────────────────────────────────┐
   │ dma_buf_unmap_attachment(      │
   │    attach, sgt, DMA_TO_DEVICE) │
   │                                │
   │ Exporter's unmap_dma_buf():    │
   │  - Calls dma_unmap_sgtable()   │
   │  - Removes IOMMU mappings      │
   │  - Frees sg_table              │
   └────────┬───────────────────────┘
            ↓

6. Detach Device
   ┌────────────────────────────────┐
   │ dma_buf_detach(dmabuf, attach) │
   │                                │
   │ Kernel:                        │
   │  - Calls exporter's detach()   │
   │  - Frees dma_buf_attachment    │
   └────────┬───────────────────────┘
            ↓

7. Release DMA-BUF
   ┌────────────────────────────────┐
   │ dma_buf_put(dmabuf)            │
   │                                │
   │ Kernel:                        │
   │  - Decrements refcount         │
   │  - If refcount = 0:            │
   │    * Calls exporter release()  │
   │    * Frees dma_buf structure   │
   │    * Frees underlying memory   │
   └────────────────────────────────┘
```

## Kernel Implementation Details

### Core Data Structures

```c
/* The central DMA-BUF object */
struct dma_buf {
	size_t size;                    /* Size of buffer */
	struct file *file;              /* Associated file */
	struct list_head attachments;   /* List of attached devices */
	const struct dma_buf_ops *ops;  /* Exporter operations */
	void *priv;                     /* Exporter private data */

	/* Resv for synchronization */
	struct dma_resv *resv;

	/* Reference counting via file->f_count */
};

/* Operations provided by exporter */
struct dma_buf_ops {
	/* Called when a device attaches */
	int (*attach)(struct dma_buf *, struct dma_buf_attachment *);

	/* Called when a device detaches */
	void (*detach)(struct dma_buf *, struct dma_buf_attachment *);

	/* Map for DMA - returns scatter-gather table */
	struct sg_table * (*map_dma_buf)(struct dma_buf_attachment *,
	                                  enum dma_data_direction);

	/* Unmap DMA */
	void (*unmap_dma_buf)(struct dma_buf_attachment *,
	                      struct sg_table *,
	                      enum dma_data_direction);

	/* Release buffer (refcount = 0) */
	void (*release)(struct dma_buf *);

	/* Map into CPU address space */
	int (*mmap)(struct dma_buf *, struct vm_area_struct *vma);

	/* CPU kernel mapping */
	void *(*vmap)(struct dma_buf *);
	void (*vunmap)(struct dma_buf *, void *vaddr);
};

/* Device attachment to a DMA-BUF */
struct dma_buf_attachment {
	struct dma_buf *dmabuf;         /* The DMA-BUF */
	struct device *dev;             /* Attached device */
	struct list_head node;          /* In dmabuf->attachments */
	void *priv;                     /* Importer private data */
	enum dma_data_direction dir;    /* DMA direction hint */
};

/* Scatter-gather table */
struct sg_table {
	struct scatterlist *sgl;        /* Array of SG entries */
	unsigned int nents;             /* Number of entries */
	unsigned int orig_nents;        /* Original number */
};

struct scatterlist {
	unsigned long page_link;        /* Page pointer */
	unsigned int offset;            /* Offset in page */
	unsigned int length;            /* Length in bytes */
	dma_addr_t dma_address;         /* DMA address (after mapping) */
	unsigned int dma_length;        /* DMA length (may differ) */
};
```

### Exporter Implementation (Detailed)

```c
/* Example: Simple page-based exporter */

struct my_buffer {
	struct page **pages;
	int nr_pages;
	size_t size;
};

/* Attach callback */
static int my_attach(struct dma_buf *dmabuf,
                     struct dma_buf_attachment *attach)
{
	struct my_buffer *buf = dmabuf->priv;

	/* Exporter can:
	 * - Check device capabilities
	 * - Allocate attachment-specific data
	 * - Pin pages if needed
	 */

	pr_debug("Device %s attached\n", dev_name(attach->dev));
	return 0;
}

/* Map for DMA */
static struct sg_table *my_map_dma_buf(struct dma_buf_attachment *attach,
                                        enum dma_data_direction dir)
{
	struct my_buffer *buf = attach->dmabuf->priv;
	struct sg_table *sgt;
	int ret;

	/* Allocate SG table */
	sgt = kzalloc(sizeof(*sgt), GFP_KERNEL);
	if (!sgt)
		return ERR_PTR(-ENOMEM);

	/* Build SG table from pages
	 * This creates scatter-gather entries for the buffer
	 */
	ret = sg_alloc_table_from_pages(sgt,
	                                 buf->pages,
	                                 buf->nr_pages,
	                                 0,          /* offset in first page */
	                                 buf->size,  /* total size */
	                                 GFP_KERNEL);
	if (ret) {
		kfree(sgt);
		return ERR_PTR(ret);
	}

	/* Map for DMA
	 * This handles:
	 * - Cache flushing if needed
	 * - IOMMU mapping if present
	 * - Fills in dma_address fields in scatterlist
	 */
	ret = dma_map_sgtable(attach->dev, sgt, dir, 0);
	if (ret) {
		sg_free_table(sgt);
		kfree(sgt);
		return ERR_PTR(-ENOMEM);
	}

	return sgt;
}

/* Unmap DMA */
static void my_unmap_dma_buf(struct dma_buf_attachment *attach,
                             struct sg_table *sgt,
                             enum dma_data_direction dir)
{
	/* Unmap from device
	 * - Removes IOMMU mappings
	 * - Cache operations if needed
	 */
	dma_unmap_sgtable(attach->dev, sgt, dir, 0);

	/* Free SG table */
	sg_free_table(sgt);
	kfree(sgt);
}

/* Release buffer */
static void my_release(struct dma_buf *dmabuf)
{
	struct my_buffer *buf = dmabuf->priv;
	int i;

	/* Free all pages */
	for (i = 0; i < buf->nr_pages; i++)
		__free_page(buf->pages[i]);

	kfree(buf->pages);
	kfree(buf);
}

/* CPU mmap */
static int my_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)
{
	struct my_buffer *buf = dmabuf->priv;
	unsigned long addr = vma->vm_start;
	int i;

	/* Map each page into user virtual address space */
	for (i = 0; i < buf->nr_pages; i++) {
		int ret = vm_insert_page(vma, addr, buf->pages[i]);
		if (ret)
			return ret;
		addr += PAGE_SIZE;
	}

	return 0;
}

static const struct dma_buf_ops my_dmabuf_ops = {
	.attach = my_attach,
	.map_dma_buf = my_map_dma_buf,
	.unmap_dma_buf = my_unmap_dma_buf,
	.release = my_release,
	.mmap = my_mmap,
};

/* Export function */
int my_export_buffer(size_t size)
{
	struct my_buffer *buf;
	struct dma_buf *dmabuf;
	DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
	int fd, i;

	/* Allocate buffer */
	buf = kzalloc(sizeof(*buf), GFP_KERNEL);
	if (!buf)
		return -ENOMEM;

	buf->size = PAGE_ALIGN(size);
	buf->nr_pages = buf->size >> PAGE_SHIFT;

	buf->pages = kcalloc(buf->nr_pages, sizeof(*buf->pages), GFP_KERNEL);
	if (!buf->pages) {
		kfree(buf);
		return -ENOMEM;
	}

	/* Allocate pages */
	for (i = 0; i < buf->nr_pages; i++) {
		buf->pages[i] = alloc_page(GFP_KERNEL | __GFP_ZERO);
		if (!buf->pages[i])
			goto err_free_pages;
	}

	/* Setup export info */
	exp_info.ops = &my_dmabuf_ops;
	exp_info.size = buf->size;
	exp_info.flags = O_RDWR | O_CLOEXEC;
	exp_info.priv = buf;

	/* Export DMA-BUF */
	dmabuf = dma_buf_export(&exp_info);
	if (IS_ERR(dmabuf))
		goto err_free_pages;

	/* Get fd */
	fd = dma_buf_fd(dmabuf, O_CLOEXEC);
	if (fd < 0) {
		dma_buf_put(dmabuf);
		return fd;
	}

	return fd;

err_free_pages:
	while (i--)
		__free_page(buf->pages[i]);
	kfree(buf->pages);
	kfree(buf);
	return -ENOMEM;
}
```

### Importer Implementation

```c
/* Example: GPU driver importing DMA-BUF */

struct gpu_bo {
	struct dma_buf *dmabuf;
	struct dma_buf_attachment *attachment;
	struct sg_table *sgt;
};

int gpu_import_dmabuf(struct gpu_device *gpu, int fd)
{
	struct gpu_bo *bo;
	struct dma_buf *dmabuf;
	struct dma_buf_attachment *attach;
	struct sg_table *sgt;

	/* Allocate BO structure */
	bo = kzalloc(sizeof(*bo), GFP_KERNEL);
	if (!bo)
		return -ENOMEM;

	/* Get DMA-BUF from fd */
	dmabuf = dma_buf_get(fd);
	if (IS_ERR(dmabuf)) {
		kfree(bo);
		return PTR_ERR(dmabuf);
	}

	/* Attach our device */
	attach = dma_buf_attach(dmabuf, gpu->dev);
	if (IS_ERR(attach)) {
		dma_buf_put(dmabuf);
		kfree(bo);
		return PTR_ERR(attach);
	}

	/* Map for DMA */
	sgt = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);
	if (IS_ERR(sgt)) {
		dma_buf_detach(dmabuf, attach);
		dma_buf_put(dmabuf);
		kfree(bo);
		return PTR_ERR(sgt);
	}

	/* Now we can use the buffer */
	bo->dmabuf = dmabuf;
	bo->attachment = attach;
	bo->sgt = sgt;

	/* Program GPU with DMA addresses */
	struct scatterlist *sg;
	int i;
	for_each_sgtable_dma_sg(sgt, sg, i) {
		dma_addr_t addr = sg_dma_address(sg);
		size_t len = sg_dma_len(sg);

		/* Configure GPU to access this memory region */
		gpu_add_memory_region(gpu, addr, len);
	}

	return 0;  /* Success, bo can be used */
}

void gpu_release_dmabuf(struct gpu_bo *bo)
{
	/* Unmap */
	dma_buf_unmap_attachment(bo->attachment, bo->sgt, DMA_BIDIRECTIONAL);

	/* Detach */
	dma_buf_detach(bo->dmabuf, bo->attachment);

	/* Release dmabuf reference */
	dma_buf_put(bo->dmabuf);

	kfree(bo);
}
```

## Synchronization and Fencing

DMA-BUF includes a synchronization mechanism called **fencing** to coordinate access.

### The Problem

```
Without synchronization:

Timeline:
T0: GPU starts writing to buffer
T1: Display controller starts reading ← RACE!
T2: GPU finishes writing
T3: Display shows corrupted frame

Problem: Display read buffer before GPU finished writing
```

### Fences (dma_fence)

```
With fences:

┌──────────────────────────────────────────────┐
│          DMA-BUF Reservation Object          │
│         (struct dma_resv)                    │
│                                              │
│  Write Fences:                               │
│  ┌────────────────────────────────┐          │
│  │ GPU Render Fence (pending)     │          │
│  └────────────────────────────────┘          │
│                                              │
│  Read Fences:                                │
│  ┌────────────────────────────────┐          │
│  │ Display Scan-out Fence (done)  │          │
│  └────────────────────────────────┘          │
└──────────────────────────────────────────────┘

Timeline with proper synchronization:
T0: GPU starts writing to buffer
    └─ Adds write fence to dmabuf
T1: Display wants to read
    └─ Waits for write fence
T2: GPU finishes, signals write fence
T3: Display fence triggered, starts reading
T4: Display finishes, signals read fence
```

### Fence Operations

```c
/* Each DMA-BUF has a reservation object */
struct dma_resv {
	struct ww_mutex lock;
	struct dma_fence __rcu *fences[];  /* Array of fences */
};

/* Fence represents async operation */
struct dma_fence {
	const struct dma_fence_ops *ops;
	spinlock_t *lock;
	u64 context;         /* Fence context */
	u64 seqno;           /* Sequence number */
	unsigned long flags; /* DMA_FENCE_FLAG_* */
	ktime_t timestamp;   /* When signaled */
};

/* Producer adds write fence */
dma_resv_lock(dmabuf->resv, NULL);
dma_resv_add_fence(dmabuf->resv, fence, DMA_RESV_USAGE_WRITE);
dma_resv_unlock(dmabuf->resv);

/* Consumer waits for fences */
dma_resv_wait_timeout(dmabuf->resv,
                      DMA_RESV_USAGE_WRITE,  /* Wait for writes */
                      true,                  /* Interruptible */
                      timeout);

/* Or get fences to wait on GPU */
struct dma_fence *fence;
dma_resv_lock(dmabuf->resv, NULL);
fence = dma_resv_get_singleton(dmabuf->resv, DMA_RESV_USAGE_WRITE);
if (fence) {
	/* Add fence to GPU command stream */
	gpu_add_fence_dependency(gpu_ctx, fence);
	dma_fence_put(fence);
}
dma_resv_unlock(dmabuf->resv);
```

### Implicit vs Explicit Sync

```
Implicit Synchronization:
- Kernel automatically manages fences
- Producer adds fence when starting work
- Consumer waits on fences before accessing
- Simpler for applications
- Less control

Example:
GPU writes to buffer → fence added automatically
Display imports buffer → waits on fence automatically

Explicit Synchronization:
- Application manages sync_file fds
- More control over dependencies
- Better for complex pipelines
- Used by Vulkan, modern graphics

Example:
GPU driver returns fence fd
App passes fence fd to display
Display waits on fence explicitly
```

## Real-World Use Cases

### Use Case 1: Camera → GPU → Display Pipeline

```
Video Preview Pipeline:

┌──────────────┐
│   Camera     │
│   Driver     │
└──────┬───────┘
       │ 1. Capture frame
       │    dmabuf = camera_alloc_buffer()
       │    fd = dma_buf_fd(dmabuf)
       ↓
┌──────────────┐
│ Application  │
│  (Camera)    │
└──────┬───────┘
       │ 2. Send fd to GPU process
       │    send_fd_via_unix_socket(fd)
       ↓
┌──────────────┐
│ Application  │
│  (Renderer)  │
└──────┬───────┘
       │ 3. Import to GPU
       │    gpu_import_dmabuf(fd)
       │    gpu_apply_filter(buf)  ← Zero-copy!
       │
       │ 4. Send fd to display
       │    send_to_compositor(fd)
       ↓
┌──────────────┐
│   Display    │
│  Compositor  │
└──────┬───────┘
       │ 5. Scan out
       │    display_import_dmabuf(fd)
       │    display_flip(buf)  ← Zero-copy!
       ↓
    [Screen]

Benefits:
- Zero copies (camera DMA → display scan-out)
- Low latency (no CPU involvement)
- Power efficient (no memcpy, less CPU usage)
```

### Use Case 2: Video Decode → Display

```
Video Playback:

┌─────────────────┐
│  Video Decoder  │
│   (Hardware)    │
└────────┬────────┘
         │ 1. Decode H.264/H.265
         │    Outputs to dmabuf
         │    fd = decoder_get_frame_fd()
         ↓
┌─────────────────┐
│  Media Player   │
│  Application    │
└────────┬────────┘
         │ 2. Optional: GPU post-processing
         │    gpu_import(fd)
         │    gpu_scale/deinterlace(buf)
         │
         │ 3. Present to display
         │    drm_atomic_add_fb(fd)
         ↓
┌─────────────────┐
│     Display     │
│   Controller    │
└─────────────────┘

The same physical memory used by:
- Video decoder (writes decoded frames)
- GPU (optional processing)
- Display controller (scans out to screen)

No CPU memcpy needed!
```

### Use Case 3: OpenGL/Vulkan → Wayland Compositor

```
Graphics Rendering:

┌─────────────────┐
│   Application   │
│   (Game/App)    │
└────────┬────────┘
         │ 1. Render with OpenGL/Vulkan
         │    EGLImage from dmabuf
         │    glTexImage2D(egl_image)
         │    Render scene...
         ↓
┌─────────────────┐
│   GPU Driver    │
│   (Mesa/etc)    │
└────────┬────────┘
         │ 2. Export rendered buffer
         │    fd = export_dmabuf()
         │
         │ 3. Send to compositor via Wayland
         │    wl_surface_attach(fd)
         ↓
┌─────────────────┐
│    Wayland      │
│   Compositor    │
└────────┬────────┘
         │ 4. Composite with other windows
         │    import_dmabuf(fd)
         │    gpu_composite(...)
         │
         │ 5. Display
         │    drm_plane_set_fb(fd)
         ↓
     [Screen]
```

### Use Case 4: Zero-Copy V4L2 (Video4Linux)

```c
/* Userspace code: Camera capture with zero-copy */

/* 1. Request buffers from camera driver */
struct v4l2_requestbuffers req = {
	.count = 4,
	.type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
	.memory = V4L2_MEMORY_DMABUF,  /* Use DMA-BUF */
};
ioctl(camera_fd, VIDIOC_REQBUFS, &req);

/* 2. Allocate buffers (could be from GPU, display, etc.) */
int dmabufs[4];
for (int i = 0; i < 4; i++) {
	dmabufs[i] = allocate_dmabuf(width * height * 4);
}

/* 3. Queue buffers to camera */
for (int i = 0; i < 4; i++) {
	struct v4l2_buffer buf = {
		.type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
		.memory = V4L2_MEMORY_DMABUF,
		.index = i,
		.m.fd = dmabufs[i],  /* Pass dmabuf fd */
	};
	ioctl(camera_fd, VIDIOC_QBUF, &buf);
}

/* 4. Start streaming */
int type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
ioctl(camera_fd, VIDIOC_STREAMON, &type);

/* 5. Capture frames */
while (capturing) {
	/* Dequeue filled buffer */
	struct v4l2_buffer buf = {
		.type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
		.memory = V4L2_MEMORY_DMABUF,
	};
	ioctl(camera_fd, VIDIOC_DQBUF, &buf);

	/* Buffer is now filled by camera
	 * Pass to GPU for processing - zero copy!
	 */
	process_on_gpu(dmabufs[buf.index]);

	/* Queue buffer back to camera */
	ioctl(camera_fd, VIDIOC_QBUF, &buf);
}
```

## Performance Considerations

### Memory Access Patterns

```
DMA-BUF access modes:

1. Device-Only Access (Best Performance)
   ┌────────┐        ┌────────┐
   │  GPU   │◄──────►│ Memory │
   └────────┘        └────────┘

   - No CPU access
   - Can use device-local memory
   - Best for ping-pong buffers

2. Device + CPU Read (Common)
   ┌────────┐        ┌────────────┐
   │  GPU   │───────►│   Memory   │
   │ (write)│        │            │
   └────────┘        │            │
   ┌────────┐        │            │
   │  CPU   │◄───────│  (cached)  │
   │ (read) │        └────────────┘
   └────────┘

   - GPU writes, CPU reads
   - Use cached memory
   - Need write-back cache

3. CPU Write + Device Read (Video)
   ┌────────┐        ┌────────────┐
   │  CPU   │───────►│   Memory   │
   │ (write)│        │            │
   └────────┘        │            │
   ┌────────┐        │            │
   │  GPU   │◄───────│(write-comb)│
   │ (read) │        └────────────┘
   └────────┘

   - CPU fills buffer
   - GPU consumes
   - Use write-combining

4. Shared Access (Slowest)
   ┌────────┐        ┌────────────┐
   │  GPU   │◄──────►│   Memory   │
   └────────┘        │            │
   ┌────────┐        │            │
   │  CPU   │◄──────►│ (uncached) │
   └────────┘        └────────────┘

   - Both read and write
   - Coherency overhead
   - Avoid if possible
```

### Cache Coherency

```
Problem: CPU and device caches can be inconsistent

CPU writes to buffer (cached):
┌─────────┐
│CPU Cache│ "new data"
└─────────┘
     ║ Not flushed yet
┌─────────┐
│ Memory  │ "old data"
└─────────┘

GPU reads (bypasses CPU cache):
┌─────────┐
│ Memory  │ "old data" ← GPU sees stale data!
└─────────┘

Solutions:

1. Cache Management (dma_sync):
   /* CPU wrote to buffer, make visible to device */
   dma_sync_sg_for_device(dev, sgt->sgl, sgt->nents, dir);

   /* Device wrote to buffer, make visible to CPU */
   dma_sync_sg_for_cpu(dev, sgt->sgl, sgt->nents, dir);

2. Uncached/Write-Combining Memory:
   /* Slower CPU access, but always coherent */
   pgprot_writecombine(vma->vm_page_prot);

3. Hardware Coherency (IOMMU snoop):
   /* IOMMU enforces coherency automatically */
   /* Best performance, not always available */
```

### Allocation Strategies

```
1. Contiguous Memory Allocator (CMA)
   - Large contiguous allocations
   - Reserved at boot
   - Good for devices without IOMMU

   pages = dma_alloc_coherent(dev, size, &dma_addr, GFP_KERNEL);

2. System Pages
   - Standard page allocator
   - Scatter-gather required
   - Works with IOMMU

   for (i = 0; i < nr_pages; i++)
       pages[i] = alloc_page(GFP_KERNEL);

3. Device Memory Heaps
   - GPU VRAM, carveouts
   - Best performance for device
   - May need CPU access via mmap

   /* Device-specific allocation */
   gpu_alloc_vram(size);
```

### Benchmarks

```
Typical Performance Numbers:

Operation                          Latency    Notes
────────────────────────────────────────────────────────
DMA-BUF export                     ~5 µs     One-time cost
DMA-BUF import + attach           ~10 µs     One-time cost
DMA mapping (4MB, IOMMU)          ~50 µs     Per map
SG table creation                  ~5 µs     Per map
Fence wait (already signaled)     ~100 ns    Fast path
Fence wait (pending)              varies     Depends on work

Memory Bandwidth:
────────────────────────────────────────────────────────
Memcpy (CPU)                      10 GB/s    Slow, avoid
DMA transfer                      50+ GB/s   Device-specific
Zero-copy (ideal)                 ∞          No transfer!

Memory Usage:
────────────────────────────────────────────────────────
Without DMA-BUF (copies):         4x        4 copies of data
With DMA-BUF (shared):            1x        Single copy
```

## Summary

DMA-BUF provides:

1. **Zero-Copy Buffer Sharing**: Multiple devices access same physical memory
2. **Device Independence**: Works across GPU, display, camera, video, etc.
3. **IOMMU Transparency**: Handles address translation automatically
4. **Synchronization**: Built-in fencing for safe concurrent access
5. **Flexibility**: File descriptor based, easy to pass between processes

### Key Concepts

- **Exporter**: Driver that allocates and owns the buffer
- **Importer**: Driver that uses a buffer allocated elsewhere
- **Scatter-Gather**: Describes potentially non-contiguous memory
- **Attachment**: Represents a device's connection to a DMA-BUF
- **Fence**: Synchronization primitive for ordering operations

### When to Use DMA-BUF

✓ **Use when:**
- Sharing buffers between devices (GPU, display, camera, etc.)
- Building zero-copy pipelines
- Need IOMMU-transparent memory sharing
- Want standardized synchronization

✗ **Don't use when:**
- Single device access only
- Small, temporary buffers
- Buffers don't leave kernel space
- Simple driver with direct control

## Code Examples and References

### Kernel Files

```
Core Implementation:
- drivers/dma-buf/dma-buf.c          - Core DMA-BUF framework
- drivers/dma-buf/dma-resv.c         - Reservation/fencing
- drivers/dma-buf/sync_file.c        - Explicit sync
- drivers/dma-buf/dma-heap.c         - DMA-BUF heaps

Headers:
- include/linux/dma-buf.h            - Core DMA-BUF API
- include/linux/dma-resv.h           - Reservation objects
- include/linux/dma-fence.h          - Fence API
- include/uapi/linux/dma-buf.h       - Userspace API

Example Exporters:
- drivers/gpu/drm/drm_prime.c        - DRM/GPU export
- drivers/media/common/videobuf2/    - V4L2 video
- drivers/dma-buf/udmabuf.c          - Userspace DMA-BUF

Example Importers:
- drivers/gpu/drm/*                  - Most DRM drivers
- drivers/media/platform/*           - Video devices
```

### Further Reading

- [Documentation/driver-api/dma-buf.rst](https://docs.kernel.org/driver-api/dma-buf.html) - Kernel documentation
- Linux DMA-BUF UAPI: [include/uapi/linux/dma-buf.h](https://github.com/torvalds/linux/blob/master/include/uapi/linux/dma-buf.h)
- DRM PRIME: Buffer sharing in graphics subsystem
- V4L2 DMA-BUF: Video4Linux integration
- Wayland: `linux-dmabuf` protocol

### Testing

```bash
# Check DMA-BUF usage
cat /sys/kernel/debug/dma_buf/bufinfo

# Example output:
Dma-buf Objects:
size    flags   mode    count   exp_name        ino
00040000    00000002    00080007    00000003    i915    00012345
        Attached Devices:
        i915
        display-controller

# Each DMA-BUF shows:
# - Size
# - Reference count
# - Exporter name
# - Attached devices
```

This demonstrates the power of DMA-BUF: A single buffer (00040000 = 256KB) is shared between three subsystems (i915 GPU, display controller, and likely userspace mmap) with zero copies.



# DMA-BUF Userland Memory Heaps

## What Are DMA-BUF Heaps?

**DMA-BUF Heaps** provide a standardized way for userspace applications to allocate DMA-BUF objects from specific memory pools **without needing a device driver**.

### The Problem They Solve

**Before DMA-BUF Heaps:**
```
App needs buffer → Opens GPU driver → GPU driver allocates DMA-BUF → Returns fd
                   ↑
            Requires device-specific driver
            Different APIs for different devices
```

**With DMA-BUF Heaps:**
```
App needs buffer → Opens /dev/dma_heap/system → Kernel allocates → Returns DMA-BUF fd
                   ↑
            Standardized API
            No device driver needed
            Works across frameworks
```

### Benefits

✅ **Framework-agnostic**: Works with graphics, video, camera, ML accelerators
✅ **Userspace control**: Applications choose allocation strategy
✅ **Zero-copy sharing**: Share buffers between subsystems efficiently
✅ **Standardized API**: Same interface for all heaps
✅ **No driver required**: Direct allocation from userspace

---

## Available Heaps

### 1. System Heap (`/dev/dma_heap/system`)

**Properties:**
- **Memory Type**: Virtually contiguous, cacheable
- **Physical Layout**: May be scattered (uses scatter-gather)
- **Performance**: Fast allocation, works with IOMMU
- **Availability**: Always present
- **Best For**: General-purpose buffer sharing

**Allocation Strategy:**
- Tries to allocate large pages first (1MB, 64KB) for IOMMU efficiency
- Falls back to 4KB pages if needed
- Orders: 256KB (order-8), 16KB (order-4), 4KB (order-0)

**Use Cases:**
- Graphics buffers (GPU rendering)
- Video codec buffers (V4L2, GStreamer)
- Camera capture buffers
- ML accelerator inputs/outputs
- Any multi-device buffer sharing

### 2. CMA Heap (`/dev/dma_heap/default_cma_region`)

**Properties:**
- **Memory Type**: Physically contiguous, cacheable
- **Physical Layout**: Single contiguous physical block
- **Performance**: Slower allocation (needs contiguous memory)
- **Availability**: Only if CMA configured
- **Best For**: Devices requiring physical contiguity

**Configuration:**
Enabled via:
- Kernel command line: `cma=256M`
- Device Tree: `linux,cma-default` property
- Kconfig: `CONFIG_CMA_SIZE_MBYTES=256`

**Use Cases:**
- Legacy devices without IOMMU/scatter-gather support
- Hardware requiring physically contiguous DMA
- Embedded systems with simple DMA controllers
- Display controllers needing contiguous framebuffers

### 3. Reserved Memory Heaps

**Properties:**
- **Memory Type**: Device-specific (often physically contiguous)
- **Physical Layout**: Fixed memory region
- **Performance**: Fast (pre-reserved)
- **Availability**: Device tree configuration
- **Best For**: Special hardware requirements

**Device Tree Example:**
```dts
reserved-memory {
    #address-cells = <1>;
    #size-cells = <1>;
    ranges;

    video_cma: video@42000000 {
        compatible = "shared-dma-pool";
        reusable;
        reg = <0x42000000 0x10000000>; /* 256 MB */
    };
};
```

Becomes: `/dev/dma_heap/video@42000000`

---

## Userspace API

### Header Files

```c
#include <linux/dma-heap.h>  /* struct dma_heap_allocation_data */
#include <linux/dma-buf.h>   /* DMA_BUF_IOCTL_SYNC */
#include <sys/ioctl.h>
#include <fcntl.h>
#include <sys/mman.h>
```

### Data Structures

```c
struct dma_heap_allocation_data {
    __u64 len;         /* Size to allocate (bytes) */
    __u32 fd;          /* Output: DMA-BUF file descriptor */
    __u32 fd_flags;    /* O_CLOEXEC, O_RDONLY, O_WRONLY, O_RDWR */
    __u64 heap_flags;  /* Reserved (must be 0) */
};
```

### Allocation Steps

**1. Open Heap Device**
```c
int heap_fd = open("/dev/dma_heap/system", O_RDONLY);
if (heap_fd < 0) {
    perror("Failed to open heap");
    return -1;
}
```

**2. Allocate Buffer**
```c
struct dma_heap_allocation_data heap_data = {
    .len = 4096 * 1024,         /* 4 MB */
    .fd = 0,
    .fd_flags = O_RDWR | O_CLOEXEC,
    .heap_flags = 0
};

int ret = ioctl(heap_fd, DMA_HEAP_IOCTL_ALLOC, &heap_data);
if (ret < 0) {
    perror("Allocation failed");
    close(heap_fd);
    return -1;
}

int dmabuf_fd = heap_data.fd;  /* This is your DMA-BUF! */
close(heap_fd);  /* No longer needed */
```

**3. Map for CPU Access**
```c
void *ptr = mmap(NULL, heap_data.len, PROT_READ | PROT_WRITE,
                 MAP_SHARED, dmabuf_fd, 0);
if (ptr == MAP_FAILED) {
    perror("mmap failed");
    close(dmabuf_fd);
    return -1;
}
```

**4. Use the Buffer**
```c
/* Before CPU access */
struct dma_buf_sync sync = { .flags = DMA_BUF_SYNC_START | DMA_BUF_SYNC_RW };
ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);

/* Write data */
memcpy(ptr, data, size);

/* After CPU access */
sync.flags = DMA_BUF_SYNC_END | DMA_BUF_SYNC_RW;
ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);
```

**5. Share with Devices**
```c
/* Pass dmabuf_fd to GPU, video codec, camera, etc. */
pass_to_gpu(dmabuf_fd);
pass_to_video_decoder(dmabuf_fd);
pass_to_camera(dmabuf_fd);
```

**6. Cleanup**
```c
munmap(ptr, heap_data.len);
close(dmabuf_fd);  /* Frees when all users done */
```

---

## Kernel Implementation

### Core Framework

**Location**: `drivers/dma-buf/dma-heap.c`

**Key Functions**:
- `dma_heap_add()` - Register new heap
- `dma_heap_buffer_alloc()` - Allocate from heap
- Device nodes: `/dev/dma_heap/<name>`

### System Heap Implementation

**Location**: `drivers/dma-buf/heaps/system_heap.c`

**Allocation Algorithm**:
```c
static const unsigned int orders[] = {8, 4, 0};  /* 1MB, 64KB, 4KB */

while (size_remaining > 0) {
    page = alloc_largest_available(size_remaining, max_order);
    /* Tries order-8 (256KB), order-4 (16KB), order-0 (4KB) */
    list_add_tail(&page->lru, &pages);
    size_remaining -= page_size(page);
}
```

**Why Large Pages?**
- Reduces IOMMU TLB pressure
- Fewer page table updates
- Better DMA performance
- Still virtually contiguous for CPU

### CMA Heap Implementation

**Location**: `drivers/dma-buf/heaps/cma_heap.c`

**Allocation**:
```c
cma_pages = cma_alloc(cma, pagecount, align, GFP_KERNEL);
/* Returns physically contiguous pages from CMA region */
```

---

## Checking Available Heaps

### System Check
```bash
# List all heaps
ls -l /dev/dma_heap/

# Common outputs:
# /dev/dma_heap/system             (always present)
# /dev/dma_heap/default_cma_region (if CMA configured)
# /dev/dma_heap/video@42000000     (if device tree regions)
```

### Your System
```bash
$ ls -la /dev/dma_heap/
crw------- 1 root root 250, 0 Dec 25 14:18 system
```

**You have**: System heap only (standard configuration)

### Kernel Configuration
```bash
# Check if heaps enabled
grep DMA_HEAP /boot/config-$(uname -r)

# Should show:
CONFIG_DMA_SHARED_BUFFER=y
CONFIG_DMA_HEAP=y
CONFIG_DMA_HEAP_SYSTEM=y
CONFIG_DMA_HEAP_CMA=y  # (if CMA heap available)
```

---

## Real-World Use Cases

### 1. Graphics Pipeline (GPU + Display)

```c
/* Allocate buffer from heap */
int dmabuf_fd = allocate_from_heap("/dev/dma_heap/system", 1920*1080*4);

/* GPU writes rendered frame */
submit_to_gpu(dmabuf_fd);

/* Display controller scans out */
display_buffer(dmabuf_fd);

/* Zero-copy! Same buffer, no memcpy */
```

### 2. Video Decode Pipeline

```c
/* Allocate buffer */
int dmabuf_fd = allocate_from_heap("/dev/dma_heap/system", 4*1024*1024);

/* Video decoder writes decoded frame */
v4l2_queue_buffer(decoder_fd, dmabuf_fd);

/* Neural network processes frame */
ml_accelerator_process(dmabuf_fd);

/* Display the result */
display_buffer(dmabuf_fd);
```

### 3. Camera Capture

```c
/* Allocate capture buffers from heap */
for (int i = 0; i < NUM_BUFFERS; i++) {
    int dmabuf_fd = allocate_from_heap("/dev/dma_heap/system", buffer_size);
    camera_queue_buffer(dmabuf_fd);
}

/* Camera writes directly to DMA-BUF */
/* Application can map and read when ready */
```

### 4. Cross-Process Sharing

```c
/* Process A: Allocate and fill buffer */
int dmabuf_fd = allocate_from_heap("/dev/dma_heap/system", size);
fill_buffer(dmabuf_fd);

/* Send fd to Process B via Unix socket */
send_fd_over_socket(socket_fd, dmabuf_fd);

/* Process B: Receive and use */
int received_fd = receive_fd_over_socket(socket_fd);
process_buffer(received_fd);

/* Zero-copy cross-process sharing! */
```

---

## Performance Characteristics

### System Heap

| Allocation Size | Time | Physical Layout | IOMMU Required |
|-----------------|------|-----------------|----------------|
| 4 KB | ~10 μs | 1 page | No |
| 64 KB | ~20 μs | ~4 pages | Preferred |
| 1 MB | ~50 μs | ~16-256 pages | Preferred |
| 16 MB | ~200 μs | Scattered | Preferred |

**Best For:**
- Modern systems with IOMMU
- Devices supporting scatter-gather DMA
- General-purpose allocations

### CMA Heap

| Allocation Size | Time | Physical Layout | IOMMU Required |
|-----------------|------|-----------------|----------------|
| 4 KB | ~50 μs | Contiguous | No |
| 64 KB | ~100 μs | Contiguous | No |
| 1 MB | ~500 μs | Contiguous | No |
| 16 MB | ~2-5 ms | Contiguous | No |

**Best For:**
- Legacy devices without IOMMU
- Hardware requiring physical contiguity
- Embedded systems

---

## Comparison Matrix

| Feature | System Heap | CMA Heap | Device-Specific |
|---------|-------------|----------|-----------------|
| **Availability** | Always | If CMA enabled | Device tree |
| **Physical Layout** | Scattered | Contiguous | Varies |
| **IOMMU Required** | Preferred | No | No |
| **Allocation Speed** | Fast | Slower | Very fast |
| **Memory Efficiency** | High | Medium | Fixed pool |
| **Use Case** | Modern devices | Legacy devices | Special hardware |
| **Typical Size Limit** | GB+ | Hundreds of MB | Fixed region |

---

## Common Mistakes

### ❌ Mistake 1: Forgetting DMA_BUF_IOCTL_SYNC

```c
/* WRONG: Direct CPU access without sync */
void *ptr = mmap(..., dmabuf_fd, ...);
memcpy(ptr, data, size);  /* Cache issues! */
```

```c
/* CORRECT: Always sync before/after CPU access */
struct dma_buf_sync sync = { .flags = DMA_BUF_SYNC_START | DMA_BUF_SYNC_WRITE };
ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);
memcpy(ptr, data, size);
sync.flags = DMA_BUF_SYNC_END | DMA_BUF_SYNC_WRITE;
ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);
```

### ❌ Mistake 2: Not Checking Return Values

```c
/* WRONG */
int heap_fd = open("/dev/dma_heap/cma", O_RDONLY);
ioctl(heap_fd, DMA_HEAP_IOCTL_ALLOC, &heap_data);  /* May fail! */
```

```c
/* CORRECT */
int heap_fd = open("/dev/dma_heap/system", O_RDONLY);
if (heap_fd < 0) {
    perror("Heap not available, try fallback");
    heap_fd = open("/dev/dma_heap/system", O_RDONLY);
}
```

### ❌ Mistake 3: Closing Heap FD Too Early

```c
/* WRONG */
int heap_fd = open("/dev/dma_heap/system", O_RDONLY);
ioctl(heap_fd, DMA_HEAP_IOCTL_ALLOC, &heap_data);
close(heap_fd);  /* ← CORRECT: Heap fd independent of dmabuf fd */

int dmabuf_fd = heap_data.fd;
/* Use dmabuf_fd... this is fine! */
```

**Note**: Heap fd and dmabuf fd are independent. Close heap fd after allocation.

---

## Android Integration

DMA-BUF heaps are heavily used in Android:

```java
// Android BufferQueue uses DMA-BUF heaps
GraphicBuffer buffer = new GraphicBuffer(...);

// Internally allocates from:
// /dev/dma_heap/system (modern)
// /dev/dma_heap/system-uncached (special)
```

**Android Heap Names:**
- `system` - General buffers
- `system-uncached` - Uncached buffers for specific hardware

---

## Debugging

### Enable Debug Messages
```bash
# Dynamic debug
echo 'module dma_heap +p' > /sys/kernel/debug/dynamic_debug/control
echo 'module system_heap +p' > /sys/kernel/debug/dynamic_debug/control

# View allocation messages
dmesg | grep dma_heap
```

### Check Allocation Stats
```bash
# DMA-BUF stats (if enabled)
cat /sys/kernel/debug/dma_buf/bufinfo

# Shows:
# - Buffer sizes
# - Exporters
# - Attachments
```

### Trace Allocations
```bash
# Use ftrace
echo 1 > /sys/kernel/debug/tracing/events/dma_buf/enable
cat /sys/kernel/debug/tracing/trace
```

---

## Summary

### Quick Facts

**What**: Userspace API to allocate DMA-BUF from specific memory pools
**Since**: Linux 5.6 (2020)
**Location**: `/dev/dma_heap/<name>`
**API**: Single ioctl: `DMA_HEAP_IOCTL_ALLOC`

### Common Heaps

| Heap | Path | Physical | Use Case |
|------|------|----------|----------|
| System | `/dev/dma_heap/system` | Scattered | General purpose |
| CMA | `/dev/dma_heap/default_cma_region` | Contiguous | Legacy devices |
| Custom | `/dev/dma_heap/<dt-name>` | Varies | Special hardware |

### Workflow

```
1. Open /dev/dma_heap/<name>
2. ioctl(DMA_HEAP_IOCTL_ALLOC)
3. Get DMA-BUF fd
4. Share fd with devices
5. mmap() for CPU access (with sync!)
6. close() when done
```

### Key Advantages

✅ No device driver needed
✅ Framework-agnostic sharing
✅ Standardized API
✅ Zero-copy between subsystems
✅ Userspace control over allocation

**Bottom Line**: DMA-BUF heaps are the modern way to allocate shareable buffers in Linux userspace!


# [RFC v2 00/11] Add dmabuf read/write via io_uring

[[RFC v2 00/11] Add dmabuf read/write via io_uring](https://lore.kernel.org/linux-block/cover.1763725387.git.asml.silence@gmail.com/)

## Overview from AI

```
This patchset introduces support for performing read and write operations on 
dmabuf file descriptors through io_uring. This allows for efficient, zero-copy 
data transfers between devices by enabling asynchronous I/O on shared dmabuf buffers.

The key changes in this patchset are: 
 - dmabuf Integration with io_uring:
   It adds the necessary infrastructure to io_uring to handle dmabuf buffers, 
   including the management of DMA addresses and dynamic buffer attachments. 

 - Block Layer Modifications: 
   The core logic is integrated into the blk-mq subsystem to support asynchronous 
   DMA mapping and request cancellation.

 - User-space API: The existing io_uring user-space API remains unchanged. A dmabuf 
   can be registered and used like a normal I/O buffer.

Overall, this patchset aims to improve performance and reduce latency for 
applications that rely on dmabuf for data sharing between devices by leveraging the 
asynchronous capabilities of io_uring. The patchset is currently in the RFC (Request for 
Comments) stage, with the author planning future simplifications to reduce complexity.

```

## Here’s a breakdown of how it works:

```
1. The Core Problem: The CPU as a Data Mover

Traditionally, when you want to move data from one device (like a GPU) to another (like an NVMe SSD), the data often has to take a
detour through the main system memory (RAM) and be copied by the CPU.

A typical, inefficient workflow looks like this:

 1. A GPU finishes rendering a frame into a memory buffer.
 2. Your application wants to save this frame to a fast NVMe drive.
 3. The application issues a write() system call.
 4. The kernel instructs the CPU to execute a memcpy-like operation, copying the data from the GPU's buffer into the kernel's page
    cache (a temporary buffer in RAM).
 5. Finally, the kernel instructs the NVMe drive to fetch the data from the page cache and write it to storage.

The bottleneck here is Step 4. The CPU spends valuable cycles just copying data from one place in memory to another. This is slow,
consumes CPU resources that could be used for other tasks, and adds latency.

2. The Solution: A Direct, CPU-less Path

This patchset creates a "fast path" that bypasses the CPU-driven copy. It achieves this in two key ways:

a) dmabuf for Zero-Copy Transfers

A dmabuf (DMA buffer) is a kernel mechanism for sharing a buffer of memory between multiple devices without involving the CPU.
When a device (like a GPU) creates a dmabuf, it's not just a block of data; it's a handle that describes where the data lives in
physical memory.

Other drivers (like a storage driver) can "import" this dmabuf and get the physical memory addresses directly. This allows them to
set up Direct Memory Access (DMA) transfers, where the hardware moves data directly from the source to the destination without the
CPU's intervention. This is the principle of "zero-copy."

b) io_uring for Asynchronous, Low-Overhead I/O

io_uring is a high-performance interface for asynchronous I/O. Its key advantages are:
 * Reduced System Calls: Instead of one system call per I/O operation (like read() or write()), you can submit hundreds of
   operations with a single system call. This dramatically reduces the overhead of switching between user-space and kernel-space.
 * True Asynchronicity: Your application can submit a batch of I/O requests and immediately go back to doing other work. The
   kernel processes them in the background and notifies the application only when they are complete.

How the Patchset Combines Them for Maximum Performance

This patchset teaches io_uring and the block layer how to speak the language of dmabuf.

The new, highly efficient workflow is:

 1. A GPU renders a frame into a dmabuf.
 2. The application registers this dmabuf with io_uring.
 3. The application submits a write operation to io_uring, telling it to write the contents of the dmabuf to the NVMe drive.
 4. Here's the magic:
     * io_uring passes the request to the block layer.
     * The NVMe driver, using the new logic from the patchset, imports the dmabuf.
     * The driver gets the list of physical memory pages for the buffer directly.
     * The driver programs the NVMe controller's DMA engine to pull the data straight from the GPU's memory buffer and write it to
       the SSD.

Summary of Performance Wins:

 * Improved Performance/Throughput: By eliminating the memcpy step, the CPU is freed up. It can be used for application logic
   (e.g., preparing the next frame) while the DMA hardware handles the data transfer in parallel. This allows for a much higher
   rate of data movement.
 * Reduced Latency: The time for a single write operation is significantly lower because you remove the entire data-copying step
   from the critical path. The request is submitted with minimal overhead via io_uring, and the hardware handles the rest.

In essence, this patchset creates a direct pipeline between device memory and storage, orchestrated by the highly efficient
io_uring interface, cutting the slow CPU out of the data path.

```

