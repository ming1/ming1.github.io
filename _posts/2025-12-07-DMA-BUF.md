---
title: DMA-BUF
category: tech
tags: [driver, DMA-BUF, zero copy, linux kernel]
---

title: DMA-BUF

* TOC
{:toc}


# DMA-BUF Subsystem Complete Walkthrough

Generated by AI.

This document provides a comprehensive guide to Linux's DMA-BUF (DMA Buffer Sharing) framework.

## Table of Contents
1. [What is DMA-BUF?](#what-is-dmabuf)
2. [The Problem DMA-BUF Solves](#the-problem-dmabuf-solves)
3. [Architecture Overview](#architecture-overview)
4. [Core Concepts](#core-concepts)
5. [DMA-BUF Operations Flow](#dmabuf-operations-flow)
6. [Kernel Implementation Details](#kernel-implementation-details)
7. [Synchronization and Fencing](#synchronization-and-fencing)
8. [Real-World Use Cases](#real-world-use-cases)
9. [Code Examples](#code-examples)
10. [Performance Considerations](#performance-considerations)

## What is DMA-BUF?

**DMA-BUF** is a Linux kernel framework for sharing buffers between different devices and subsystems without copying data.

### Key Features

- **Zero-Copy Sharing**: Multiple devices access the same physical memory
- **Device-Agnostic**: Works across GPU, Display, Camera, Video, etc.
- **IOMMU-Aware**: Handles address translation transparently
- **Synchronization**: Built-in fencing mechanism for access coordination
- **File Descriptor Based**: Buffers are represented as file descriptors

### Primary Use Cases

1. **GPU ↔ Display**: Render to GPU buffer, scan out from display controller
2. **Camera ↔ GPU**: Capture to camera buffer, process on GPU
3. **Video Decode ↔ Display**: Decode video, display directly
4. **Cross-Process Sharing**: Share buffers between processes via fd passing

## The Problem DMA-BUF Solves

### Before DMA-BUF: The Copy Problem

```
Traditional approach - WASTEFUL:

┌──────────────┐
│   Camera     │ Capture image
│   Driver     │
└──────┬───────┘
       │ DMA to camera buffer
       ↓
┌─────────────────────┐
│  Camera Buffer      │ (Physical memory region A)
│  [Image Data]       │
└──────┬──────────────┘
       │ memcpy() ← EXPENSIVE!
       ↓
┌─────────────────────┐
│  Application Buffer │ (Physical memory region B)
│  [Image Data COPY]  │
└──────┬──────────────┘
       │ memcpy() ← EXPENSIVE AGAIN!
       ↓
┌─────────────────────┐
│  GPU Buffer         │ (Physical memory region C)
│  [Image Data COPY]  │
└──────┬──────────────┘
       │ GPU processes
       ↓
┌─────────────────────┐
│  Display Buffer     │ (Physical memory region D)
│  [Image Data COPY]  │ ← memcpy() AGAIN!
└─────────────────────┘

Problems:
- 3 memory copies (camera → app → GPU → display)
- 4x memory usage
- CPU cycles wasted on memcpy
- Cache pollution
- Bandwidth consumption
- Increased latency
```

### With DMA-BUF: Zero-Copy Sharing

```
DMA-BUF approach - EFFICIENT:

┌──────────────┐
│   Camera     │ Allocates and exports DMA-BUF
│   Driver     │ fd = camera_export_dmabuf()
└──────┬───────┘
       │
       ↓
┌─────────────────────────────────────────┐
│         DMA-BUF (fd=50)                 │
│     [Physical Memory - ONE copy]        │
│                                         │
│  ┌────────────────────────────────┐    │
│  │  Actual buffer data             │    │
│  │  [Image: 1920x1080 RGB]         │    │
│  └────────────────────────────────┘    │
└─────────────────────────────────────────┘
       ↑         ↑         ↑         ↑
       │         │         │         │
    Camera     App      GPU     Display
   (owner)   (import) (import) (import)

Each device:
- Gets scatter-gather table for the SAME memory
- Maps into its own address space (CPU VA, GPU VA, etc.)
- NO data copying!

Benefits:
- 1x memory usage (shared)
- No CPU memcpy overhead
- Zero-copy pipeline
- Much lower latency
- Bandwidth saved for real work
```

## Architecture Overview

### High-Level Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                      Userspace                              │
│                                                             │
│  ┌──────────┐        ┌──────────┐        ┌──────────┐    │
│  │  Camera  │        │   GPU    │        │ Display  │    │
│  │  App     │        │  Process │        │  Manager │    │
│  └────┬─────┘        └────┬─────┘        └────┬─────┘    │
│       │ fd=50             │ fd=50             │ fd=50     │
│       │ (export)          │ (import)          │ (import)  │
└───────┼───────────────────┼───────────────────┼───────────┘
        │                   │                   │
   ═════════════════════════════════════════════════════
             Kernel DMA-BUF Framework
   ═════════════════════════════════════════════════════
        │                   │                   │
┌───────┼───────────────────┼───────────────────┼───────────┐
│       ↓                   ↓                   ↓           │
│  ┌─────────┐         ┌─────────┐        ┌─────────┐     │
│  │ Camera  │         │   GPU   │        │ Display │     │
│  │ Driver  │         │  Driver │        │ Driver  │     │
│  │(export) │         │(import) │        │(import) │     │
│  └────┬────┘         └────┬────┘        └────┬────┘     │
│       │                   │                   │           │
│       │    DMA-BUF Core (drivers/dma-buf/)    │           │
│       │         - Reference counting          │           │
│       │         - Attachment management        │           │
│       │         - Fence synchronization        │           │
│       └───────────────────┼───────────────────┘           │
│                           │                               │
│                  ┌────────┴────────┐                      │
│                  │  DMA-BUF Object │                      │
│                  │  struct dma_buf │                      │
│                  └────────┬────────┘                      │
│                           │                               │
│                  ┌────────┴────────┐                      │
│                  │ Physical Memory │                      │
│                  │  (pages/CMA)    │                      │
│                  └─────────────────┘                      │
└─────────────────────────────────────────────────────────────┘
```

### Key Components

```
1. struct dma_buf
   - The central object representing a shared buffer
   - Reference counted
   - Contains operations callbacks (dma_buf_ops)

2. struct dma_buf_ops
   - Exporter-provided operations
   - attach/detach: Device attachment
   - map_dma_buf/unmap_dma_buf: DMA mapping
   - mmap: CPU mapping
   - release: Cleanup

3. struct dma_buf_attachment
   - Represents a device's attachment to a DMA-BUF
   - One per importing device
   - Stores device-specific info

4. struct sg_table (scatter-gather table)
   - Describes buffer's physical memory layout
   - List of <physical address, length> pairs
   - Used for DMA operations

5. struct dma_fence
   - Synchronization primitive
   - Signals when operations complete
   - Used for producer-consumer coordination
```

## Core Concepts

### 1. Exporter vs Importer

```
Exporter (Producer):
┌──────────────────────────────┐
│  Camera Driver               │
│                              │
│  1. Allocates physical mem   │
│  2. Creates dma_buf          │
│  3. Provides dma_buf_ops     │
│  4. Returns fd to userspace  │
└──────────────────────────────┘

Importer (Consumer):
┌──────────────────────────────┐
│  GPU Driver                  │
│                              │
│  1. Receives fd from user    │
│  2. Gets dma_buf from fd     │
│  3. Attaches device          │
│  4. Maps for DMA access      │
│  5. Performs operations      │
│  6. Unmaps and detaches      │
└──────────────────────────────┘

One Exporter, Multiple Importers:
        ┌─────────────┐
        │  Exporter   │
        │  (Camera)   │
        └──────┬──────┘
               │ Owns buffer
               ↓
        ┌──────────────┐
        │   dma_buf    │
        └──┬────┬────┬─┘
           │    │    │
     ┌─────┘    │    └─────┐
     ↓          ↓          ↓
┌─────────┐ ┌─────────┐ ┌─────────┐
│ GPU     │ │ Display │ │ Video   │
│(import) │ │(import) │ │(import) │
└─────────┘ └─────────┘ └─────────┘
```

### 2. Scatter-Gather Tables

Physical memory isn't always contiguous:

```
Virtual Buffer View (contiguous):
┌─────────────────────────────────────┐
│ 0x0000  [  4KB data  ]              │
│ 0x1000  [  4KB data  ]              │
│ 0x2000  [  4KB data  ]              │
│ 0x3000  [  4KB data  ]              │
└─────────────────────────────────────┘

Physical Memory (scattered):
Physical Address    Length
─────────────────────────────
0x10000000      →   4KB    (page 0)
0x20000000      →   4KB    (page 1)  ← Not contiguous!
0x15000000      →   4KB    (page 2)  ← Scattered
0x18000000      →   4KB    (page 3)

Scatter-Gather Table:
struct scatterlist entries[] = {
    { .dma_address = 0x10000000, .length = 4096 },
    { .dma_address = 0x20000000, .length = 4096 },
    { .dma_address = 0x15000000, .length = 4096 },
    { .dma_address = 0x18000000, .length = 4096 },
};

Device reads via SG table:
- Device DMA engine reads SG table
- For each entry, DMAs from physical address
- Assembles contiguous stream for device
```

### 3. DMA Mapping and IOMMU

DMA-BUF handles IOMMU transparently:

```
Without IOMMU:
Device sees physical addresses directly

┌──────────┐
│  Device  │
└────┬─────┘
     │ DMA: read from 0x10000000
     ↓
┌──────────────────┐
│ Physical Memory  │
│ 0x10000000: data │
└──────────────────┘

With IOMMU:
Device uses I/O virtual addresses (IOVA)

┌──────────┐
│  Device  │
└────┬─────┘
     │ DMA: read from IOVA 0x5000
     ↓
┌────────────────┐
│     IOMMU      │
│  Translate:    │
│  0x5000 →      │
│  0x10000000    │
└────┬───────────┘
     │ Physical: 0x10000000
     ↓
┌──────────────────┐
│ Physical Memory  │
│ 0x10000000: data │
└──────────────────┘

DMA-BUF Handles This:
- dma_map_sgtable() automatically:
  * Checks if IOMMU present
  * If yes: creates IOMMU mappings
  * If no: uses physical addresses
- Importer doesn't need to care!
```

## DMA-BUF Operations Flow

### Export Flow

```
Step-by-step buffer export:

1. Driver Allocates Memory
   ┌──────────────────────────┐
   │ pages = alloc_pages()    │
   │ or CMA allocation        │
   │ or from reserved pool    │
   └────────┬─────────────────┘
            ↓

2. Create DMA-BUF Export Info
   ┌────────────────────────────────┐
   │ DEFINE_DMA_BUF_EXPORT_INFO()   │
   │   .ops = &my_dmabuf_ops        │
   │   .size = buffer_size          │
   │   .flags = O_RDWR              │
   │   .priv = driver_private_data  │
   └────────┬───────────────────────┘
            ↓

3. Export DMA-BUF
   ┌────────────────────────────────┐
   │ dmabuf = dma_buf_export(&exp)  │
   │                                │
   │ Kernel creates:                │
   │  - struct dma_buf              │
   │  - struct file                 │
   │  - Initializes refcount        │
   └────────┬───────────────────────┘
            ↓

4. Get File Descriptor
   ┌────────────────────────────────┐
   │ fd = dma_buf_fd(dmabuf, flags) │
   │                                │
   │ Installs fd in current         │
   │ process's file descriptor      │
   │ table                          │
   └────────┬───────────────────────┘
            ↓

5. Return fd to Userspace
   ┌────────────────────────────────┐
   │ return fd to application       │
   │                                │
   │ App can now:                   │
   │  - Share fd with other process │
   │  - Pass to other drivers       │
   │  - mmap() for CPU access       │
   └────────────────────────────────┘
```

### Import and Use Flow

```
Application shares fd=50 with GPU process

1. Get DMA-BUF from FD
   ┌────────────────────────────────┐
   │ GPU Process / Driver:          │
   │                                │
   │ dmabuf = dma_buf_get(fd)       │
   │                                │
   │ Kernel:                        │
   │  - Looks up fd in file table   │
   │  - Verifies it's a dma_buf     │
   │  - Increments refcount         │
   │  - Returns dma_buf pointer     │
   └────────┬───────────────────────┘
            ↓

2. Attach Device
   ┌────────────────────────────────┐
   │ attach = dma_buf_attach(       │
   │            dmabuf, gpu_dev)    │
   │                                │
   │ Kernel:                        │
   │  - Allocates dma_buf_attachment│
   │  - Calls exporter's attach()   │
   │  - Links attachment to dmabuf  │
   └────────┬───────────────────────┘
            ↓

3. Map for DMA
   ┌────────────────────────────────┐
   │ sgt = dma_buf_map_attachment(  │
   │         attach, DMA_TO_DEVICE) │
   │                                │
   │ Exporter's map_dma_buf():      │
   │  - Builds scatter-gather table │
   │  - Calls dma_map_sgtable()     │
   │  - IOMMU mapping if present    │
   │  - Returns sg_table            │
   └────────┬───────────────────────┘
            ↓

4. Use Buffer (DMA)
   ┌────────────────────────────────┐
   │ GPU Driver:                    │
   │                                │
   │ for_each_sgtable_dma_sg(sgt) { │
   │   dma_addr = sg_dma_address(sg)│
   │   dma_len  = sg_dma_len(sg)    │
   │                                │
   │   /* Program GPU */            │
   │   gpu_set_src_addr(dma_addr)   │
   │   gpu_process(dma_len)         │
   │ }                              │
   └────────┬───────────────────────┘
            ↓

5. Unmap
   ┌────────────────────────────────┐
   │ dma_buf_unmap_attachment(      │
   │    attach, sgt, DMA_TO_DEVICE) │
   │                                │
   │ Exporter's unmap_dma_buf():    │
   │  - Calls dma_unmap_sgtable()   │
   │  - Removes IOMMU mappings      │
   │  - Frees sg_table              │
   └────────┬───────────────────────┘
            ↓

6. Detach Device
   ┌────────────────────────────────┐
   │ dma_buf_detach(dmabuf, attach) │
   │                                │
   │ Kernel:                        │
   │  - Calls exporter's detach()   │
   │  - Frees dma_buf_attachment    │
   └────────┬───────────────────────┘
            ↓

7. Release DMA-BUF
   ┌────────────────────────────────┐
   │ dma_buf_put(dmabuf)            │
   │                                │
   │ Kernel:                        │
   │  - Decrements refcount         │
   │  - If refcount = 0:            │
   │    * Calls exporter release()  │
   │    * Frees dma_buf structure   │
   │    * Frees underlying memory   │
   └────────────────────────────────┘
```

## Kernel Implementation Details

### Core Data Structures

```c
/* The central DMA-BUF object */
struct dma_buf {
	size_t size;                    /* Size of buffer */
	struct file *file;              /* Associated file */
	struct list_head attachments;   /* List of attached devices */
	const struct dma_buf_ops *ops;  /* Exporter operations */
	void *priv;                     /* Exporter private data */

	/* Resv for synchronization */
	struct dma_resv *resv;

	/* Reference counting via file->f_count */
};

/* Operations provided by exporter */
struct dma_buf_ops {
	/* Called when a device attaches */
	int (*attach)(struct dma_buf *, struct dma_buf_attachment *);

	/* Called when a device detaches */
	void (*detach)(struct dma_buf *, struct dma_buf_attachment *);

	/* Map for DMA - returns scatter-gather table */
	struct sg_table * (*map_dma_buf)(struct dma_buf_attachment *,
	                                  enum dma_data_direction);

	/* Unmap DMA */
	void (*unmap_dma_buf)(struct dma_buf_attachment *,
	                      struct sg_table *,
	                      enum dma_data_direction);

	/* Release buffer (refcount = 0) */
	void (*release)(struct dma_buf *);

	/* Map into CPU address space */
	int (*mmap)(struct dma_buf *, struct vm_area_struct *vma);

	/* CPU kernel mapping */
	void *(*vmap)(struct dma_buf *);
	void (*vunmap)(struct dma_buf *, void *vaddr);
};

/* Device attachment to a DMA-BUF */
struct dma_buf_attachment {
	struct dma_buf *dmabuf;         /* The DMA-BUF */
	struct device *dev;             /* Attached device */
	struct list_head node;          /* In dmabuf->attachments */
	void *priv;                     /* Importer private data */
	enum dma_data_direction dir;    /* DMA direction hint */
};

/* Scatter-gather table */
struct sg_table {
	struct scatterlist *sgl;        /* Array of SG entries */
	unsigned int nents;             /* Number of entries */
	unsigned int orig_nents;        /* Original number */
};

struct scatterlist {
	unsigned long page_link;        /* Page pointer */
	unsigned int offset;            /* Offset in page */
	unsigned int length;            /* Length in bytes */
	dma_addr_t dma_address;         /* DMA address (after mapping) */
	unsigned int dma_length;        /* DMA length (may differ) */
};
```

### Exporter Implementation (Detailed)

```c
/* Example: Simple page-based exporter */

struct my_buffer {
	struct page **pages;
	int nr_pages;
	size_t size;
};

/* Attach callback */
static int my_attach(struct dma_buf *dmabuf,
                     struct dma_buf_attachment *attach)
{
	struct my_buffer *buf = dmabuf->priv;

	/* Exporter can:
	 * - Check device capabilities
	 * - Allocate attachment-specific data
	 * - Pin pages if needed
	 */

	pr_debug("Device %s attached\n", dev_name(attach->dev));
	return 0;
}

/* Map for DMA */
static struct sg_table *my_map_dma_buf(struct dma_buf_attachment *attach,
                                        enum dma_data_direction dir)
{
	struct my_buffer *buf = attach->dmabuf->priv;
	struct sg_table *sgt;
	int ret;

	/* Allocate SG table */
	sgt = kzalloc(sizeof(*sgt), GFP_KERNEL);
	if (!sgt)
		return ERR_PTR(-ENOMEM);

	/* Build SG table from pages
	 * This creates scatter-gather entries for the buffer
	 */
	ret = sg_alloc_table_from_pages(sgt,
	                                 buf->pages,
	                                 buf->nr_pages,
	                                 0,          /* offset in first page */
	                                 buf->size,  /* total size */
	                                 GFP_KERNEL);
	if (ret) {
		kfree(sgt);
		return ERR_PTR(ret);
	}

	/* Map for DMA
	 * This handles:
	 * - Cache flushing if needed
	 * - IOMMU mapping if present
	 * - Fills in dma_address fields in scatterlist
	 */
	ret = dma_map_sgtable(attach->dev, sgt, dir, 0);
	if (ret) {
		sg_free_table(sgt);
		kfree(sgt);
		return ERR_PTR(-ENOMEM);
	}

	return sgt;
}

/* Unmap DMA */
static void my_unmap_dma_buf(struct dma_buf_attachment *attach,
                             struct sg_table *sgt,
                             enum dma_data_direction dir)
{
	/* Unmap from device
	 * - Removes IOMMU mappings
	 * - Cache operations if needed
	 */
	dma_unmap_sgtable(attach->dev, sgt, dir, 0);

	/* Free SG table */
	sg_free_table(sgt);
	kfree(sgt);
}

/* Release buffer */
static void my_release(struct dma_buf *dmabuf)
{
	struct my_buffer *buf = dmabuf->priv;
	int i;

	/* Free all pages */
	for (i = 0; i < buf->nr_pages; i++)
		__free_page(buf->pages[i]);

	kfree(buf->pages);
	kfree(buf);
}

/* CPU mmap */
static int my_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)
{
	struct my_buffer *buf = dmabuf->priv;
	unsigned long addr = vma->vm_start;
	int i;

	/* Map each page into user virtual address space */
	for (i = 0; i < buf->nr_pages; i++) {
		int ret = vm_insert_page(vma, addr, buf->pages[i]);
		if (ret)
			return ret;
		addr += PAGE_SIZE;
	}

	return 0;
}

static const struct dma_buf_ops my_dmabuf_ops = {
	.attach = my_attach,
	.map_dma_buf = my_map_dma_buf,
	.unmap_dma_buf = my_unmap_dma_buf,
	.release = my_release,
	.mmap = my_mmap,
};

/* Export function */
int my_export_buffer(size_t size)
{
	struct my_buffer *buf;
	struct dma_buf *dmabuf;
	DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
	int fd, i;

	/* Allocate buffer */
	buf = kzalloc(sizeof(*buf), GFP_KERNEL);
	if (!buf)
		return -ENOMEM;

	buf->size = PAGE_ALIGN(size);
	buf->nr_pages = buf->size >> PAGE_SHIFT;

	buf->pages = kcalloc(buf->nr_pages, sizeof(*buf->pages), GFP_KERNEL);
	if (!buf->pages) {
		kfree(buf);
		return -ENOMEM;
	}

	/* Allocate pages */
	for (i = 0; i < buf->nr_pages; i++) {
		buf->pages[i] = alloc_page(GFP_KERNEL | __GFP_ZERO);
		if (!buf->pages[i])
			goto err_free_pages;
	}

	/* Setup export info */
	exp_info.ops = &my_dmabuf_ops;
	exp_info.size = buf->size;
	exp_info.flags = O_RDWR | O_CLOEXEC;
	exp_info.priv = buf;

	/* Export DMA-BUF */
	dmabuf = dma_buf_export(&exp_info);
	if (IS_ERR(dmabuf))
		goto err_free_pages;

	/* Get fd */
	fd = dma_buf_fd(dmabuf, O_CLOEXEC);
	if (fd < 0) {
		dma_buf_put(dmabuf);
		return fd;
	}

	return fd;

err_free_pages:
	while (i--)
		__free_page(buf->pages[i]);
	kfree(buf->pages);
	kfree(buf);
	return -ENOMEM;
}
```

### Importer Implementation

```c
/* Example: GPU driver importing DMA-BUF */

struct gpu_bo {
	struct dma_buf *dmabuf;
	struct dma_buf_attachment *attachment;
	struct sg_table *sgt;
};

int gpu_import_dmabuf(struct gpu_device *gpu, int fd)
{
	struct gpu_bo *bo;
	struct dma_buf *dmabuf;
	struct dma_buf_attachment *attach;
	struct sg_table *sgt;

	/* Allocate BO structure */
	bo = kzalloc(sizeof(*bo), GFP_KERNEL);
	if (!bo)
		return -ENOMEM;

	/* Get DMA-BUF from fd */
	dmabuf = dma_buf_get(fd);
	if (IS_ERR(dmabuf)) {
		kfree(bo);
		return PTR_ERR(dmabuf);
	}

	/* Attach our device */
	attach = dma_buf_attach(dmabuf, gpu->dev);
	if (IS_ERR(attach)) {
		dma_buf_put(dmabuf);
		kfree(bo);
		return PTR_ERR(attach);
	}

	/* Map for DMA */
	sgt = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);
	if (IS_ERR(sgt)) {
		dma_buf_detach(dmabuf, attach);
		dma_buf_put(dmabuf);
		kfree(bo);
		return PTR_ERR(sgt);
	}

	/* Now we can use the buffer */
	bo->dmabuf = dmabuf;
	bo->attachment = attach;
	bo->sgt = sgt;

	/* Program GPU with DMA addresses */
	struct scatterlist *sg;
	int i;
	for_each_sgtable_dma_sg(sgt, sg, i) {
		dma_addr_t addr = sg_dma_address(sg);
		size_t len = sg_dma_len(sg);

		/* Configure GPU to access this memory region */
		gpu_add_memory_region(gpu, addr, len);
	}

	return 0;  /* Success, bo can be used */
}

void gpu_release_dmabuf(struct gpu_bo *bo)
{
	/* Unmap */
	dma_buf_unmap_attachment(bo->attachment, bo->sgt, DMA_BIDIRECTIONAL);

	/* Detach */
	dma_buf_detach(bo->dmabuf, bo->attachment);

	/* Release dmabuf reference */
	dma_buf_put(bo->dmabuf);

	kfree(bo);
}
```

## Synchronization and Fencing

DMA-BUF includes a synchronization mechanism called **fencing** to coordinate access.

### The Problem

```
Without synchronization:

Timeline:
T0: GPU starts writing to buffer
T1: Display controller starts reading ← RACE!
T2: GPU finishes writing
T3: Display shows corrupted frame

Problem: Display read buffer before GPU finished writing
```

### Fences (dma_fence)

```
With fences:

┌──────────────────────────────────────────────┐
│          DMA-BUF Reservation Object          │
│         (struct dma_resv)                    │
│                                              │
│  Write Fences:                               │
│  ┌────────────────────────────────┐          │
│  │ GPU Render Fence (pending)     │          │
│  └────────────────────────────────┘          │
│                                              │
│  Read Fences:                                │
│  ┌────────────────────────────────┐          │
│  │ Display Scan-out Fence (done)  │          │
│  └────────────────────────────────┘          │
└──────────────────────────────────────────────┘

Timeline with proper synchronization:
T0: GPU starts writing to buffer
    └─ Adds write fence to dmabuf
T1: Display wants to read
    └─ Waits for write fence
T2: GPU finishes, signals write fence
T3: Display fence triggered, starts reading
T4: Display finishes, signals read fence
```

### Fence Operations

```c
/* Each DMA-BUF has a reservation object */
struct dma_resv {
	struct ww_mutex lock;
	struct dma_fence __rcu *fences[];  /* Array of fences */
};

/* Fence represents async operation */
struct dma_fence {
	const struct dma_fence_ops *ops;
	spinlock_t *lock;
	u64 context;         /* Fence context */
	u64 seqno;           /* Sequence number */
	unsigned long flags; /* DMA_FENCE_FLAG_* */
	ktime_t timestamp;   /* When signaled */
};

/* Producer adds write fence */
dma_resv_lock(dmabuf->resv, NULL);
dma_resv_add_fence(dmabuf->resv, fence, DMA_RESV_USAGE_WRITE);
dma_resv_unlock(dmabuf->resv);

/* Consumer waits for fences */
dma_resv_wait_timeout(dmabuf->resv,
                      DMA_RESV_USAGE_WRITE,  /* Wait for writes */
                      true,                  /* Interruptible */
                      timeout);

/* Or get fences to wait on GPU */
struct dma_fence *fence;
dma_resv_lock(dmabuf->resv, NULL);
fence = dma_resv_get_singleton(dmabuf->resv, DMA_RESV_USAGE_WRITE);
if (fence) {
	/* Add fence to GPU command stream */
	gpu_add_fence_dependency(gpu_ctx, fence);
	dma_fence_put(fence);
}
dma_resv_unlock(dmabuf->resv);
```

### Implicit vs Explicit Sync

```
Implicit Synchronization:
- Kernel automatically manages fences
- Producer adds fence when starting work
- Consumer waits on fences before accessing
- Simpler for applications
- Less control

Example:
GPU writes to buffer → fence added automatically
Display imports buffer → waits on fence automatically

Explicit Synchronization:
- Application manages sync_file fds
- More control over dependencies
- Better for complex pipelines
- Used by Vulkan, modern graphics

Example:
GPU driver returns fence fd
App passes fence fd to display
Display waits on fence explicitly
```

## Real-World Use Cases

### Use Case 1: Camera → GPU → Display Pipeline

```
Video Preview Pipeline:

┌──────────────┐
│   Camera     │
│   Driver     │
└──────┬───────┘
       │ 1. Capture frame
       │    dmabuf = camera_alloc_buffer()
       │    fd = dma_buf_fd(dmabuf)
       ↓
┌──────────────┐
│ Application  │
│  (Camera)    │
└──────┬───────┘
       │ 2. Send fd to GPU process
       │    send_fd_via_unix_socket(fd)
       ↓
┌──────────────┐
│ Application  │
│  (Renderer)  │
└──────┬───────┘
       │ 3. Import to GPU
       │    gpu_import_dmabuf(fd)
       │    gpu_apply_filter(buf)  ← Zero-copy!
       │
       │ 4. Send fd to display
       │    send_to_compositor(fd)
       ↓
┌──────────────┐
│   Display    │
│  Compositor  │
└──────┬───────┘
       │ 5. Scan out
       │    display_import_dmabuf(fd)
       │    display_flip(buf)  ← Zero-copy!
       ↓
    [Screen]

Benefits:
- Zero copies (camera DMA → display scan-out)
- Low latency (no CPU involvement)
- Power efficient (no memcpy, less CPU usage)
```

### Use Case 2: Video Decode → Display

```
Video Playback:

┌─────────────────┐
│  Video Decoder  │
│   (Hardware)    │
└────────┬────────┘
         │ 1. Decode H.264/H.265
         │    Outputs to dmabuf
         │    fd = decoder_get_frame_fd()
         ↓
┌─────────────────┐
│  Media Player   │
│  Application    │
└────────┬────────┘
         │ 2. Optional: GPU post-processing
         │    gpu_import(fd)
         │    gpu_scale/deinterlace(buf)
         │
         │ 3. Present to display
         │    drm_atomic_add_fb(fd)
         ↓
┌─────────────────┐
│     Display     │
│   Controller    │
└─────────────────┘

The same physical memory used by:
- Video decoder (writes decoded frames)
- GPU (optional processing)
- Display controller (scans out to screen)

No CPU memcpy needed!
```

### Use Case 3: OpenGL/Vulkan → Wayland Compositor

```
Graphics Rendering:

┌─────────────────┐
│   Application   │
│   (Game/App)    │
└────────┬────────┘
         │ 1. Render with OpenGL/Vulkan
         │    EGLImage from dmabuf
         │    glTexImage2D(egl_image)
         │    Render scene...
         ↓
┌─────────────────┐
│   GPU Driver    │
│   (Mesa/etc)    │
└────────┬────────┘
         │ 2. Export rendered buffer
         │    fd = export_dmabuf()
         │
         │ 3. Send to compositor via Wayland
         │    wl_surface_attach(fd)
         ↓
┌─────────────────┐
│    Wayland      │
│   Compositor    │
└────────┬────────┘
         │ 4. Composite with other windows
         │    import_dmabuf(fd)
         │    gpu_composite(...)
         │
         │ 5. Display
         │    drm_plane_set_fb(fd)
         ↓
     [Screen]
```

### Use Case 4: Zero-Copy V4L2 (Video4Linux)

```c
/* Userspace code: Camera capture with zero-copy */

/* 1. Request buffers from camera driver */
struct v4l2_requestbuffers req = {
	.count = 4,
	.type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
	.memory = V4L2_MEMORY_DMABUF,  /* Use DMA-BUF */
};
ioctl(camera_fd, VIDIOC_REQBUFS, &req);

/* 2. Allocate buffers (could be from GPU, display, etc.) */
int dmabufs[4];
for (int i = 0; i < 4; i++) {
	dmabufs[i] = allocate_dmabuf(width * height * 4);
}

/* 3. Queue buffers to camera */
for (int i = 0; i < 4; i++) {
	struct v4l2_buffer buf = {
		.type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
		.memory = V4L2_MEMORY_DMABUF,
		.index = i,
		.m.fd = dmabufs[i],  /* Pass dmabuf fd */
	};
	ioctl(camera_fd, VIDIOC_QBUF, &buf);
}

/* 4. Start streaming */
int type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
ioctl(camera_fd, VIDIOC_STREAMON, &type);

/* 5. Capture frames */
while (capturing) {
	/* Dequeue filled buffer */
	struct v4l2_buffer buf = {
		.type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
		.memory = V4L2_MEMORY_DMABUF,
	};
	ioctl(camera_fd, VIDIOC_DQBUF, &buf);

	/* Buffer is now filled by camera
	 * Pass to GPU for processing - zero copy!
	 */
	process_on_gpu(dmabufs[buf.index]);

	/* Queue buffer back to camera */
	ioctl(camera_fd, VIDIOC_QBUF, &buf);
}
```

## Performance Considerations

### Memory Access Patterns

```
DMA-BUF access modes:

1. Device-Only Access (Best Performance)
   ┌────────┐        ┌────────┐
   │  GPU   │◄──────►│ Memory │
   └────────┘        └────────┘

   - No CPU access
   - Can use device-local memory
   - Best for ping-pong buffers

2. Device + CPU Read (Common)
   ┌────────┐        ┌────────────┐
   │  GPU   │───────►│   Memory   │
   │ (write)│        │            │
   └────────┘        │            │
   ┌────────┐        │            │
   │  CPU   │◄───────│  (cached)  │
   │ (read) │        └────────────┘
   └────────┘

   - GPU writes, CPU reads
   - Use cached memory
   - Need write-back cache

3. CPU Write + Device Read (Video)
   ┌────────┐        ┌────────────┐
   │  CPU   │───────►│   Memory   │
   │ (write)│        │            │
   └────────┘        │            │
   ┌────────┐        │            │
   │  GPU   │◄───────│(write-comb)│
   │ (read) │        └────────────┘
   └────────┘

   - CPU fills buffer
   - GPU consumes
   - Use write-combining

4. Shared Access (Slowest)
   ┌────────┐        ┌────────────┐
   │  GPU   │◄──────►│   Memory   │
   └────────┘        │            │
   ┌────────┐        │            │
   │  CPU   │◄──────►│ (uncached) │
   └────────┘        └────────────┘

   - Both read and write
   - Coherency overhead
   - Avoid if possible
```

### Cache Coherency

```
Problem: CPU and device caches can be inconsistent

CPU writes to buffer (cached):
┌─────────┐
│CPU Cache│ "new data"
└─────────┘
     ║ Not flushed yet
┌─────────┐
│ Memory  │ "old data"
└─────────┘

GPU reads (bypasses CPU cache):
┌─────────┐
│ Memory  │ "old data" ← GPU sees stale data!
└─────────┘

Solutions:

1. Cache Management (dma_sync):
   /* CPU wrote to buffer, make visible to device */
   dma_sync_sg_for_device(dev, sgt->sgl, sgt->nents, dir);

   /* Device wrote to buffer, make visible to CPU */
   dma_sync_sg_for_cpu(dev, sgt->sgl, sgt->nents, dir);

2. Uncached/Write-Combining Memory:
   /* Slower CPU access, but always coherent */
   pgprot_writecombine(vma->vm_page_prot);

3. Hardware Coherency (IOMMU snoop):
   /* IOMMU enforces coherency automatically */
   /* Best performance, not always available */
```

### Allocation Strategies

```
1. Contiguous Memory Allocator (CMA)
   - Large contiguous allocations
   - Reserved at boot
   - Good for devices without IOMMU

   pages = dma_alloc_coherent(dev, size, &dma_addr, GFP_KERNEL);

2. System Pages
   - Standard page allocator
   - Scatter-gather required
   - Works with IOMMU

   for (i = 0; i < nr_pages; i++)
       pages[i] = alloc_page(GFP_KERNEL);

3. Device Memory Heaps
   - GPU VRAM, carveouts
   - Best performance for device
   - May need CPU access via mmap

   /* Device-specific allocation */
   gpu_alloc_vram(size);
```

### Benchmarks

```
Typical Performance Numbers:

Operation                          Latency    Notes
────────────────────────────────────────────────────────
DMA-BUF export                     ~5 µs     One-time cost
DMA-BUF import + attach           ~10 µs     One-time cost
DMA mapping (4MB, IOMMU)          ~50 µs     Per map
SG table creation                  ~5 µs     Per map
Fence wait (already signaled)     ~100 ns    Fast path
Fence wait (pending)              varies     Depends on work

Memory Bandwidth:
────────────────────────────────────────────────────────
Memcpy (CPU)                      10 GB/s    Slow, avoid
DMA transfer                      50+ GB/s   Device-specific
Zero-copy (ideal)                 ∞          No transfer!

Memory Usage:
────────────────────────────────────────────────────────
Without DMA-BUF (copies):         4x        4 copies of data
With DMA-BUF (shared):            1x        Single copy
```

## Summary

DMA-BUF provides:

1. **Zero-Copy Buffer Sharing**: Multiple devices access same physical memory
2. **Device Independence**: Works across GPU, display, camera, video, etc.
3. **IOMMU Transparency**: Handles address translation automatically
4. **Synchronization**: Built-in fencing for safe concurrent access
5. **Flexibility**: File descriptor based, easy to pass between processes

### Key Concepts

- **Exporter**: Driver that allocates and owns the buffer
- **Importer**: Driver that uses a buffer allocated elsewhere
- **Scatter-Gather**: Describes potentially non-contiguous memory
- **Attachment**: Represents a device's connection to a DMA-BUF
- **Fence**: Synchronization primitive for ordering operations

### When to Use DMA-BUF

✓ **Use when:**
- Sharing buffers between devices (GPU, display, camera, etc.)
- Building zero-copy pipelines
- Need IOMMU-transparent memory sharing
- Want standardized synchronization

✗ **Don't use when:**
- Single device access only
- Small, temporary buffers
- Buffers don't leave kernel space
- Simple driver with direct control

## Code Examples and References

### Kernel Files

```
Core Implementation:
- drivers/dma-buf/dma-buf.c          - Core DMA-BUF framework
- drivers/dma-buf/dma-resv.c         - Reservation/fencing
- drivers/dma-buf/sync_file.c        - Explicit sync
- drivers/dma-buf/dma-heap.c         - DMA-BUF heaps

Headers:
- include/linux/dma-buf.h            - Core DMA-BUF API
- include/linux/dma-resv.h           - Reservation objects
- include/linux/dma-fence.h          - Fence API
- include/uapi/linux/dma-buf.h       - Userspace API

Example Exporters:
- drivers/gpu/drm/drm_prime.c        - DRM/GPU export
- drivers/media/common/videobuf2/    - V4L2 video
- drivers/dma-buf/udmabuf.c          - Userspace DMA-BUF

Example Importers:
- drivers/gpu/drm/*                  - Most DRM drivers
- drivers/media/platform/*           - Video devices
```

### Further Reading

- [Documentation/driver-api/dma-buf.rst](https://docs.kernel.org/driver-api/dma-buf.html) - Kernel documentation
- Linux DMA-BUF UAPI: [include/uapi/linux/dma-buf.h](https://github.com/torvalds/linux/blob/master/include/uapi/linux/dma-buf.h)
- DRM PRIME: Buffer sharing in graphics subsystem
- V4L2 DMA-BUF: Video4Linux integration
- Wayland: `linux-dmabuf` protocol

### Testing

```bash
# Check DMA-BUF usage
cat /sys/kernel/debug/dma_buf/bufinfo

# Example output:
Dma-buf Objects:
size    flags   mode    count   exp_name        ino
00040000    00000002    00080007    00000003    i915    00012345
        Attached Devices:
        i915
        display-controller

# Each DMA-BUF shows:
# - Size
# - Reference count
# - Exporter name
# - Attached devices
```

This demonstrates the power of DMA-BUF: A single buffer (00040000 = 256KB) is shared between three subsystems (i915 GPU, display controller, and likely userspace mmap) with zero copies.


# DMA-BUF Subsystem API Reference

**Completely written by AI**

## Overview

The DMA-BUF subsystem provides a unified mechanism for sharing buffers between 
different device drivers and subsystems in the Linux kernel. It implements an 
exporter/importer model where one driver (the exporter) allocates and manages 
the backing storage, while other drivers (importers) can attach to the buffer 
and obtain DMA mappings for their devices.

### Key Concepts

| Role | Description |
|------|-------------|
| **Exporter** | The driver that creates and owns the buffer's backing storage. Implements `struct dma_buf_ops` callbacks. |
| **Importer** | A driver that wants to access the buffer via DMA. Attaches to the dma_buf and obtains scatter-gather tables for its device. |
| **Attachment** | Represents a connection between an importer's device and a dma_buf. Multiple devices can attach to the same buffer. |
| **Dynamic Importer** | Can handle buffer movement and implements the `move_notify` callback. |
| **Static Importer** | Pins the buffer on first mapping. |

---

## Data Structures

### struct dma_buf

The core buffer object representing a shared buffer:

```c
struct dma_buf {
    size_t size;                    /* Buffer size, invariant */
    struct file *file;              /* File for sharing/refcounting */
    struct list_head attachments;   /* List of attachments */
    const struct dma_buf_ops *ops;  /* Exporter callbacks */
    unsigned vmapping_counter;      /* vmap reference count */
    struct iosys_map vmap_ptr;      /* Current vmap if vmapping_counter > 0 */
    const char *exp_name;           /* Exporter name for debugging */
    const char *name;               /* User-provided name */
    spinlock_t name_lock;           /* Protects name access */
    struct module *owner;           /* Exporter module */
    struct list_head list_node;     /* Global list for debugging */
    void *priv;                     /* Exporter private data */
    struct dma_resv *resv;          /* Reservation object for fences */
    wait_queue_head_t poll;         /* For userspace poll support */
};
```

### struct dma_buf_ops

Callbacks that exporters must implement:

```c
struct dma_buf_ops {
    /* Optional: Called when a device attaches */
    int (*attach)(struct dma_buf *, struct dma_buf_attachment *);

    /* Optional: Called when a device detaches */
    void (*detach)(struct dma_buf *, struct dma_buf_attachment *);

    /* Optional: Pin buffer for DMA (dynamic exporters) */
    int (*pin)(struct dma_buf_attachment *attach);

    /* Optional: Unpin buffer (dynamic exporters) */
    void (*unpin)(struct dma_buf_attachment *attach);

    /* REQUIRED: Map buffer for device DMA access */
    struct sg_table *(*map_dma_buf)(struct dma_buf_attachment *,
                                    enum dma_data_direction);

    /* REQUIRED: Unmap buffer from device */
    void (*unmap_dma_buf)(struct dma_buf_attachment *,
                          struct sg_table *,
                          enum dma_data_direction);

    /* REQUIRED: Release buffer resources */
    void (*release)(struct dma_buf *);

    /* Optional: Prepare for CPU access */
    int (*begin_cpu_access)(struct dma_buf *, enum dma_data_direction);

    /* Optional: End CPU access */
    int (*end_cpu_access)(struct dma_buf *, enum dma_data_direction);

    /* Optional: mmap support for userspace */
    int (*mmap)(struct dma_buf *, struct vm_area_struct *vma);

    /* Optional: Create kernel virtual mapping */
    int (*vmap)(struct dma_buf *dmabuf, struct iosys_map *map);

    /* Optional: Remove kernel virtual mapping */
    void (*vunmap)(struct dma_buf *dmabuf, struct iosys_map *map);
};
```

### struct dma_buf_attachment

Represents a device's attachment to a buffer:

```c
struct dma_buf_attachment {
    struct dma_buf *dmabuf;                     /* The buffer */
    struct device *dev;                         /* Attached device */
    struct list_head node;                      /* In dmabuf->attachments */
    bool peer2peer;                             /* P2P capable */
    const struct dma_buf_attach_ops *importer_ops;  /* Dynamic importer ops */
    void *importer_priv;                        /* Importer private data */
    void *priv;                                 /* Exporter private data */
};
```

### struct dma_buf_export_info

Information needed to export a new buffer:

```c
struct dma_buf_export_info {
    const char *exp_name;           /* Exporter name */
    struct module *owner;           /* Owning module */
    const struct dma_buf_ops *ops;  /* Exporter callbacks */
    size_t size;                    /* Buffer size */
    int flags;                      /* File flags (O_RDWR, etc) */
    struct dma_resv *resv;          /* Optional external resv */
    void *priv;                     /* Private data */
};

/* Helper macro to initialize with module info */
DEFINE_DMA_BUF_EXPORT_INFO(name);
```

### struct dma_buf_attach_ops

Importer operations for dynamic attachments:

```c
struct dma_buf_attach_ops {
    bool allow_peer2peer;   /* Can handle peer resources without pages */

    /* Notification that buffer is moving - must invalidate mappings */
    void (*move_notify)(struct dma_buf_attachment *attach);
};
```

---

## Exported APIs

### Buffer Creation and Destruction

#### dma_buf_export()

**For**: EXPORTER

```c
struct dma_buf *dma_buf_export(const struct dma_buf_export_info *exp_info);
```

| | |
|---|---|
| **Purpose** | Create a new dma_buf and associate it with an anonymous file |
| **Parameters** | `exp_info`: Export information structure |
| **Returns** | Pointer to new dma_buf on success, ERR_PTR on failure |
| **Context** | May sleep. Cannot hold dma_resv lock |

**Usage:**
```c
DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
exp_info.ops = &my_dma_buf_ops;
exp_info.size = buffer_size;
exp_info.flags = O_RDWR;
exp_info.priv = my_buffer;

dmabuf = dma_buf_export(&exp_info);
if (IS_ERR(dmabuf))
    return PTR_ERR(dmabuf);
```

---

#### dma_buf_fd()

**For**: EXPORTER

```c
int dma_buf_fd(struct dma_buf *dmabuf, int flags);
```

| | |
|---|---|
| **Purpose** | Create a file descriptor for sharing the buffer with userspace or other processes |
| **Parameters** | `dmabuf`: The buffer to create fd for<br>`flags`: File flags (O_CLOEXEC, etc) |
| **Returns** | File descriptor on success, negative error on failure |
| **Context** | May sleep. Cannot hold dma_resv lock |

---

#### dma_buf_get()

**For**: IMPORTER

```c
struct dma_buf *dma_buf_get(int fd);
```

| | |
|---|---|
| **Purpose** | Get a dma_buf reference from a file descriptor received from an exporter or another process |
| **Parameters** | `fd`: File descriptor from dma_buf_fd() |
| **Returns** | dma_buf pointer with incremented refcount, or ERR_PTR |
| **Context** | May sleep. Cannot hold dma_resv lock |

> **Note**: Caller must call dma_buf_put() when done.

---

#### dma_buf_put()

**For**: IMPORTER / EXPORTER (any holder)

```c
void dma_buf_put(struct dma_buf *dmabuf);
```

| | |
|---|---|
| **Purpose** | Release a reference to a dma_buf |
| **Parameters** | `dmabuf`: Buffer to release |
| **Context** | May sleep. Cannot hold dma_resv lock |

> **Note**: When refcount reaches 0, the exporter's release callback is invoked.

---

### Device Attachment APIs

#### dma_buf_attach()

**For**: IMPORTER (static)

```c
struct dma_buf_attachment *dma_buf_attach(struct dma_buf *dmabuf,
                                          struct device *dev);
```

| | |
|---|---|
| **Purpose** | Attach a device to a dma_buf using static attachment model. The buffer will be pinned when first mapped |
| **Parameters** | `dmabuf`: Buffer to attach to<br>`dev`: Device that will access the buffer |
| **Returns** | Attachment pointer on success, ERR_PTR on failure |
| **Context** | May sleep. Cannot hold dma_resv lock |

> **Note**: Use for simple importers that don't need move notifications.

---

#### dma_buf_dynamic_attach()

**For**: IMPORTER (dynamic)

```c
struct dma_buf_attachment *
dma_buf_dynamic_attach(struct dma_buf *dmabuf, struct device *dev,
                       const struct dma_buf_attach_ops *importer_ops,
                       void *importer_priv);
```

| | |
|---|---|
| **Purpose** | Attach a device with dynamic importer capabilities. Dynamic importers can handle buffer movement without pinning |
| **Parameters** | `dmabuf`: Buffer to attach to<br>`dev`: Device that will access the buffer<br>`importer_ops`: Importer callbacks (must have move_notify)<br>`importer_priv`: Private data for importer |
| **Returns** | Attachment pointer on success, ERR_PTR on failure |
| **Context** | May sleep. Cannot hold dma_resv lock |

> **Note**: Dynamic importers must implement move_notify callback.

---

#### dma_buf_detach()

**For**: IMPORTER

```c
void dma_buf_detach(struct dma_buf *dmabuf,
                    struct dma_buf_attachment *attach);
```

| | |
|---|---|
| **Purpose** | Remove a device attachment from a dma_buf |
| **Parameters** | `dmabuf`: The buffer<br>`attach`: Attachment to remove |
| **Context** | May sleep. Cannot hold dma_resv lock |

> **Note**: Must unmap before detaching. Frees the attachment structure.

---

### DMA Mapping APIs

#### dma_buf_map_attachment()

**For**: IMPORTER

```c
struct sg_table *dma_buf_map_attachment(struct dma_buf_attachment *attach,
                                        enum dma_data_direction direction);
```

| | |
|---|---|
| **Purpose** | Get DMA mapping (scatter-gather table) for device access. This is the main API importers use to get DMA addresses |
| **Parameters** | `attach`: Device attachment<br>`direction`: DMA_TO_DEVICE, DMA_FROM_DEVICE, or DMA_BIDIRECTIONAL |
| **Returns** | sg_table with DMA addresses, or ERR_PTR |
| **Context** | May sleep. **MUST hold dma_resv lock** |

> **Note**: Addresses in returned sg_table are PAGE_SIZE aligned.

---

#### dma_buf_map_attachment_unlocked()

**For**: IMPORTER

```c
struct sg_table *
dma_buf_map_attachment_unlocked(struct dma_buf_attachment *attach,
                                enum dma_data_direction direction);
```

| | |
|---|---|
| **Purpose** | Same as dma_buf_map_attachment() but acquires lock internally. Convenience wrapper for importers that don't need to hold the lock |
| **Context** | May sleep. Must NOT hold dma_resv lock |

---

#### dma_buf_unmap_attachment()

**For**: IMPORTER

```c
void dma_buf_unmap_attachment(struct dma_buf_attachment *attach,
                              struct sg_table *sg_table,
                              enum dma_data_direction direction);
```

| | |
|---|---|
| **Purpose** | Release DMA mapping obtained from dma_buf_map_attachment() |
| **Parameters** | `attach`: Device attachment<br>`sg_table`: Mapping to release<br>`direction`: Same direction used for mapping |
| **Context** | May sleep. **MUST hold dma_resv lock** |

---

#### dma_buf_unmap_attachment_unlocked()

**For**: IMPORTER

```c
void dma_buf_unmap_attachment_unlocked(struct dma_buf_attachment *attach,
                                       struct sg_table *sg_table,
                                       enum dma_data_direction direction);
```

| | |
|---|---|
| **Purpose** | Same as dma_buf_unmap_attachment() but acquires lock internally |
| **Context** | May sleep. Must NOT hold dma_resv lock |

---

### Pin/Unpin APIs

#### dma_buf_pin()

**For**: IMPORTER (dynamic only)

```c
int dma_buf_pin(struct dma_buf_attachment *attach);
```

| | |
|---|---|
| **Purpose** | Pin buffer so it cannot be moved. Only dynamic importers (those using dma_buf_dynamic_attach) should call this |
| **Parameters** | `attach`: Dynamic attachment (must have importer_ops) |
| **Returns** | 0 on success, negative error on failure |
| **Context** | **MUST hold dma_resv lock** |

> **Note**: Use for limited cases like scanout, not for general pinning.

---

#### dma_buf_unpin()

**For**: IMPORTER (dynamic only)

```c
void dma_buf_unpin(struct dma_buf_attachment *attach);
```

| | |
|---|---|
| **Purpose** | Unpin a previously pinned buffer, allowing the exporter to move it again |
| **Parameters** | `attach`: Dynamic attachment |
| **Context** | **MUST hold dma_resv lock** |

---

### CPU Access APIs

#### dma_buf_begin_cpu_access()

**For**: IMPORTER / ANY HOLDER

```c
int dma_buf_begin_cpu_access(struct dma_buf *dmabuf,
                             enum dma_data_direction direction);
```

| | |
|---|---|
| **Purpose** | Prepare buffer for CPU access. Ensures cache coherency and waits for pending DMA operations (implicit fences) |
| **Parameters** | `dmabuf`: Buffer to access<br>`direction`: DMA_TO_DEVICE (CPU write), DMA_FROM_DEVICE (CPU read), or DMA_BIDIRECTIONAL |
| **Returns** | 0 on success, negative error on failure |
| **Context** | May sleep. Cannot hold dma_resv lock |

> **Note**: Must be paired with end_cpu_access.

---

#### dma_buf_end_cpu_access()

**For**: IMPORTER / ANY HOLDER

```c
int dma_buf_end_cpu_access(struct dma_buf *dmabuf,
                           enum dma_data_direction direction);
```

| | |
|---|---|
| **Purpose** | Signal end of CPU access. Flushes caches if needed so device can see CPU modifications |
| **Parameters** | `dmabuf`: Buffer accessed<br>`direction`: Access direction (must match begin_cpu_access) |
| **Returns** | 0 on success, negative error on failure |
| **Context** | May sleep. Cannot hold dma_resv lock |

---

### Memory Mapping APIs

#### dma_buf_mmap()

**For**: IMPORTER / FRAMEWORK

```c
int dma_buf_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma,
                 unsigned long pgoff);
```

| | |
|---|---|
| **Purpose** | Set up userspace mmap for the buffer. Used by frameworks that need to provide mmap to userspace |
| **Parameters** | `dmabuf`: Buffer to map<br>`vma`: Virtual memory area<br>`pgoff`: Page offset within buffer |
| **Returns** | 0 on success, negative error on failure |
| **Context** | May sleep. Cannot hold dma_resv lock |

---

#### dma_buf_vmap()

**For**: IMPORTER / ANY HOLDER

```c
int dma_buf_vmap(struct dma_buf *dmabuf, struct iosys_map *map);
```

| | |
|---|---|
| **Purpose** | Create kernel virtual mapping for the buffer. Used for kernel-space CPU access to buffer contents |
| **Parameters** | `dmabuf`: Buffer to map<br>`map`: Output iosys_map with virtual address |
| **Returns** | 0 on success, negative error on failure |
| **Context** | May sleep. **MUST hold dma_resv lock** |

> **Note**: Reference counted - multiple vmap calls return same mapping.

---

#### dma_buf_vmap_unlocked()

**For**: IMPORTER / ANY HOLDER

```c
int dma_buf_vmap_unlocked(struct dma_buf *dmabuf, struct iosys_map *map);
```

| | |
|---|---|
| **Purpose** | Same as dma_buf_vmap() but acquires lock internally |
| **Context** | May sleep. Must NOT hold dma_resv lock |

---

#### dma_buf_vunmap()

**For**: IMPORTER / ANY HOLDER

```c
void dma_buf_vunmap(struct dma_buf *dmabuf, struct iosys_map *map);
```

| | |
|---|---|
| **Purpose** | Unmap a kernel virtual mapping |
| **Parameters** | `dmabuf`: Buffer to unmap<br>`map`: Mapping to release |
| **Context** | **MUST hold dma_resv lock** |

---

#### dma_buf_vunmap_unlocked()

**For**: IMPORTER / ANY HOLDER

```c
void dma_buf_vunmap_unlocked(struct dma_buf *dmabuf, struct iosys_map *map);
```

| | |
|---|---|
| **Purpose** | Same as dma_buf_vunmap() but acquires lock internally |
| **Context** | May sleep. Must NOT hold dma_resv lock |

---

### Dynamic Buffer Movement

#### dma_buf_move_notify()

**For**: EXPORTER

```c
void dma_buf_move_notify(struct dma_buf *dmabuf);
```

| | |
|---|---|
| **Purpose** | Notify all dynamic attachments that buffer is moving. Exporter calls this before relocating buffer backing storage |
| **Parameters** | `dmabuf`: Buffer that is moving |
| **Context** | **MUST hold dma_resv lock** |

> **Note**: All dynamic importers' move_notify callbacks will be invoked. They must invalidate their mappings.

---

### Iteration APIs

#### dma_buf_iter_begin()

**For**: DEBUG / STATISTICS

```c
struct dma_buf *dma_buf_iter_begin(void);
```

| | |
|---|---|
| **Purpose** | Begin iteration through global list of all DMA buffers. Used for debugging and collecting statistics |
| **Returns** | First buffer with elevated refcount, or NULL |

> **Note**: Caller must release with dma_buf_iter_next() or dma_buf_put().

---

#### dma_buf_iter_next()

**For**: DEBUG / STATISTICS

```c
struct dma_buf *dma_buf_iter_next(struct dma_buf *dmabuf);
```

| | |
|---|---|
| **Purpose** | Continue iteration, releasing current buffer |
| **Parameters** | `dmabuf`: Current buffer (reference released) |
| **Returns** | Next buffer with elevated refcount, or NULL |

---

### Helper Functions

#### get_dma_buf()

**For**: ANY HOLDER

```c
static inline void get_dma_buf(struct dma_buf *dmabuf);
```

| | |
|---|---|
| **Purpose** | Increment reference count on dma_buf. For kernel-internal use when driver needs an additional reference |

---

#### dma_buf_is_dynamic()

**For**: IMPORTER

```c
static inline bool dma_buf_is_dynamic(struct dma_buf *dmabuf);
```

| | |
|---|---|
| **Purpose** | Check if buffer uses dynamic mappings (exporter implements pin/unpin) |
| **Returns** | true if exporter is dynamic |

---

## DMA Heap APIs

The DMA heap subsystem provides userspace allocation of DMA buffers.

#### dma_heap_add()

**For**: HEAP PROVIDER (kernel module)

```c
struct dma_heap *dma_heap_add(const struct dma_heap_export_info *exp_info);
```

| | |
|---|---|
| **Purpose** | Register a new DMA heap that userspace can allocate from |
| **Parameters** | `exp_info`: Heap information (name, ops, priv) |
| **Returns** | Heap pointer on success, ERR_PTR on failure |

> **Note**: Creates `/dev/dma_heap/<name>` device node.

---

#### dma_heap_get_drvdata()

**For**: HEAP PROVIDER

```c
void *dma_heap_get_drvdata(struct dma_heap *heap);
```

| | |
|---|---|
| **Purpose** | Get private data from heap structure |

---

#### dma_heap_get_name()

**For**: HEAP PROVIDER

```c
const char *dma_heap_get_name(struct dma_heap *heap);
```

| | |
|---|---|
| **Purpose** | Get heap name string |

---

## Userspace Interface

### DMA-BUF Sync IOCTL

**For**: USERSPACE APPLICATION

Userspace must synchronize CPU access using the sync ioctl:

```c
#include <linux/dma-buf.h>

struct dma_buf_sync sync = {
    .flags = DMA_BUF_SYNC_START | DMA_BUF_SYNC_READ,
};
ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);

/* ... read from mmap'd buffer ... */

sync.flags = DMA_BUF_SYNC_END | DMA_BUF_SYNC_READ;
ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);
```

**Flags:**
| Flag | Description |
|------|-------------|
| `DMA_BUF_SYNC_READ` | CPU will read |
| `DMA_BUF_SYNC_WRITE` | CPU will write |
| `DMA_BUF_SYNC_RW` | Both read and write |
| `DMA_BUF_SYNC_START` | Begin access |
| `DMA_BUF_SYNC_END` | End access |

---

### DMA Heap Allocation IOCTL

**For**: USERSPACE APPLICATION

Allocate from a DMA heap:

```c
#include <linux/dma-heap.h>

int heap_fd = open("/dev/dma_heap/system", O_RDWR);

struct dma_heap_allocation_data alloc = {
    .len = 4096,
    .fd_flags = O_RDWR | O_CLOEXEC,
};

ioctl(heap_fd, DMA_HEAP_IOCTL_ALLOC, &alloc);
int dmabuf_fd = alloc.fd;
```

---

## Examples

### Example 1: Simple Exporter

```c
#include <linux/dma-buf.h>
#include <linux/slab.h>
#include <linux/dma-mapping.h>

struct my_buffer {
    void *cpu_addr;
    dma_addr_t dma_addr;
    size_t size;
    struct device *dev;
};

static struct sg_table *my_map_dma_buf(struct dma_buf_attachment *attach,
                                       enum dma_data_direction dir)
{
    struct my_buffer *buf = attach->dmabuf->priv;
    struct sg_table *sgt;
    int ret;

    sgt = kzalloc(sizeof(*sgt), GFP_KERNEL);
    if (!sgt)
        return ERR_PTR(-ENOMEM);

    ret = sg_alloc_table(sgt, 1, GFP_KERNEL);
    if (ret) {
        kfree(sgt);
        return ERR_PTR(ret);
    }

    sg_set_page(sgt->sgl, virt_to_page(buf->cpu_addr), buf->size, 0);

    ret = dma_map_sgtable(attach->dev, sgt, dir, 0);
    if (ret) {
        sg_free_table(sgt);
        kfree(sgt);
        return ERR_PTR(ret);
    }

    return sgt;
}

static void my_unmap_dma_buf(struct dma_buf_attachment *attach,
                             struct sg_table *sgt,
                             enum dma_data_direction dir)
{
    dma_unmap_sgtable(attach->dev, sgt, dir, 0);
    sg_free_table(sgt);
    kfree(sgt);
}

static void my_release(struct dma_buf *dmabuf)
{
    struct my_buffer *buf = dmabuf->priv;

    dma_free_coherent(buf->dev, buf->size, buf->cpu_addr, buf->dma_addr);
    kfree(buf);
}

static const struct dma_buf_ops my_dmabuf_ops = {
    .map_dma_buf = my_map_dma_buf,
    .unmap_dma_buf = my_unmap_dma_buf,
    .release = my_release,
};

struct dma_buf *my_alloc_dmabuf(struct device *dev, size_t size)
{
    struct my_buffer *buf;
    DEFINE_DMA_BUF_EXPORT_INFO(exp_info);

    buf = kzalloc(sizeof(*buf), GFP_KERNEL);
    if (!buf)
        return ERR_PTR(-ENOMEM);

    buf->dev = dev;
    buf->size = size;
    buf->cpu_addr = dma_alloc_coherent(dev, size, &buf->dma_addr, GFP_KERNEL);
    if (!buf->cpu_addr) {
        kfree(buf);
        return ERR_PTR(-ENOMEM);
    }

    exp_info.ops = &my_dmabuf_ops;
    exp_info.size = size;
    exp_info.flags = O_RDWR;
    exp_info.priv = buf;

    return dma_buf_export(&exp_info);
}
```

---

### Example 2: Simple Importer

```c
#include <linux/dma-buf.h>

int import_dmabuf(struct device *dev, int fd)
{
    struct dma_buf *dmabuf;
    struct dma_buf_attachment *attach;
    struct sg_table *sgt;
    struct scatterlist *sg;
    int i;

    /* Get dma_buf from fd */
    dmabuf = dma_buf_get(fd);
    if (IS_ERR(dmabuf))
        return PTR_ERR(dmabuf);

    /* Attach our device */
    attach = dma_buf_attach(dmabuf, dev);
    if (IS_ERR(attach)) {
        dma_buf_put(dmabuf);
        return PTR_ERR(attach);
    }

    /* Get DMA mapping */
    sgt = dma_buf_map_attachment_unlocked(attach, DMA_BIDIRECTIONAL);
    if (IS_ERR(sgt)) {
        dma_buf_detach(dmabuf, attach);
        dma_buf_put(dmabuf);
        return PTR_ERR(sgt);
    }

    /* Use the scatter-gather table for DMA */
    for_each_sgtable_dma_sg(sgt, sg, i) {
        dma_addr_t addr = sg_dma_address(sg);
        unsigned int len = sg_dma_len(sg);

        pr_info("DMA segment %d: addr=0x%llx len=%u\n",
                i, (u64)addr, len);
        /* Program DMA engine with addr/len */
    }

    /* Cleanup when done */
    dma_buf_unmap_attachment_unlocked(attach, sgt, DMA_BIDIRECTIONAL);
    dma_buf_detach(dmabuf, attach);
    dma_buf_put(dmabuf);

    return 0;
}
```

---

### Example 3: CPU Access with Sync

```c
#include <linux/dma-buf.h>

int cpu_access_dmabuf(struct dma_buf *dmabuf)
{
    struct iosys_map map;
    int ret;

    /* Prepare for CPU read */
    ret = dma_buf_begin_cpu_access(dmabuf, DMA_FROM_DEVICE);
    if (ret)
        return ret;

    /* Get kernel virtual mapping */
    ret = dma_buf_vmap_unlocked(dmabuf, &map);
    if (ret) {
        dma_buf_end_cpu_access(dmabuf, DMA_FROM_DEVICE);
        return ret;
    }

    /* Access the buffer */
    if (iosys_map_is_vaddr(&map)) {
        void *vaddr = map.vaddr;
        /* Read from vaddr */
    } else {
        void __iomem *iomem = map.vaddr_iomem;
        /* Use ioread/iowrite for I/O memory */
    }

    /* Cleanup */
    dma_buf_vunmap_unlocked(dmabuf, &map);
    dma_buf_end_cpu_access(dmabuf, DMA_FROM_DEVICE);

    return 0;
}
```

---

### Example 4: Userspace Allocation and Mapping

```c
#include <fcntl.h>
#include <sys/ioctl.h>
#include <sys/mman.h>
#include <linux/dma-buf.h>
#include <linux/dma-heap.h>

int userspace_dmabuf_example(void)
{
    int heap_fd, dmabuf_fd;
    void *map;
    size_t size = 4096;
    struct dma_heap_allocation_data alloc = {0};
    struct dma_buf_sync sync = {0};

    /* Open system heap */
    heap_fd = open("/dev/dma_heap/system", O_RDWR);
    if (heap_fd < 0)
        return -1;

    /* Allocate buffer */
    alloc.len = size;
    alloc.fd_flags = O_RDWR | O_CLOEXEC;
    if (ioctl(heap_fd, DMA_HEAP_IOCTL_ALLOC, &alloc) < 0) {
        close(heap_fd);
        return -1;
    }
    dmabuf_fd = alloc.fd;
    close(heap_fd);

    /* mmap the buffer */
    map = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED,
               dmabuf_fd, 0);
    if (map == MAP_FAILED) {
        close(dmabuf_fd);
        return -1;
    }

    /* Begin CPU access */
    sync.flags = DMA_BUF_SYNC_START | DMA_BUF_SYNC_WRITE;
    ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);

    /* Write to buffer */
    memset(map, 0x42, size);

    /* End CPU access */
    sync.flags = DMA_BUF_SYNC_END | DMA_BUF_SYNC_WRITE;
    ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);

    /* Pass dmabuf_fd to other drivers/processes */
    /* ... */

    /* Cleanup */
    munmap(map, size);
    close(dmabuf_fd);

    return 0;
}
```

---

## Locking Convention

To avoid deadlocks, all DMA-BUF users must follow these rules:

### Importers

**Must hold dma_resv lock when calling:**
- `dma_buf_pin()`
- `dma_buf_unpin()`
- `dma_buf_map_attachment()`
- `dma_buf_unmap_attachment()`
- `dma_buf_vmap()`
- `dma_buf_vunmap()`

**Must NOT hold dma_resv lock when calling:**
- `dma_buf_attach()`
- `dma_buf_dynamic_attach()`
- `dma_buf_detach()`
- `dma_buf_export()`
- `dma_buf_fd()`
- `dma_buf_get()`
- `dma_buf_put()`
- `dma_buf_mmap()`
- `dma_buf_begin_cpu_access()`
- `dma_buf_end_cpu_access()`
- `*_unlocked()` variants

### Exporters

**Callbacks invoked with unlocked dma_resv:**
- `attach()`
- `detach()`
- `release()`
- `begin_cpu_access()`
- `end_cpu_access()`
- `mmap()`

**Callbacks invoked with locked dma_resv:**
- `pin()`
- `unpin()`
- `map_dma_buf()`
- `unmap_dma_buf()`
- `vmap()`
- `vunmap()`

**Must hold dma_resv lock when calling:**
- `dma_buf_move_notify()`

---

## API Summary Table

| API | For | Lock Requirement |
|-----|-----|------------------|
| `dma_buf_export()` | EXPORTER | No lock |
| `dma_buf_fd()` | EXPORTER | No lock |
| `dma_buf_get()` | IMPORTER | No lock |
| `dma_buf_put()` | ANY HOLDER | No lock |
| `dma_buf_attach()` | IMPORTER (static) | No lock |
| `dma_buf_dynamic_attach()` | IMPORTER (dynamic) | No lock |
| `dma_buf_detach()` | IMPORTER | No lock |
| `dma_buf_map_attachment()` | IMPORTER | Hold resv lock |
| `dma_buf_map_attachment_unlocked()` | IMPORTER | No lock |
| `dma_buf_unmap_attachment()` | IMPORTER | Hold resv lock |
| `dma_buf_unmap_attachment_unlocked()` | IMPORTER | No lock |
| `dma_buf_pin()` | IMPORTER (dynamic) | Hold resv lock |
| `dma_buf_unpin()` | IMPORTER (dynamic) | Hold resv lock |
| `dma_buf_begin_cpu_access()` | ANY HOLDER | No lock |
| `dma_buf_end_cpu_access()` | ANY HOLDER | No lock |
| `dma_buf_mmap()` | IMPORTER/FRAMEWORK | No lock |
| `dma_buf_vmap()` | ANY HOLDER | Hold resv lock |
| `dma_buf_vmap_unlocked()` | ANY HOLDER | No lock |
| `dma_buf_vunmap()` | ANY HOLDER | Hold resv lock |
| `dma_buf_vunmap_unlocked()` | ANY HOLDER | No lock |
| `dma_buf_move_notify()` | EXPORTER | Hold resv lock |
| `dma_heap_add()` | HEAP PROVIDER | N/A |

---

## See Also

- [Documentation/driver-api/dma-buf.rst](https://github.com/torvalds/linux/blob/master/Documentation/driver-api/dma-buf.rst)
- [include/linux/dma-buf.h](https://github.com/torvalds/linux/blob/master/include/linux/dma-buf.h)
- [include/uapi/linux/dma-buf.h](https://github.com/torvalds/linux/blob/master/include/uapi/linux/dma-buf.h)
- [include/linux/dma-heap.h](https://github.com/torvalds/linux/blob/master/include/linux/dma-heap.h)
- [include/uapi/linux/dma-heap.h](https://github.com/torvalds/linux/blob/master/include/uapi/linux/dma-heap.h)
- [drivers/dma-buf/dma-buf.c](https://github.com/torvalds/linux/blob/master/drivers/dma-buf/dma-buf.c)
- [drivers/dma-buf/dma-heap.c](https://github.com/torvalds/linux/blob/master/drivers/dma-buf/dma-heap.c)
- [drivers/dma-buf/heaps/system_heap.c](https://github.com/torvalds/linux/blob/master/drivers/dma-buf/heaps/system_heap.c)


# DMA-BUF Userland Memory Heaps

**Completely written by AI**

## What Are DMA-BUF Heaps?

**DMA-BUF Heaps** provide a standardized way for userspace applications to allocate DMA-BUF objects from specific memory pools **without needing a device driver**.

### The Problem They Solve

**Before DMA-BUF Heaps:**
```
App needs buffer → Opens GPU driver → GPU driver allocates DMA-BUF → Returns fd
                   ↑
            Requires device-specific driver
            Different APIs for different devices
```

**With DMA-BUF Heaps:**
```
App needs buffer → Opens /dev/dma_heap/system → Kernel allocates → Returns DMA-BUF fd
                   ↑
            Standardized API
            No device driver needed
            Works across frameworks
```

### Benefits

✅ **Framework-agnostic**: Works with graphics, video, camera, ML accelerators
✅ **Userspace control**: Applications choose allocation strategy
✅ **Zero-copy sharing**: Share buffers between subsystems efficiently
✅ **Standardized API**: Same interface for all heaps
✅ **No driver required**: Direct allocation from userspace

---

## Available Heaps

### 1. System Heap (`/dev/dma_heap/system`)

**Properties:**
- **Memory Type**: Virtually contiguous, cacheable
- **Physical Layout**: May be scattered (uses scatter-gather)
- **Performance**: Fast allocation, works with IOMMU
- **Availability**: Always present
- **Best For**: General-purpose buffer sharing

**Allocation Strategy:**
- Tries to allocate large pages first (1MB, 64KB) for IOMMU efficiency
- Falls back to 4KB pages if needed
- Orders: 256KB (order-8), 16KB (order-4), 4KB (order-0)

**Use Cases:**
- Graphics buffers (GPU rendering)
- Video codec buffers (V4L2, GStreamer)
- Camera capture buffers
- ML accelerator inputs/outputs
- Any multi-device buffer sharing

### 2. CMA Heap (`/dev/dma_heap/default_cma_region`)

**Properties:**
- **Memory Type**: Physically contiguous, cacheable
- **Physical Layout**: Single contiguous physical block
- **Performance**: Slower allocation (needs contiguous memory)
- **Availability**: Only if CMA configured
- **Best For**: Devices requiring physical contiguity

**Configuration:**
Enabled via:
- Kernel command line: `cma=256M`
- Device Tree: `linux,cma-default` property
- Kconfig: `CONFIG_CMA_SIZE_MBYTES=256`

**Use Cases:**
- Legacy devices without IOMMU/scatter-gather support
- Hardware requiring physically contiguous DMA
- Embedded systems with simple DMA controllers
- Display controllers needing contiguous framebuffers

### 3. Reserved Memory Heaps

**Properties:**
- **Memory Type**: Device-specific (often physically contiguous)
- **Physical Layout**: Fixed memory region
- **Performance**: Fast (pre-reserved)
- **Availability**: Device tree configuration
- **Best For**: Special hardware requirements

**Device Tree Example:**
```dts
reserved-memory {
    #address-cells = <1>;
    #size-cells = <1>;
    ranges;

    video_cma: video@42000000 {
        compatible = "shared-dma-pool";
        reusable;
        reg = <0x42000000 0x10000000>; /* 256 MB */
    };
};
```

Becomes: `/dev/dma_heap/video@42000000`

---

## Userspace API

### Header Files

```c
#include <linux/dma-heap.h>  /* struct dma_heap_allocation_data */
#include <linux/dma-buf.h>   /* DMA_BUF_IOCTL_SYNC */
#include <sys/ioctl.h>
#include <fcntl.h>
#include <sys/mman.h>
```

### Data Structures

```c
struct dma_heap_allocation_data {
    __u64 len;         /* Size to allocate (bytes) */
    __u32 fd;          /* Output: DMA-BUF file descriptor */
    __u32 fd_flags;    /* O_CLOEXEC, O_RDONLY, O_WRONLY, O_RDWR */
    __u64 heap_flags;  /* Reserved (must be 0) */
};
```

### Allocation Steps

**1. Open Heap Device**
```c
int heap_fd = open("/dev/dma_heap/system", O_RDONLY);
if (heap_fd < 0) {
    perror("Failed to open heap");
    return -1;
}
```

**2. Allocate Buffer**
```c
struct dma_heap_allocation_data heap_data = {
    .len = 4096 * 1024,         /* 4 MB */
    .fd = 0,
    .fd_flags = O_RDWR | O_CLOEXEC,
    .heap_flags = 0
};

int ret = ioctl(heap_fd, DMA_HEAP_IOCTL_ALLOC, &heap_data);
if (ret < 0) {
    perror("Allocation failed");
    close(heap_fd);
    return -1;
}

int dmabuf_fd = heap_data.fd;  /* This is your DMA-BUF! */
close(heap_fd);  /* No longer needed */
```

**3. Map for CPU Access**
```c
void *ptr = mmap(NULL, heap_data.len, PROT_READ | PROT_WRITE,
                 MAP_SHARED, dmabuf_fd, 0);
if (ptr == MAP_FAILED) {
    perror("mmap failed");
    close(dmabuf_fd);
    return -1;
}
```

**4. Use the Buffer**
```c
/* Before CPU access */
struct dma_buf_sync sync = { .flags = DMA_BUF_SYNC_START | DMA_BUF_SYNC_RW };
ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);

/* Write data */
memcpy(ptr, data, size);

/* After CPU access */
sync.flags = DMA_BUF_SYNC_END | DMA_BUF_SYNC_RW;
ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);
```

**5. Share with Devices**
```c
/* Pass dmabuf_fd to GPU, video codec, camera, etc. */
pass_to_gpu(dmabuf_fd);
pass_to_video_decoder(dmabuf_fd);
pass_to_camera(dmabuf_fd);
```

**6. Cleanup**
```c
munmap(ptr, heap_data.len);
close(dmabuf_fd);  /* Frees when all users done */
```

---

## Kernel Implementation

### Core Framework

**Location**: `drivers/dma-buf/dma-heap.c`

**Key Functions**:
- `dma_heap_add()` - Register new heap
- `dma_heap_buffer_alloc()` - Allocate from heap
- Device nodes: `/dev/dma_heap/<name>`

### System Heap Implementation

**Location**: `drivers/dma-buf/heaps/system_heap.c`

**Allocation Algorithm**:
```c
static const unsigned int orders[] = {8, 4, 0};  /* 1MB, 64KB, 4KB */

while (size_remaining > 0) {
    page = alloc_largest_available(size_remaining, max_order);
    /* Tries order-8 (256KB), order-4 (16KB), order-0 (4KB) */
    list_add_tail(&page->lru, &pages);
    size_remaining -= page_size(page);
}
```

**Why Large Pages?**
- Reduces IOMMU TLB pressure
- Fewer page table updates
- Better DMA performance
- Still virtually contiguous for CPU

### CMA Heap Implementation

**Location**: `drivers/dma-buf/heaps/cma_heap.c`

**Allocation**:
```c
cma_pages = cma_alloc(cma, pagecount, align, GFP_KERNEL);
/* Returns physically contiguous pages from CMA region */
```

---

## Checking Available Heaps

### System Check
```bash
# List all heaps
ls -l /dev/dma_heap/

# Common outputs:
# /dev/dma_heap/system             (always present)
# /dev/dma_heap/default_cma_region (if CMA configured)
# /dev/dma_heap/video@42000000     (if device tree regions)
```

### Your System
```bash
$ ls -la /dev/dma_heap/
crw------- 1 root root 250, 0 Dec 25 14:18 system
```

**You have**: System heap only (standard configuration)

### Kernel Configuration
```bash
# Check if heaps enabled
grep DMA_HEAP /boot/config-$(uname -r)

# Should show:
CONFIG_DMA_SHARED_BUFFER=y
CONFIG_DMA_HEAP=y
CONFIG_DMA_HEAP_SYSTEM=y
CONFIG_DMA_HEAP_CMA=y  # (if CMA heap available)
```

---

## Real-World Use Cases

### 1. Graphics Pipeline (GPU + Display)

```c
/* Allocate buffer from heap */
int dmabuf_fd = allocate_from_heap("/dev/dma_heap/system", 1920*1080*4);

/* GPU writes rendered frame */
submit_to_gpu(dmabuf_fd);

/* Display controller scans out */
display_buffer(dmabuf_fd);

/* Zero-copy! Same buffer, no memcpy */
```

### 2. Video Decode Pipeline

```c
/* Allocate buffer */
int dmabuf_fd = allocate_from_heap("/dev/dma_heap/system", 4*1024*1024);

/* Video decoder writes decoded frame */
v4l2_queue_buffer(decoder_fd, dmabuf_fd);

/* Neural network processes frame */
ml_accelerator_process(dmabuf_fd);

/* Display the result */
display_buffer(dmabuf_fd);
```

### 3. Camera Capture

```c
/* Allocate capture buffers from heap */
for (int i = 0; i < NUM_BUFFERS; i++) {
    int dmabuf_fd = allocate_from_heap("/dev/dma_heap/system", buffer_size);
    camera_queue_buffer(dmabuf_fd);
}

/* Camera writes directly to DMA-BUF */
/* Application can map and read when ready */
```

### 4. Cross-Process Sharing

```c
/* Process A: Allocate and fill buffer */
int dmabuf_fd = allocate_from_heap("/dev/dma_heap/system", size);
fill_buffer(dmabuf_fd);

/* Send fd to Process B via Unix socket */
send_fd_over_socket(socket_fd, dmabuf_fd);

/* Process B: Receive and use */
int received_fd = receive_fd_over_socket(socket_fd);
process_buffer(received_fd);

/* Zero-copy cross-process sharing! */
```

---

## Performance Characteristics

### System Heap

| Allocation Size | Time | Physical Layout | IOMMU Required |
|-----------------|------|-----------------|----------------|
| 4 KB | ~10 μs | 1 page | No |
| 64 KB | ~20 μs | ~4 pages | Preferred |
| 1 MB | ~50 μs | ~16-256 pages | Preferred |
| 16 MB | ~200 μs | Scattered | Preferred |

**Best For:**
- Modern systems with IOMMU
- Devices supporting scatter-gather DMA
- General-purpose allocations

### CMA Heap

| Allocation Size | Time | Physical Layout | IOMMU Required |
|-----------------|------|-----------------|----------------|
| 4 KB | ~50 μs | Contiguous | No |
| 64 KB | ~100 μs | Contiguous | No |
| 1 MB | ~500 μs | Contiguous | No |
| 16 MB | ~2-5 ms | Contiguous | No |

**Best For:**
- Legacy devices without IOMMU
- Hardware requiring physical contiguity
- Embedded systems

---

## Comparison Matrix

| Feature | System Heap | CMA Heap | Device-Specific |
|---------|-------------|----------|-----------------|
| **Availability** | Always | If CMA enabled | Device tree |
| **Physical Layout** | Scattered | Contiguous | Varies |
| **IOMMU Required** | Preferred | No | No |
| **Allocation Speed** | Fast | Slower | Very fast |
| **Memory Efficiency** | High | Medium | Fixed pool |
| **Use Case** | Modern devices | Legacy devices | Special hardware |
| **Typical Size Limit** | GB+ | Hundreds of MB | Fixed region |

---

## Common Mistakes

### ❌ Mistake 1: Forgetting DMA_BUF_IOCTL_SYNC

```c
/* WRONG: Direct CPU access without sync */
void *ptr = mmap(..., dmabuf_fd, ...);
memcpy(ptr, data, size);  /* Cache issues! */
```

```c
/* CORRECT: Always sync before/after CPU access */
struct dma_buf_sync sync = { .flags = DMA_BUF_SYNC_START | DMA_BUF_SYNC_WRITE };
ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);
memcpy(ptr, data, size);
sync.flags = DMA_BUF_SYNC_END | DMA_BUF_SYNC_WRITE;
ioctl(dmabuf_fd, DMA_BUF_IOCTL_SYNC, &sync);
```

### ❌ Mistake 2: Not Checking Return Values

```c
/* WRONG */
int heap_fd = open("/dev/dma_heap/cma", O_RDONLY);
ioctl(heap_fd, DMA_HEAP_IOCTL_ALLOC, &heap_data);  /* May fail! */
```

```c
/* CORRECT */
int heap_fd = open("/dev/dma_heap/system", O_RDONLY);
if (heap_fd < 0) {
    perror("Heap not available, try fallback");
    heap_fd = open("/dev/dma_heap/system", O_RDONLY);
}
```

### ❌ Mistake 3: Closing Heap FD Too Early

```c
/* WRONG */
int heap_fd = open("/dev/dma_heap/system", O_RDONLY);
ioctl(heap_fd, DMA_HEAP_IOCTL_ALLOC, &heap_data);
close(heap_fd);  /* ← CORRECT: Heap fd independent of dmabuf fd */

int dmabuf_fd = heap_data.fd;
/* Use dmabuf_fd... this is fine! */
```

**Note**: Heap fd and dmabuf fd are independent. Close heap fd after allocation.

---

## Android Integration

DMA-BUF heaps are heavily used in Android:

```java
// Android BufferQueue uses DMA-BUF heaps
GraphicBuffer buffer = new GraphicBuffer(...);

// Internally allocates from:
// /dev/dma_heap/system (modern)
// /dev/dma_heap/system-uncached (special)
```

**Android Heap Names:**
- `system` - General buffers
- `system-uncached` - Uncached buffers for specific hardware

---

## Debugging

### Enable Debug Messages
```bash
# Dynamic debug
echo 'module dma_heap +p' > /sys/kernel/debug/dynamic_debug/control
echo 'module system_heap +p' > /sys/kernel/debug/dynamic_debug/control

# View allocation messages
dmesg | grep dma_heap
```

### Check Allocation Stats
```bash
# DMA-BUF stats (if enabled)
cat /sys/kernel/debug/dma_buf/bufinfo

# Shows:
# - Buffer sizes
# - Exporters
# - Attachments
```

### Trace Allocations
```bash
# Use ftrace
echo 1 > /sys/kernel/debug/tracing/events/dma_buf/enable
cat /sys/kernel/debug/tracing/trace
```

---

## Summary

### Quick Facts

**What**: Userspace API to allocate DMA-BUF from specific memory pools
**Since**: Linux 5.6 (2020)
**Location**: `/dev/dma_heap/<name>`
**API**: Single ioctl: `DMA_HEAP_IOCTL_ALLOC`

### Common Heaps

| Heap | Path | Physical | Use Case |
|------|------|----------|----------|
| System | `/dev/dma_heap/system` | Scattered | General purpose |
| CMA | `/dev/dma_heap/default_cma_region` | Contiguous | Legacy devices |
| Custom | `/dev/dma_heap/<dt-name>` | Varies | Special hardware |

### Workflow

```
1. Open /dev/dma_heap/<name>
2. ioctl(DMA_HEAP_IOCTL_ALLOC)
3. Get DMA-BUF fd
4. Share fd with devices
5. mmap() for CPU access (with sync!)
6. close() when done
```

### Key Advantages

✅ No device driver needed
✅ Framework-agnostic sharing
✅ Standardized API
✅ Zero-copy between subsystems
✅ Userspace control over allocation

**Bottom Line**: DMA-BUF heaps are the modern way to allocate shareable buffers in Linux userspace!


# [RFC v2 00/11] Add dmabuf read/write via io_uring

[[RFC v2 00/11] Add dmabuf read/write via io_uring](https://lore.kernel.org/linux-block/cover.1763725387.git.asml.silence@gmail.com/)

## Overview from AI

```
This patchset introduces support for performing read and write operations on 
dmabuf file descriptors through io_uring. This allows for efficient, zero-copy 
data transfers between devices by enabling asynchronous I/O on shared dmabuf buffers.

The key changes in this patchset are: 
 - dmabuf Integration with io_uring:
   It adds the necessary infrastructure to io_uring to handle dmabuf buffers, 
   including the management of DMA addresses and dynamic buffer attachments. 

 - Block Layer Modifications: 
   The core logic is integrated into the blk-mq subsystem to support asynchronous 
   DMA mapping and request cancellation.

 - User-space API: The existing io_uring user-space API remains unchanged. A dmabuf 
   can be registered and used like a normal I/O buffer.

Overall, this patchset aims to improve performance and reduce latency for 
applications that rely on dmabuf for data sharing between devices by leveraging the 
asynchronous capabilities of io_uring. The patchset is currently in the RFC (Request for 
Comments) stage, with the author planning future simplifications to reduce complexity.

```

## Here’s a breakdown of how it works:

```
1. The Core Problem: The CPU as a Data Mover

Traditionally, when you want to move data from one device (like a GPU) to another (like an NVMe SSD), the data often has to take a
detour through the main system memory (RAM) and be copied by the CPU.

A typical, inefficient workflow looks like this:

 1. A GPU finishes rendering a frame into a memory buffer.
 2. Your application wants to save this frame to a fast NVMe drive.
 3. The application issues a write() system call.
 4. The kernel instructs the CPU to execute a memcpy-like operation, copying the data from the GPU's buffer into the kernel's page
    cache (a temporary buffer in RAM).
 5. Finally, the kernel instructs the NVMe drive to fetch the data from the page cache and write it to storage.

The bottleneck here is Step 4. The CPU spends valuable cycles just copying data from one place in memory to another. This is slow,
consumes CPU resources that could be used for other tasks, and adds latency.

2. The Solution: A Direct, CPU-less Path

This patchset creates a "fast path" that bypasses the CPU-driven copy. It achieves this in two key ways:

a) dmabuf for Zero-Copy Transfers

A dmabuf (DMA buffer) is a kernel mechanism for sharing a buffer of memory between multiple devices without involving the CPU.
When a device (like a GPU) creates a dmabuf, it's not just a block of data; it's a handle that describes where the data lives in
physical memory.

Other drivers (like a storage driver) can "import" this dmabuf and get the physical memory addresses directly. This allows them to
set up Direct Memory Access (DMA) transfers, where the hardware moves data directly from the source to the destination without the
CPU's intervention. This is the principle of "zero-copy."

b) io_uring for Asynchronous, Low-Overhead I/O

io_uring is a high-performance interface for asynchronous I/O. Its key advantages are:
 * Reduced System Calls: Instead of one system call per I/O operation (like read() or write()), you can submit hundreds of
   operations with a single system call. This dramatically reduces the overhead of switching between user-space and kernel-space.
 * True Asynchronicity: Your application can submit a batch of I/O requests and immediately go back to doing other work. The
   kernel processes them in the background and notifies the application only when they are complete.

How the Patchset Combines Them for Maximum Performance

This patchset teaches io_uring and the block layer how to speak the language of dmabuf.

The new, highly efficient workflow is:

 1. A GPU renders a frame into a dmabuf.
 2. The application registers this dmabuf with io_uring.
 3. The application submits a write operation to io_uring, telling it to write the contents of the dmabuf to the NVMe drive.
 4. Here's the magic:
     * io_uring passes the request to the block layer.
     * The NVMe driver, using the new logic from the patchset, imports the dmabuf.
     * The driver gets the list of physical memory pages for the buffer directly.
     * The driver programs the NVMe controller's DMA engine to pull the data straight from the GPU's memory buffer and write it to
       the SSD.

Summary of Performance Wins:

 * Improved Performance/Throughput: By eliminating the memcpy step, the CPU is freed up. It can be used for application logic
   (e.g., preparing the next frame) while the DMA hardware handles the data transfer in parallel. This allows for a much higher
   rate of data movement.
 * Reduced Latency: The time for a single write operation is significantly lower because you remove the entire data-copying step
   from the critical path. The request is submitted with minimal overhead via io_uring, and the hardware handles the rest.

In essence, this patchset creates a direct pipeline between device memory and storage, orchestrated by the highly efficient
io_uring interface, cutting the slow CPU out of the data path.

```

## Review

### `[RFC v2 01/11] file: add callback for pre-mapping dmabuf`

[[RFC v2 01/11] file: add callback for pre-mapping dmabuf](https://lore.kernel.org/linux-block/74d689540fa200fe37f1a930165357a92fe9e68c.1763725387.git.asml.silence@gmail.com/)

- add `dma_map` callback to `struct file_operations`

- add helper(dma_token_create) for calling ->dma_map()

- add `struct dma_token`

```
+struct dma_token_params {
+	struct dma_buf			*dmabuf;
+	enum dma_data_direction		dir;
+};
+
+struct dma_token {
+	void (*release)(struct dma_token *);
+};
```

```
patch 10: [RFC v2 10/11] io_uring/rsrc: add dmabuf-backed buffer registeration

dma_token_create
    <-io_register_dmabuf
        <-io_sqe_buffer_register
```

### `[RFC v2 02/11] iov_iter: introduce iter type for pre-registered dma`

[[RFC v2 02/11] iov_iter: introduce iter type for pre-registered dma](https://lore.kernel.org/linux-block/f57269489c4d6f670ab1f9de4d0764030d8d080c.1763725387.git.asml.silence@gmail.com/)

```
Introduce a new iterator type backed by a pre mapped dmabuf represented
by struct dma_token. The token is specific to the file for which it was
created, and the user must avoid the token and the iterator to any other
file. This limitation will be softened in the future.
```

### `[RFC v2 04/11] block: introduce dma token backed bio type`

[[RFC v2 04/11] block: introduce dma token backed bio type](https://lore.kernel.org/linux-block/12530de6d1907afb44be3e76e7668b935f1fd441.1763725387.git.asml.silence@gmail.com/)

```
Premapped buffers don't require a generic bio_vec since these have
already been dma mapped. Repurpose the bi_io_vec space for the dma
token as they are mutually exclusive, and provide setup to support
dma tokens.

In order to use this, a driver must implement the dma_map blk-mq op,
in which case it must be aware that any given bio may be using a
dma_tag instead of a bio_vec.

+void bio_iov_dma_token_set(struct bio *bio, struct iov_iter *iter)
+{
+	WARN_ON_ONCE(bio->bi_max_vecs);
+
+	bio->dma_token = iter->dma_token;
+	bio->bi_vcnt = 0;
+	bio->bi_iter.bi_bvec_done = iter->iov_offset;
+	bio->bi_iter.bi_size = iov_iter_count(iter);
+	bio->bi_opf |= REQ_NOMERGE;
+	bio_set_flag(bio, BIO_DMA_TOKEN);
+}

```

### `[RFC v2 05/11] block: add infra to handle dmabuf tokens`

[[RFC v2 05/11] block: add infra to handle dmabuf tokens](https://lore.kernel.org/linux-block/51cddd97b31d80ec8842a88b9f3c9881419e8a7b.1763725387.git.asml.silence@gmail.com/)

```
Add blk-mq infrastructure to handle dmabuf tokens. There are two main
objects. The first is struct blk_mq_dma_token, which is an extension of
struct dma_token and passed in an iterator. The second is struct
blk_mq_dma_map, which keeps the actual mapping and unlike the token, can
be ejected (e.g. by move_notify) and recreated.

The token keeps an rcu protected pointer to the mapping, so when it
resolves a token into a mapping to pass it to a request, it'll do an rcu
protected lookup and get a percpu reference to the mapping.

If there is no current mapping attached to a token, it'll need to be
created by calling the driver (e.g. nvme) via a new callback. It
requires waiting, thefore can't be done for nowait requests and couldn't
happen deeper in the stack, e.g. during nvme request submission.

The structure split is needed because move_notify can request to
invalidate the dma mapping at any moment, and we need a way to
concurrently remove it and wait for the inflight requests using the
previous mapping to complete.

```

- implement the ->dma_map() for block device, wire dma_buf and bdev

```
@@ -973,6 +973,7 @@ const struct file_operations def_blk_fops = {
 	.fallocate	= blkdev_fallocate,
 	.uring_cmd	= blkdev_uring_cmd,
 	.fop_flags	= FOP_BUFFER_RASYNC,
+	.dma_map	= blkdev_dma_map,
 };

blkdev_dma_map
    ->blk_mq_dma_map
        ->dma_fence_context_alloc
        ->q->mq_ops->init_dma_token
            ->nvme_init_dma_token()
                ->dma_buf_dynamic_attach
```


### `[RFC v2 10/11] io_uring/rsrc: add dmabuf-backed buffer registeration`

[[RFC v2 10/11] io_uring/rsrc: add dmabuf-backed buffer registeration](https://lore.kernel.org/linux-block/b38f2c3af8c03ee4fc5f67f97b4412ecd8588924.1763725388.git.asml.silence@gmail.com/)

```
+	target_file = fget(rb->target_fd);
+	if (!target_file) {
+		ret = -EBADF;
+		goto err;
+	}
+
+	dmabuf = dma_buf_get(rb->dmabuf_fd);
+	if (IS_ERR(dmabuf)) {
+		ret = PTR_ERR(dmabuf);
+		dmabuf = NULL;
+		goto err;
+	}
+
+	params.dmabuf = dmabuf;
+	params.dir = DMA_BIDIRECTIONAL;
+	token = dma_token_create(target_file, &params);
```


